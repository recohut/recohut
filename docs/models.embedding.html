---

title: Embedding


keywords: fastai
sidebar: home_sidebar

summary: "Embedding Models."
description: "Embedding Models."
nb_path: "nbs/models/models.embedding.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/models/models.embedding.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="EmbeddingNet" class="doc_header"><code>class</code> <code>EmbeddingNet</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/embedding.py#L11" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>EmbeddingNet</code>(<strong><code>n_users</code></strong>, <strong><code>n_items</code></strong>, <strong><code>n_factors</code></strong>=<em><code>50</code></em>, <strong><code>embedding_dropout</code></strong>=<em><code>0.02</code></em>, <strong><code>hidden</code></strong>=<em><code>10</code></em>, <strong><code>dropouts</code></strong>=<em><code>0.2</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Creates a dense network with embedding layers.</p>
<p>Args:</p>

<pre><code>n_users:
    Number of unique users in the dataset.
n_items:
    Number of unique items in the dataset.
n_factors:
    Number of columns in the embeddings matrix.
embedding_dropout:
    Dropout rate to apply right after embeddings layer.
hidden:
    A single integer or a list of integers defining the number of
    units in hidden layer(s).
dropouts:
    A single integer or a list of integers defining the dropout
    layers rates applyied right after each of hidden layers.</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="get_list" class="doc_header"><code>get_list</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/embedding.py#L93" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>get_list</code>(<strong><code>n</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># import torch.nn as nn</span>
<span class="c1"># import torch.nn.functional as F</span>

<span class="c1"># from recohut.datasets.synthetic import Synthetic</span>
<span class="c1"># from recohut.transforms.split import chrono_split</span>
<span class="c1"># from recohut.transforms.encode import label_encode as le</span>

<span class="c1"># # generate synthetic implicit data</span>
<span class="c1"># synt = Synthetic()</span>
<span class="c1"># df = synt.implicit()</span>

<span class="c1"># # drop duplicates</span>
<span class="c1"># df = df.drop_duplicates()</span>

<span class="c1"># # chronological split</span>
<span class="c1"># df_train, df_valid = chrono_split(df)</span>
<span class="c1"># print(f&quot;Train set:\n\n{df_train}\n{&#39;=&#39;*100}\n&quot;)</span>
<span class="c1"># print(f&quot;Validation set:\n\n{df_valid}\n{&#39;=&#39;*100}\n&quot;)</span>

<span class="c1"># # label encoding</span>
<span class="c1"># df_train, uid_maps = le(df_train, col=&#39;USERID&#39;)</span>
<span class="c1"># df_train, iid_maps = le(df_train, col=&#39;ITEMID&#39;)</span>
<span class="c1"># df_valid = le(df_valid, col=&#39;USERID&#39;, maps=uid_maps)</span>
<span class="c1"># df_valid = le(df_valid, col=&#39;ITEMID&#39;, maps=iid_maps)</span>

<span class="c1"># # an Embedding module containing 10 user or item embedding size 3</span>
<span class="c1"># # embedding will be initialized at random</span>
<span class="c1"># embed = nn.Embedding(10, 2)</span>

<span class="c1"># # given a list of ids we can &quot;look up&quot; the embedding corresponing to each id</span>
<span class="c1"># ids = [1,2,0,4,5,1]</span>
<span class="c1"># a = torch.LongTensor([ids])</span>
<span class="c1"># print(f&quot;Randomly initialized Embeddings of a list of ids {ids}:\n\n{embed(a)}\n{&#39;=&#39;*100}\n&quot;)</span>

<span class="c1"># # initializing and multiplying users, items embeddings for the sample dataset</span>
<span class="c1"># emb_size = 2</span>
<span class="c1"># user_emb = nn.Embedding(df_train.USERID.nunique(), emb_size)</span>
<span class="c1"># item_emb = nn.Embedding(df_train.ITEMID.nunique(), emb_size)</span>
<span class="c1"># users = torch.LongTensor(df_train.USERID.values)</span>
<span class="c1"># items = torch.LongTensor(df_train.ITEMID.values)</span>
<span class="c1"># U = user_emb(users)</span>
<span class="c1"># V = item_emb(items)</span>
<span class="c1"># print(f&quot;User embeddings of length {emb_size}:\n\n{U}\n{&#39;=&#39;*100}\n&quot;)</span>
<span class="c1"># print(f&quot;Item embeddings of length {emb_size}:\n\n{V}\n{&#39;=&#39;*100}\n&quot;)</span>
<span class="c1"># print(f&quot;Element-wise multiplication of user and item embeddings:\n\n{U*V}\n{&#39;=&#39;*100}\n&quot;)</span>
<span class="c1"># print(f&quot;Dot product per row:\n\n{(U*V).sum(1)}\n{&#39;=&#39;*100}\n&quot;)</span>


<span class="c1"># &quot;&quot;&quot;</span>
<span class="c1"># Train set:</span>
<span class="c1">#     USERID  ITEMID     EVENT   TIMESTAMP</span>
<span class="c1"># 0        1       1     click  2000-01-01</span>
<span class="c1"># 2        1       2     click  2000-01-02</span>
<span class="c1"># 5        2       1     click  2000-01-01</span>
<span class="c1"># 6        2       2  purchase  2000-01-01</span>
<span class="c1"># 7        2       1       add  2000-01-03</span>
<span class="c1"># 8        2       2  purchase  2000-01-03</span>
<span class="c1"># 10       3       3     click  2000-01-01</span>
<span class="c1"># 11       3       3     click  2000-01-03</span>
<span class="c1"># 12       3       3       add  2000-01-03</span>
<span class="c1"># 13       3       3  purchase  2000-01-03</span>
<span class="c1"># ====================================================================================================</span>
<span class="c1"># Validation set:</span>
<span class="c1">#     USERID  ITEMID     EVENT   TIMESTAMP</span>
<span class="c1"># 4        1       2  purchase  2000-01-02</span>
<span class="c1"># 9        2       3  purchase  2000-01-03</span>
<span class="c1"># 14       3       1     click  2000-01-04</span>
<span class="c1"># ====================================================================================================</span>
<span class="c1"># Randomly initialized Embeddings of a list of ids [1, 2, 0, 4, 5, 1]:</span>
<span class="c1"># tensor([[[-0.4989, -0.0017],</span>
<span class="c1">#          [ 0.2724,  0.1308],</span>
<span class="c1">#          [-0.3845,  1.0548],</span>
<span class="c1">#          [ 0.0951, -0.7816],</span>
<span class="c1">#          [-1.2381,  0.4325],</span>
<span class="c1">#          [-0.4989, -0.0017]]], grad_fn=&lt;EmbeddingBackward&gt;)</span>
<span class="c1"># ====================================================================================================</span>
<span class="c1"># User embeddings of length 2:</span>
<span class="c1"># tensor([[-0.7574, -1.1494],</span>
<span class="c1">#         [-0.7574, -1.1494],</span>
<span class="c1">#         [ 1.3911,  1.0157],</span>
<span class="c1">#         [ 1.3911,  1.0157],</span>
<span class="c1">#         [ 1.3911,  1.0157],</span>
<span class="c1">#         [ 1.3911,  1.0157],</span>
<span class="c1">#         [ 0.0271, -1.2206],</span>
<span class="c1">#         [ 0.0271, -1.2206],</span>
<span class="c1">#         [ 0.0271, -1.2206],</span>
<span class="c1">#         [ 0.0271, -1.2206]], grad_fn=&lt;EmbeddingBackward&gt;)</span>
<span class="c1"># ====================================================================================================</span>
<span class="c1"># Item embeddings of length 2:</span>
<span class="c1"># tensor([[ 0.0406,  0.4805],</span>
<span class="c1">#         [-0.7570, -1.6676],</span>
<span class="c1">#         [ 0.0406,  0.4805],</span>
<span class="c1">#         [-0.7570, -1.6676],</span>
<span class="c1">#         [ 0.0406,  0.4805],</span>
<span class="c1">#         [-0.7570, -1.6676],</span>
<span class="c1">#         [-0.9237,  1.2666],</span>
<span class="c1">#         [-0.9237,  1.2666],</span>
<span class="c1">#         [-0.9237,  1.2666],</span>
<span class="c1">#         [-0.9237,  1.2666]], grad_fn=&lt;EmbeddingBackward&gt;)</span>
<span class="c1"># ====================================================================================================</span>
<span class="c1"># Element-wise multiplication of user and item embeddings:</span>
<span class="c1"># tensor([[-0.0308, -0.5522],</span>
<span class="c1">#         [ 0.5733,  1.9167],</span>
<span class="c1">#         [ 0.0565,  0.4880],</span>
<span class="c1">#         [-1.0530, -1.6937],</span>
<span class="c1">#         [ 0.0565,  0.4880],</span>
<span class="c1">#         [-1.0530, -1.6937],</span>
<span class="c1">#         [-0.0251, -1.5460],</span>
<span class="c1">#         [-0.0251, -1.5460],</span>
<span class="c1">#         [-0.0251, -1.5460],</span>
<span class="c1">#         [-0.0251, -1.5460]], grad_fn=&lt;MulBackward0&gt;)</span>
<span class="c1"># ====================================================================================================</span>
<span class="c1"># Dot product per row:</span>
<span class="c1"># tensor([-0.5830,  2.4900,  0.5445, -2.7467,  0.5445, -2.7467, -1.5711, -1.5711,</span>
<span class="c1">#         -1.5711, -1.5711], grad_fn=&lt;SumBackward1&gt;)</span>
<span class="c1"># ====================================================================================================</span>
<span class="c1"># &quot;&quot;&quot;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="GroupEmbedding" class="doc_header"><code>class</code> <code>GroupEmbedding</code><a href="https://github.com/RecoHut-Projects/recohut/tree/master/recohut/models/embedding.py#L101" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>GroupEmbedding</code>(<strong><code>embedding_size</code></strong>:<code>int</code>, <strong><code>user_num</code></strong>:<code>int</code>, <strong><code>item_num</code></strong>:<code>int</code>) :: <code>Module</code></p>
</blockquote>
<p>Embedding Network</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">group_embeddings</span> <span class="o">=</span> <span class="n">GroupEmbedding</span><span class="p">(</span><span class="n">embedding_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                      <span class="n">user_num</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                      <span class="n">item_num</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">group_embeddings</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">group_members</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span>
                         <span class="n">history</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([ 1.4833,  1.4087,  0.9842, -1.9608, -0.5692,  0.9200,  1.1108,  1.2899,
        -1.0670,  1.1149, -0.1407,  0.8058, -0.1740, -0.6787,  0.9383,  0.4889],
       grad_fn=&lt;CatBackward0&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

