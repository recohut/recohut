{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8WPZqXax2ox"
      },
      "outputs": [],
      "source": [
        "# default_exp datasets.bases.sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvconAiQx2o4"
      },
      "source": [
        "# Sequential Base Dataset\n",
        "> Implementation of sequential base dataset modules in Pytorch Lightning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJTxZGK_x2pA"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "from nbdev.showdoc import *\n",
        "from fastcore.nb_imports import *\n",
        "from fastcore.test import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eH_8tx7ix2pC"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "from typing import Any, Iterable, List, Optional, Tuple, Union, Callable\n",
        "\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from pytorch_lightning import LightningDataModule\n",
        "\n",
        "from recohut.datasets.bases.common import Dataset as BaseDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V07HcsF3x2pE"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class SequentialDataset(Dataset, BaseDataset):\n",
        "    def __init__(self,\n",
        "                 data_dir,\n",
        "                 data_type='train',\n",
        "                 history_size=8,\n",
        "                 step_size=1,\n",
        "                 seed=42,\n",
        "                 mask=1,\n",
        "                 *args,\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir: Where to save/load the data\n",
        "            data_type: train/valid/test\n",
        "        \"\"\"\n",
        "        self.data_type = data_type\n",
        "        self.history_size = history_size\n",
        "        self.step_size = step_size\n",
        "        self.seed = seed\n",
        "        self.mask = mask\n",
        "\n",
        "        super().__init__(data_dir)\n",
        "\n",
        "        self._process()\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['data.pt']\n",
        "\n",
        "    def download(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def load_ratings_df(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def map_column(self, df: pd.DataFrame, col_name: str):\n",
        "        \"\"\"Maps column values to integers.\n",
        "        \"\"\"\n",
        "        values = sorted(list(df[col_name].unique()))\n",
        "        mapping = {k: i + 2 for i, k in enumerate(values)}\n",
        "        inverse_mapping = {v: k for k, v in mapping.items()}\n",
        "        df[col_name + \"_mapped\"] = df[col_name].map(mapping)\n",
        "        return df, mapping, inverse_mapping\n",
        "\n",
        "    def get_context(self, df: pd.DataFrame, split: str, context_size: int = 120, val_context_size: int = 5, seed: int = 42):\n",
        "        \"\"\"Create a training / validation samples.\n",
        "        \"\"\"\n",
        "        random.seed(seed)\n",
        "        if split == \"train\":\n",
        "            end_index = random.randint(10, df.shape[0] - val_context_size)\n",
        "        elif split in [\"valid\", \"test\"]:\n",
        "            end_index = df.shape[0]\n",
        "        else:\n",
        "            raise ValueError\n",
        "        start_index = max(0, end_index - context_size)\n",
        "        context = df[start_index:end_index]\n",
        "        return context\n",
        "\n",
        "    def pad_list(self, list_integers, history_size: int, pad_val: int = 0, mode=\"left\"):\n",
        "        \"\"\"Pad list from left or right\n",
        "        \"\"\"\n",
        "        if len(list_integers) < history_size:\n",
        "            if mode == \"left\":\n",
        "                list_integers = [pad_val] * (history_size - len(list_integers)) + list_integers\n",
        "            else:\n",
        "                list_integers = list_integers + [pad_val] * (history_size - len(list_integers))\n",
        "        return list_integers\n",
        "\n",
        "    def mask_list(self, l1, p=0.8):\n",
        "        random.seed(self.seed)\n",
        "        l1 = [a if random.random() < p else self.mask for a in l1]\n",
        "        return l1\n",
        "\n",
        "    def mask_last_elements_list(self, l1, val_context_size: int = 5):\n",
        "        l1 = l1[:-val_context_size] + self.mask_list(l1[-val_context_size:], p=0.5)\n",
        "        return l1\n",
        "\n",
        "    def make_user_history(self, data):\n",
        "        user_history = [ [] for _ in range(self.num_users) ]\n",
        "        for u, i, r in data: user_history[u].append(i)\n",
        "        return user_history\n",
        "\n",
        "    # def pad(self, arr, max_len = None, pad_with = -1, side = 'right'):\n",
        "    #     seq_len = max_len if max_len is not None else max(map(len, arr))\n",
        "    #     seq_len = min(seq_len, 200) # You don't need more than this\n",
        "\n",
        "    #     for i in range(len(arr)):\n",
        "    #         while len(arr[i]) < seq_len: \n",
        "    #             pad_elem = arr[i][-1] if len(arr[i]) > 0 else 0\n",
        "    #             pad_elem = pad_elem if pad_with == -1 else pad_with\n",
        "    #             if side == 'right': arr[i].append(pad_elem)\n",
        "    #             else: arr[i] = [ pad_elem ] + arr[i]\n",
        "    #         arr[i] = arr[i][-seq_len:] # Keep last `seq_len` items\n",
        "    #     return arr\n",
        "\n",
        "    # def sequential_pad(self, arr, max_seq_len, total_items):\n",
        "    #     # Padding left side so that we can simply take out [:, -1, :] in the output\n",
        "    #     return self.pad(\n",
        "    #         arr, max_len = max_seq_len, \n",
        "    #         pad_with = total_items, side = 'left'\n",
        "    #     )\n",
        "\n",
        "    # def scatter(self, batch, tensor_kind, last_dimension):\n",
        "    #     ret = tensor_kind(len(batch), last_dimension).zero_()\n",
        "\n",
        "    #     if not torch.is_tensor(batch):\n",
        "    #         if ret.is_cuda: batch = torch.cuda.LongTensor(batch)\n",
        "    #         else: batch = torch.LongTensor(batch)\n",
        "\n",
        "    #     return ret.scatter_(1, batch, 1)\n",
        "\n",
        "    # def get_item_count_map(self, data):\n",
        "    #     item_count = defaultdict(int)\n",
        "    #     for u, i, r in data: item_count[i] += 1\n",
        "    #     return item_count\n",
        "\n",
        "    # def get_item_propensity(self, data, num_items, A = 0.55, B = 1.5):\n",
        "    #     item_freq_map = self.get_item_count_map()\n",
        "    #     item_freq = [ item_freq_map[i] for i in range(num_items) ]\n",
        "    #     num_instances = len(data)\n",
        "\n",
        "    #     C = (np.log(num_instances)-1)*np.power(B+1, A)\n",
        "    #     wts = 1.0 + C*np.power(np.array(item_freq)+B, -A)\n",
        "    #     return np.ravel(wts)\n",
        "\n",
        "    def create_sequences(self, values, window_size, step_size):\n",
        "        sequences = []\n",
        "        start_index = 0\n",
        "        while True:\n",
        "            end_index = start_index + window_size\n",
        "            seq = values[start_index:end_index]\n",
        "            if len(seq) < window_size:\n",
        "                seq = values[-window_size:]\n",
        "                if len(seq) == window_size:\n",
        "                    sequences.append(seq)\n",
        "                break\n",
        "            sequences.append(seq)\n",
        "            start_index += step_size\n",
        "        return sequences\n",
        "\n",
        "    def process(self):\n",
        "        df = self.load_ratings_df()\n",
        "        df.sort_values(by=\"timestamp\", inplace=True)\n",
        "        df, self.mapping, self.inverse_mapping = self.map_column(df, col_name=\"sid\")\n",
        "        self.grp_by = df.groupby(by=\"uid\")\n",
        "        self.groups = list(self.grp_by.groups)\n",
        "\n",
        "    def __len__(self):\n",
        "            return len(self.groups)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        group = self.groups[index]\n",
        "        df = self.grp_by.get_group(group)\n",
        "        context = self.get_context(df, split=self.data_type, context_size=self.history_size)\n",
        "        trg_items = context[\"sid_mapped\"].tolist()\n",
        "        if self.data_type == \"train\":\n",
        "            src_items = self.mask_list(trg_items)\n",
        "        else:\n",
        "            src_items = self.mask_last_elements_list(trg_items)\n",
        "        pad_mode = \"left\" if random.random() < 0.5 else \"right\"\n",
        "        trg_items = self.pad_list(trg_items, history_size=self.history_size, mode=pad_mode)\n",
        "        src_items = self.pad_list(src_items, history_size=self.history_size, mode=pad_mode)\n",
        "        src_items = torch.tensor(src_items, dtype=torch.long)\n",
        "        trg_items = torch.tensor(trg_items, dtype=torch.long)\n",
        "        return src_items, trg_items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4cgOXllx2pI"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class SequentialDataModule(LightningDataModule):\n",
        "\n",
        "    dataset_cls: str = \"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_dir: Optional[str] = None,\n",
        "                 num_workers: int = 0,\n",
        "                 normalize: bool = False,\n",
        "                 batch_size: int = 32,\n",
        "                 shuffle: bool = True,\n",
        "                 pin_memory: bool = True,\n",
        "                 drop_last: bool = False,\n",
        "                 *args, \n",
        "                 **kwargs) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir: Where to save/load the data\n",
        "            num_workers: How many workers to use for loading data\n",
        "            normalize: If true applies rating normalize\n",
        "            batch_size: How many samples per batch to load\n",
        "            shuffle: If true shuffles the train data every epoch\n",
        "            pin_memory: If true, the data loader will copy Tensors into CUDA pinned memory before\n",
        "                        returning them\n",
        "            drop_last: If true drops the last incomplete batch\n",
        "        \"\"\"\n",
        "        super().__init__(data_dir)\n",
        "\n",
        "        self.data_dir = data_dir if data_dir is not None else os.getcwd()\n",
        "        self.num_workers = num_workers\n",
        "        self.normalize = normalize\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.pin_memory = pin_memory\n",
        "        self.drop_last = drop_last\n",
        "        self.kwargs = kwargs\n",
        "\n",
        "    def prepare_data(self, *args: Any, **kwargs: Any) -> None:\n",
        "        \"\"\"Saves files to data_dir.\"\"\"\n",
        "        self.data = self.dataset_cls(self.data_dir, **self.kwargs)\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None) -> None:\n",
        "        \"\"\"Creates train, val, and test dataset.\"\"\"\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            self.dataset_train = self.dataset_cls(self.data_dir, data_type='train', **self.kwargs)\n",
        "            self.dataset_val = self.dataset_cls(self.data_dir, data_type='valid', **self.kwargs)\n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.dataset_test = self.dataset_cls(self.data_dir, data_type='test', **self.kwargs)\n",
        "\n",
        "    def train_dataloader(self, *args: Any, **kwargs: Any) -> DataLoader:\n",
        "        \"\"\"The train dataloader.\"\"\"\n",
        "        return self._data_loader(self.dataset_train, shuffle=self.shuffle)\n",
        "\n",
        "    def val_dataloader(self, *args: Any, **kwargs: Any) -> Union[DataLoader, List[DataLoader]]:\n",
        "        \"\"\"The val dataloader.\"\"\"\n",
        "        return self._data_loader(self.dataset_val)\n",
        "\n",
        "    def test_dataloader(self, *args: Any, **kwargs: Any) -> Union[DataLoader, List[DataLoader]]:\n",
        "        \"\"\"The test dataloader.\"\"\"\n",
        "        return self._data_loader(self.dataset_test)\n",
        "\n",
        "    def _data_loader(self, dataset: Dataset, shuffle: bool = False) -> DataLoader:\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=shuffle,\n",
        "            num_workers=self.num_workers,\n",
        "            drop_last=self.drop_last,\n",
        "            pin_memory=self.pin_memory,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGv-xeWFx2pO"
      },
      "source": [
        "Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wg-SUnJDx2pU"
      },
      "outputs": [],
      "source": [
        "class ML1mDataset(SequentialDataset):\n",
        "    url = \"http://files.grouplens.org/datasets/movielens/ml-1m.zip\"\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return 'ratings.dat'\n",
        "\n",
        "    def download(self):\n",
        "        path = download_url(self.url, self.raw_dir)\n",
        "        extract_zip(path, self.raw_dir)\n",
        "        from shutil import move, rmtree\n",
        "        move(os.path.join(self.raw_dir, 'ml-1m', self.raw_file_names), self.raw_dir)\n",
        "        rmtree(os.path.join(self.raw_dir, 'ml-1m'))\n",
        "        os.unlink(path)\n",
        "\n",
        "    def load_ratings_df(self):\n",
        "        df = pd.read_csv(self.raw_paths[0], sep='::', header=None, engine='python')\n",
        "        df.columns = ['uid', 'sid', 'rating', 'timestamp']\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI5m1Alsx2pV"
      },
      "outputs": [],
      "source": [
        "class ML1mDataModule(SequentialDataModule):\n",
        "    dataset_cls = ML1mDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0A8abvcSx2pW"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.pad = 0\n",
        "        self.mask = 1\n",
        "        self.cap = 0\n",
        "        self.seed = 42\n",
        "        self.vocab_size = 10000\n",
        "        self.channels = 128\n",
        "        self.dropout = 0.4\n",
        "        self.learning_rate = 1e-4\n",
        "        self.history_size = 30\n",
        "        self.data_dir = '/content/data'\n",
        "        self.log_dir = '/content/recommender_logs'\n",
        "        self.model_dir = '/content/recommender_models'\n",
        "        self.batch_size = 32\n",
        "        self.shuffle = True\n",
        "        self.max_epochs = 2\n",
        "        self.val_epoch = 1\n",
        "        self.gpus = None\n",
        "        self.monitor = 'valid_loss'\n",
        "        self.mode = 'min'\n",
        "\n",
        "args = Args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AidPOt8Ox2pX"
      },
      "outputs": [],
      "source": [
        "ds = ML1mDataModule(data_sir=args.data_dir, **args.__dict__)\n",
        "ds.prepare_data()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "name": "datasets.bases.sequential.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}