{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mb3PMzY7BbW4"
      },
      "outputs": [],
      "source": [
        "# default_exp models.sasrec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTX8X2wHBbW9"
      },
      "source": [
        "# SASRec\n",
        "> Self-Attentive Sequential Recommendation Model.\n",
        "\n",
        "Sequential dynamics are a key feature of many modern recommender systems, which seek to capture the ‘context’ of users’ activities on the basis of actions they have performed recently. To capture such patterns, two approaches have proliferated: Markov Chains (MCs) and Recurrent Neural Networks (RNNs). Markov Chains assume that a user’s next action can be predicted on the basis of just their last (or last few) actions, while RNNs in principle allow for longer-term semantics to be uncovered. Generally speaking, MC-based methods perform best in extremely sparse datasets, where model parsimony is critical, while RNNs perform better in denser datasets where higher model complexity is affordable. SASRec captures the long-term semantics (like an RNN), but, using an attention mechanism, makes its predictions based on relatively few actions (like an MC).\n",
        "\n",
        "At each time step, SASRec seeks to identify which items are ‘relevant’ from a user’s action history, and use them to predict the next item. Extensive empirical studies show that this method outperforms various state-of-the-art sequential models (including MC/CNN/RNN-based approaches) on both sparse and dense datasets. Moreover, the model is an order of magnitude more efficient than comparable CNN/RNN-based models.\n",
        "\n",
        "We adopt the binary cross entropy loss as the objective function:\n",
        "\n",
        "$$-\\sum_{S^u\\in S} \\sum_{t \\in [1,2,\\dots,n]}\\left[ log(\\sigma(r_{o_t,t})) + \\sum_{j \\notin S^u} log(1-\\sigma(r_{j,t})) \\right]$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sc6CiFnIBbXA"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "from nbdev.showdoc import *\n",
        "from fastcore.nb_imports import *\n",
        "from fastcore.test import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy3MRlcJBbXB"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "\n",
        "from recohut.models.layers.attention import *\n",
        "from recohut.models.layers.encoding import Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2R5Cq9RCBbXC"
      },
      "outputs": [],
      "source": [
        "#exporti\n",
        "class SASEmbedding(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        vocab_size = args.num_items + 1\n",
        "        hidden = args.bert_hidden_units\n",
        "        max_len = args.bert_max_len\n",
        "        dropout = args.bert_dropout\n",
        "\n",
        "        self.token = TokenEmbedding(\n",
        "            vocab_size=vocab_size, embed_size=hidden)\n",
        "        self.position = PositionalEmbedding(\n",
        "            max_len=max_len, d_model=hidden)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def get_mask(self, x):\n",
        "        if len(x.shape) > 2:\n",
        "            x = torch.ones(x.shape[:2]).to(x.device)\n",
        "        return (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mask = self.get_mask(x)\n",
        "        if len(x.shape) > 2:\n",
        "            pos = self.position(torch.ones(x.shape[:2]).to(x.device))\n",
        "            x = torch.matmul(x, self.token.weight) + pos\n",
        "        else:\n",
        "            x = self.token(x) + self.position(x)\n",
        "        return self.dropout(x), mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O29SMq3pBbXD"
      },
      "outputs": [],
      "source": [
        "#exporti\n",
        "class SASModel(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        hidden = args.bert_hidden_units\n",
        "        heads = args.bert_num_heads\n",
        "        head_size = args.bert_head_size\n",
        "        dropout = args.bert_dropout\n",
        "        attn_dropout = args.bert_attn_dropout\n",
        "        layers = args.bert_num_blocks\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList([SASTransformerBlock(\n",
        "            hidden, heads, head_size, hidden * 4, dropout, attn_dropout) for _ in range(layers)])\n",
        "\n",
        "    def forward(self, x, embedding_weight, mask):\n",
        "        for transformer in self.transformer_blocks:\n",
        "            x = transformer.forward(x, mask)\n",
        "        scores = torch.matmul(x, embedding_weight.permute(1, 0))\n",
        "        return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DD-n04XYBbXE"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class SASRec(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.embedding = SASEmbedding(self.args)\n",
        "        self.model = SASModel(self.args)\n",
        "        self.truncated_normal_init()\n",
        "\n",
        "    def truncated_normal_init(self, mean=0, std=0.02, lower=-0.04, upper=0.04):\n",
        "        with torch.no_grad():\n",
        "            l = (1. + math.erf(((lower - mean) / std) / math.sqrt(2.))) / 2.\n",
        "            u = (1. + math.erf(((upper - mean) / std) / math.sqrt(2.))) / 2.\n",
        "\n",
        "            for n, p in self.model.named_parameters():\n",
        "                if not 'layer_norm' in n:\n",
        "                    p.uniform_(2 * l - 1, 2 * u - 1)\n",
        "                    p.erfinv_()\n",
        "                    p.mul_(std * math.sqrt(2.))\n",
        "                    p.add_(mean)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x, mask = self.embedding(x)\n",
        "        scores = self.model(x, self.embedding.token.weight, mask)\n",
        "        return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEBZAC03BbXF",
        "outputId": "a86e5ee3-b5b1-44bf-9b15-f2fe4e68f472"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method Module.parameters of SASRec(\n",
              "  (embedding): SASEmbedding(\n",
              "    (token): TokenEmbedding(11, 4, padding_idx=0)\n",
              "    (position): PositionalEmbedding(\n",
              "      (pe): Embedding(9, 4)\n",
              "    )\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (model): SASModel(\n",
              "    (transformer_blocks): ModuleList(\n",
              "      (0): SASTransformerBlock(\n",
              "        (layer_norm): LayerNorm()\n",
              "        (attention): SASMultiHeadedAttention(\n",
              "          (linear_layers): ModuleList(\n",
              "            (0): Linear(in_features=4, out_features=8, bias=True)\n",
              "            (1): Linear(in_features=4, out_features=8, bias=True)\n",
              "            (2): Linear(in_features=4, out_features=8, bias=True)\n",
              "          )\n",
              "          (attention): Attention()\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "          (layer_norm): LayerNorm()\n",
              "        )\n",
              "        (feed_forward): SASPositionwiseFeedForward(\n",
              "          (conv1): Conv1d(4, 16, kernel_size=(1,), stride=(1,))\n",
              "          (activation): ReLU()\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "          (conv2): Conv1d(16, 4, kernel_size=(1,), stride=(1,))\n",
              "          (layer_norm): LayerNorm()\n",
              "        )\n",
              "      )\n",
              "      (1): SASTransformerBlock(\n",
              "        (layer_norm): LayerNorm()\n",
              "        (attention): SASMultiHeadedAttention(\n",
              "          (linear_layers): ModuleList(\n",
              "            (0): Linear(in_features=4, out_features=8, bias=True)\n",
              "            (1): Linear(in_features=4, out_features=8, bias=True)\n",
              "            (2): Linear(in_features=4, out_features=8, bias=True)\n",
              "          )\n",
              "          (attention): Attention()\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "          (layer_norm): LayerNorm()\n",
              "        )\n",
              "        (feed_forward): SASPositionwiseFeedForward(\n",
              "          (conv1): Conv1d(4, 16, kernel_size=(1,), stride=(1,))\n",
              "          (activation): ReLU()\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "          (conv2): Conv1d(16, 4, kernel_size=(1,), stride=(1,))\n",
              "          (layer_norm): LayerNorm()\n",
              "        )\n",
              "      )\n",
              "      (2): SASTransformerBlock(\n",
              "        (layer_norm): LayerNorm()\n",
              "        (attention): SASMultiHeadedAttention(\n",
              "          (linear_layers): ModuleList(\n",
              "            (0): Linear(in_features=4, out_features=8, bias=True)\n",
              "            (1): Linear(in_features=4, out_features=8, bias=True)\n",
              "            (2): Linear(in_features=4, out_features=8, bias=True)\n",
              "          )\n",
              "          (attention): Attention()\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "          (layer_norm): LayerNorm()\n",
              "        )\n",
              "        (feed_forward): SASPositionwiseFeedForward(\n",
              "          (conv1): Conv1d(4, 16, kernel_size=(1,), stride=(1,))\n",
              "          (activation): ReLU()\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "          (conv2): Conv1d(16, 4, kernel_size=(1,), stride=(1,))\n",
              "          (layer_norm): LayerNorm()\n",
              "        )\n",
              "      )\n",
              "      (3): SASTransformerBlock(\n",
              "        (layer_norm): LayerNorm()\n",
              "        (attention): SASMultiHeadedAttention(\n",
              "          (linear_layers): ModuleList(\n",
              "            (0): Linear(in_features=4, out_features=8, bias=True)\n",
              "            (1): Linear(in_features=4, out_features=8, bias=True)\n",
              "            (2): Linear(in_features=4, out_features=8, bias=True)\n",
              "          )\n",
              "          (attention): Attention()\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "          (layer_norm): LayerNorm()\n",
              "        )\n",
              "        (feed_forward): SASPositionwiseFeedForward(\n",
              "          (conv1): Conv1d(4, 16, kernel_size=(1,), stride=(1,))\n",
              "          (activation): ReLU()\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "          (conv2): Conv1d(16, 4, kernel_size=(1,), stride=(1,))\n",
              "          (layer_norm): LayerNorm()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class Args:\n",
        "    bert_hidden_units = 4\n",
        "    bert_num_heads = 2\n",
        "    bert_head_size = 4\n",
        "    bert_dropout = 0.2\n",
        "    bert_attn_dropout = 0.2\n",
        "    bert_num_blocks = 4\n",
        "    num_items = 10\n",
        "    bert_hidden_units = 4\n",
        "    bert_max_len = 8\n",
        "    bert_dropout = 0.2\n",
        "\n",
        "args = Args()\n",
        "model = SASRec(args)\n",
        "model.parameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class SASRec_v2(nn.Module):\n",
        "    \"\"\"\n",
        "    References:\n",
        "        1. https://github.com/RecoHut-Stanzas/STOSA/blob/ee14e2eabcc60922eb52cc7d3231df4954d9ff16/seqmodels.py#L5\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 item_size,\n",
        "                 hidden_size,\n",
        "                 attribute_size,\n",
        "                 max_seq_length,\n",
        "                 mask_id,\n",
        "                 num_attention_heads,\n",
        "                 num_hidden_layers=2,\n",
        "                 hidden_dropout_prob=0.2,\n",
        "                 attention_probs_dropout_prob=0.2,\n",
        "                 hidden_act='gelu',\n",
        "                 initializer_range=0.02):\n",
        "        super().__init__()\n",
        "        self.item_size = item_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.attribute_size = attribute_size\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.mask_id = mask_id\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\n",
        "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "        self.hidden_act = hidden_act\n",
        "        self.initializer_range = initializer_range\n",
        "\n",
        "        self.item_embeddings = nn.Embedding(item_size, hidden_size, padding_idx=0)\n",
        "        self.position_embeddings = nn.Embedding(max_seq_length, hidden_size)\n",
        "        self.item_encoder = Encoder(hidden_size, hidden_act, num_attention_heads, \n",
        "                                    hidden_dropout_prob, attention_probs_dropout_prob, \n",
        "                                    num_hidden_layers)\n",
        "        self.layernorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
        "        self.criterion = nn.BCELoss(reduction='none')\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def add_position_embedding(self, sequence):\n",
        "        seq_length = sequence.size(1)\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=sequence.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(sequence)\n",
        "        item_embeddings = self.item_embeddings(sequence)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        sequence_emb = item_embeddings + position_embeddings\n",
        "        sequence_emb = self.layernorm(sequence_emb)\n",
        "        sequence_emb = self.dropout(sequence_emb)\n",
        "        return sequence_emb\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        attention_mask = (input_ids > 0).long()\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2) # torch.int64\n",
        "        max_len = attention_mask.size(-1)\n",
        "        attn_shape = (1, max_len, max_len)\n",
        "        subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1) # torch.uint8\n",
        "        subsequent_mask = (subsequent_mask == 0).unsqueeze(1)\n",
        "        subsequent_mask = subsequent_mask.long()\n",
        "\n",
        "        extended_attention_mask = extended_attention_mask * subsequent_mask\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        sequence_emb = self.add_position_embedding(input_ids)\n",
        "\n",
        "        item_encoded_layers = self.item_encoder(sequence_emb,\n",
        "                                                extended_attention_mask,\n",
        "                                                output_all_encoded_layers=True)\n",
        "\n",
        "        sequence_output, attention_scores = item_encoded_layers[-1]\n",
        "        return sequence_output, attention_scores\n",
        "\n",
        "    def init_weights(self, module):\n",
        "        \"\"\" Initialize the weights.\n",
        "        \"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()"
      ],
      "metadata": {
        "id": "x5ry6UrnxSXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_size = 10\n",
        "hidden_size = 10\n",
        "attribute_size = 5\n",
        "max_seq_length = 20\n",
        "hidden_dropout_prob = 0.2\n",
        "mask_id = 20\n",
        "initializer_range = 0.02\n",
        "num_hidden_layers = 2\n",
        "attention_probs_dropout_prob = 0.2\n",
        "hidden_act = 'gelu'\n",
        "num_attention_heads = 2\n",
        "\n",
        "model = SASRec_v2(item_size, hidden_size, attribute_size, max_seq_length, mask_id, \n",
        "                   num_attention_heads, num_hidden_layers, hidden_dropout_prob, \n",
        "                   attention_probs_dropout_prob, hidden_act, initializer_range)\n",
        "\n",
        "x = torch.randint(0, 5, (item_size, hidden_size))\n",
        "\n",
        "output = model.forward(x)\n",
        "output_shapes = [list(x.shape) for x in [j for sub in output for j in sub]]\n",
        "\n",
        "test_eq(output_shapes[0], [10, 10])\n",
        "test_eq(output_shapes[11], [2, 10, 10])"
      ],
      "metadata": {
        "id": "6Hd-2b6p0XL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27meIjlIBbXH"
      },
      "source": [
        "> References\n",
        "1. https://cseweb.ucsd.edu/~jmcauley/pdfs/icdm18.pdf\n",
        "2. https://github.com/Yueeeeeeee/RecSys-Extraction-Attack/blob/main/model/sasrec.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAuMDgX-BbXI",
        "outputId": "26f36789-71ca-4ddb-b27d-4defedfbb872"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Author: Sparsh A.\n",
            "\n",
            "Last updated: 2021-12-31 06:51:33\n",
            "\n",
            "recohut: 0.0.8\n",
            "\n",
            "Compiler    : GCC 7.5.0\n",
            "OS          : Linux\n",
            "Release     : 5.4.144+\n",
            "Machine     : x86_64\n",
            "Processor   : x86_64\n",
            "CPU cores   : 2\n",
            "Architecture: 64bit\n",
            "\n",
            "PIL       : 7.1.2\n",
            "torch     : 1.10.0+cu111\n",
            "matplotlib: 3.2.2\n",
            "numpy     : 1.19.5\n",
            "IPython   : 5.5.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#hide\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Sparsh A.\" -m -iv -u -t -d -p recohut"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "name": "models.sasrec.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}