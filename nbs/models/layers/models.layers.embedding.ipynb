{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1_rWVsdH43f"
      },
      "outputs": [],
      "source": [
        "# default_exp models.layers.embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSqdVNu5H43k"
      },
      "source": [
        "# Embedding Layers\n",
        "> Implementation of NN embedding layers in Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAl7a8YcH43n"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "from nbdev.showdoc import *\n",
        "from fastcore.nb_imports import *\n",
        "from fastcore.test import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrjZLa6gH43p"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "import torch\n",
        "from torch import nn\n",
        "import h5py\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Du5wvsYsH43q"
      },
      "outputs": [],
      "source": [
        "#exporti\n",
        "class MaskedAveragePooling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MaskedAveragePooling, self).__init__()\n",
        "\n",
        "    def forward(self, embedding_matrix):\n",
        "        sum_pooling_matrix = torch.sum(embedding_matrix, dim=1)\n",
        "        non_padding_length = (embedding_matrix != 0).sum(dim=1)\n",
        "        embedding_vec = sum_pooling_matrix / (non_padding_length.float() + 1e-16)\n",
        "        return embedding_vec\n",
        "\n",
        "\n",
        "class MaskedSumPooling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MaskedSumPooling, self).__init__()\n",
        "\n",
        "    def forward(self, embedding_matrix):\n",
        "        # mask by zeros\n",
        "        return torch.sum(embedding_matrix, dim=1)\n",
        "\n",
        "\n",
        "class KMaxPooling(nn.Module):\n",
        "    def __init__(self, k, dim):\n",
        "        super(KMaxPooling, self).__init__()\n",
        "        self.k = k\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, X):\n",
        "        index = X.topk(self.k, dim=self.dim)[1].sort(dim=self.dim)[0]\n",
        "        output = X.gather(self.dim, index)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrh4Sf2EH43r"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class EmbeddingLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 feature_map,\n",
        "                 embedding_dim,\n",
        "                 use_pretrain=True,\n",
        "                 required_feature_columns=[],\n",
        "                 not_required_feature_columns=[]):\n",
        "        super(EmbeddingLayer, self).__init__()\n",
        "        self.embedding_layer = EmbeddingDictLayer(feature_map, \n",
        "                                                  embedding_dim,\n",
        "                                                  use_pretrain=use_pretrain,\n",
        "                                                  required_feature_columns=required_feature_columns,\n",
        "                                                  not_required_feature_columns=not_required_feature_columns)\n",
        "\n",
        "    def forward(self, X):\n",
        "        feature_emb_dict = self.embedding_layer(X)\n",
        "        feature_emb = self.embedding_layer.dict2tensor(feature_emb_dict)\n",
        "        return feature_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FT3whdkAH43t"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class EmbeddingDictLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 feature_map, \n",
        "                 embedding_dim, \n",
        "                 use_pretrain=True,\n",
        "                 required_feature_columns=[],\n",
        "                 not_required_feature_columns=[]):\n",
        "        super(EmbeddingDictLayer, self).__init__()\n",
        "        self._feature_map = feature_map\n",
        "        self.required_feature_columns = required_feature_columns\n",
        "        self.not_required_feature_columns = not_required_feature_columns\n",
        "        self.embedding_layer = nn.ModuleDict()\n",
        "        self.sequence_encoder = nn.ModuleDict()\n",
        "        self.embedding_hooks = nn.ModuleDict()\n",
        "        for feature, feature_spec in self._feature_map.feature_specs.items():\n",
        "            if self.is_required(feature):\n",
        "                if (not use_pretrain) and embedding_dim == 1:\n",
        "                    feat_emb_dim = 1 # in case for LR\n",
        "                else:\n",
        "                    feat_emb_dim = feature_spec.get(\"embedding_dim\", embedding_dim)\n",
        "                    if \"pretrained_emb\" in feature_spec:\n",
        "                        self.embedding_hooks[feature] = nn.Linear(feat_emb_dim, embedding_dim, bias=False)\n",
        "\n",
        "                # Set embedding_layer according to share_embedding\n",
        "                if use_pretrain and \"share_embedding\" in feature_spec:\n",
        "                    self.embedding_layer[feature] = self.embedding_layer[feature_spec[\"share_embedding\"]]\n",
        "                    self.set_sequence_encoder(feature, feature_spec.get(\"encoder\", None))\n",
        "                    continue\n",
        "\n",
        "                if feature_spec[\"type\"] == \"numeric\":\n",
        "                    self.embedding_layer[feature] = nn.Linear(1, feat_emb_dim, bias=False)\n",
        "                elif feature_spec[\"type\"] == \"categorical\":\n",
        "                    padding_idx = feature_spec.get(\"padding_idx\", None)\n",
        "                    embedding_matrix = nn.Embedding(feature_spec[\"vocab_size\"], \n",
        "                                                    feat_emb_dim, \n",
        "                                                    padding_idx=padding_idx)\n",
        "                    if use_pretrain and \"pretrained_emb\" in feature_spec:\n",
        "                        embeddings = self.get_pretrained_embedding(feature_map.data_dir, feature, feature_spec)\n",
        "                        embedding_matrix = self.set_pretrained_embedding(embedding_matrix, \n",
        "                                                                         embeddings, \n",
        "                                                                         freeze=feature_spec[\"freeze_emb\"],\n",
        "                                                                         padding_idx=padding_idx)\n",
        "                    self.embedding_layer[feature] = embedding_matrix\n",
        "                elif feature_spec[\"type\"] == \"sequence\":\n",
        "                    padding_idx = feature_spec[\"vocab_size\"] - 1\n",
        "                    embedding_matrix = nn.Embedding(feature_spec[\"vocab_size\"], \n",
        "                                                    feat_emb_dim, \n",
        "                                                    padding_idx=padding_idx)\n",
        "                    if use_pretrain and \"pretrained_emb\" in feature_spec:\n",
        "                        embeddings = self.get_pretrained_embedding(feature_map.data_dir, feature, feature_spec)\n",
        "                        embedding_matrix = self.set_pretrained_embedding(embedding_matrix, \n",
        "                                                                         embeddings, \n",
        "                                                                         freeze=feature_spec[\"freeze_emb\"],\n",
        "                                                                         padding_idx=padding_idx)\n",
        "                    self.embedding_layer[feature] = embedding_matrix\n",
        "                    self.set_sequence_encoder(feature, feature_spec.get(\"encoder\", None))\n",
        "\n",
        "    def is_required(self, feature):\n",
        "        \"\"\" Check whether feature is required for embedding \"\"\"\n",
        "        feature_spec = self._feature_map.feature_specs[feature]\n",
        "        if len(self.required_feature_columns) > 0 and (feature not in self.required_feature_columns):\n",
        "            return False\n",
        "        elif feature in self.not_required_feature_columns:\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "\n",
        "    def set_sequence_encoder(self, feature, encoder):\n",
        "        if encoder is None or encoder in [\"none\", \"null\"]:\n",
        "            self.sequence_encoder.update({feature: None})\n",
        "        elif encoder == \"MaskedAveragePooling\":\n",
        "            self.sequence_encoder.update({feature: MaskedAveragePooling()})\n",
        "        elif encoder == \"MaskedSumPooling\":\n",
        "            self.sequence_encoder.update({feature: MaskedSumPooling()})\n",
        "        else:\n",
        "            raise RuntimeError(\"Sequence encoder={} is not supported.\".format(encoder))\n",
        "\n",
        "    def get_pretrained_embedding(self, data_dir, feature_name, feature_spec):\n",
        "        pretrained_path = os.path.join(data_dir, feature_spec[\"pretrained_emb\"])\n",
        "        with h5py.File(pretrained_path, 'r') as hf:\n",
        "            embeddings = hf[feature_name][:]\n",
        "        return embeddings\n",
        "\n",
        "    def set_pretrained_embedding(self, embedding_matrix, embeddings, freeze=False, padding_idx=None):\n",
        "        if padding_idx is not None:\n",
        "            embeddings[padding_idx] = np.zeros(embeddings.shape[-1])\n",
        "        embeddings = torch.from_numpy(embeddings).float()\n",
        "        embedding_matrix.weight = torch.nn.Parameter(embeddings)\n",
        "        if freeze:\n",
        "            embedding_matrix.weight.requires_grad = False\n",
        "        return embedding_matrix\n",
        "\n",
        "    def dict2tensor(self, embedding_dict, feature_source=None, feature_type=None):\n",
        "        if feature_source is not None:\n",
        "            if not isinstance(feature_source, list):\n",
        "                feature_source = [feature_source]\n",
        "            feature_emb_list = []\n",
        "            for feature, feature_spec in self._feature_map.feature_specs.items():\n",
        "                if feature_spec[\"source\"] in feature_source:\n",
        "                    feature_emb_list.append(embedding_dict[feature])\n",
        "            return torch.stack(feature_emb_list, dim=1)\n",
        "        elif feature_type is not None:\n",
        "            if not isinstance(feature_type, list):\n",
        "                feature_type = [feature_type]\n",
        "            feature_emb_list = []\n",
        "            for feature, feature_spec in self._feature_map.feature_specs.items():\n",
        "                if feature_spec[\"type\"] in feature_type:\n",
        "                    feature_emb_list.append(embedding_dict[feature])\n",
        "            return torch.stack(feature_emb_list, dim=1)\n",
        "        else:\n",
        "            return torch.stack(list(embedding_dict.values()), dim=1)\n",
        "\n",
        "    def forward(self, X):\n",
        "        feature_emb_dict = OrderedDict()\n",
        "        for feature, feature_spec in self._feature_map.feature_specs.items():\n",
        "            if feature in self.embedding_layer:\n",
        "                if feature_spec[\"type\"] == \"numeric\":\n",
        "                    inp = X[:, feature_spec[\"index\"]].float().view(-1, 1)\n",
        "                    embedding_vec = self.embedding_layer[feature](inp)\n",
        "                elif feature_spec[\"type\"] == \"categorical\":\n",
        "                    inp = X[:, feature_spec[\"index\"]].long()\n",
        "                    embedding_vec = self.embedding_layer[feature](inp)\n",
        "                elif feature_spec[\"type\"] == \"sequence\":\n",
        "                    inp = X[:, feature_spec[\"index\"]].long()\n",
        "                    seq_embed_matrix = self.embedding_layer[feature](inp)\n",
        "                    if self.sequence_encoder[feature] is not None:\n",
        "                        embedding_vec = self.sequence_encoder[feature](seq_embed_matrix)\n",
        "                    else:\n",
        "                        embedding_vec = seq_embed_matrix\n",
        "                if feature in self.embedding_hooks:\n",
        "                    embedding_vec = self.embedding_hooks[feature](embedding_vec)\n",
        "                feature_emb_dict[feature] = embedding_vec\n",
        "        return feature_emb_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ItemPosEmbedding"
      ],
      "metadata": {
        "id": "MwDqx-I2H9q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class ItemPosEmbedding(nn.Module):\n",
        "    \"\"\"Construct the embeddings from item, position.\n",
        "\n",
        "    References:\n",
        "        1. https://github.com/RecoHut-Stanzas/STOSA/blob/ee14e2eabcc60922eb52cc7d3231df4954d9ff16/modules.py#L102\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 item_size,\n",
        "                 hidden_size,\n",
        "                 max_seq_length,\n",
        "                 hidden_dropout_prob):\n",
        "        super().__init__()\n",
        "\n",
        "        self.item_embeddings = nn.Embedding(item_size, hidden_size, padding_idx=0)\n",
        "        self.position_embeddings = nn.Embedding(max_seq_length, hidden_size)\n",
        "\n",
        "        self.layernorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "        items_embeddings = self.item_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        embeddings = items_embeddings + position_embeddings\n",
        "        embeddings = self.layernorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "xKedDKtG_fXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_size = 10\n",
        "hidden_size = 5\n",
        "max_seq_length = 20\n",
        "hidden_dropout_prob = 0.2\n",
        "layer = ItemPosEmbedding(item_size, hidden_size, max_seq_length, hidden_dropout_prob)\n",
        "\n",
        "torch.manual_seed(0)\n",
        "x = torch.randint(0,5,(item_size, hidden_size))\n",
        "\n",
        "output = torch.round(layer.forward(x).detach()*1e4)/1e4\n",
        "\n",
        "test_eq(output.shape.numel(), 250)\n",
        "test_eq(list(output.shape), [10, 5, 5])\n",
        "test_eq(round(float(output[5][2][2]), 2), 1.41)"
      ],
      "metadata": {
        "id": "jTL5TnCAAalh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JXKMrEbH43y"
      },
      "source": [
        "> **References**\n",
        "> - https://github.com/xue-pai/FuxiCTR/blob/main/fuxictr/pytorch/layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igcgMNvIH43z",
        "outputId": "e84b45bb-eccf-4f61-82de-78094eb50b38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Author: Sparsh A.\n",
            "\n",
            "Last updated: 2022-01-11 12:16:07\n",
            "\n",
            "Compiler    : GCC 7.5.0\n",
            "OS          : Linux\n",
            "Release     : 5.4.144+\n",
            "Machine     : x86_64\n",
            "Processor   : x86_64\n",
            "CPU cores   : 2\n",
            "Architecture: 64bit\n",
            "\n",
            "torch     : 1.10.0+cu111\n",
            "numpy     : 1.19.5\n",
            "h5py      : 3.1.0\n",
            "PIL       : 7.1.2\n",
            "IPython   : 5.5.0\n",
            "matplotlib: 3.2.2\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#hide\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Sparsh A.\" -m -iv -u -t -d"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "name": "models.layers.embedding.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}