"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[53],{1109:function(e){e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Introduction","href":"/ai/docs/intro","docId":"intro"},{"type":"category","label":"Concept - Basics","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Challenges","href":"/ai/docs/concept-basics/challenges","docId":"concept-basics/challenges"},{"type":"link","label":"Collaborative Filtering","href":"/ai/docs/concept-basics/collaborative-filtering","docId":"concept-basics/collaborative-filtering"},{"type":"link","label":"Evaluation","href":"/ai/docs/concept-basics/evaluation","docId":"concept-basics/evaluation"},{"type":"link","label":"User Feedback","href":"/ai/docs/concept-basics/implicit-feedback","docId":"concept-basics/implicit-feedback"},{"type":"link","label":"Processes","href":"/ai/docs/concept-basics/processes","docId":"concept-basics/processes"},{"type":"link","label":"Session-based Recommenders","href":"/ai/docs/concept-basics/session-based-recommenders","docId":"concept-basics/session-based-recommenders"},{"type":"link","label":"Tasks","href":"/ai/docs/concept-basics/tasks","docId":"concept-basics/tasks"},{"type":"link","label":"Types of Recommender Systems","href":"/ai/docs/concept-basics/types-of-recommender-systems","docId":"concept-basics/types-of-recommender-systems"}],"href":"/ai/docs/concept-basics/"},{"type":"category","label":"Concept - Extras","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Amazon Personalize","href":"/ai/docs/concept-extras/amazon-personalize","docId":"concept-extras/amazon-personalize"},{"type":"link","label":"Apps","href":"/ai/docs/concept-extras/apps","docId":"concept-extras/apps"},{"type":"link","label":"Bias & Fairness","href":"/ai/docs/concept-extras/bias-&-fairness","docId":"concept-extras/bias-&-fairness"},{"type":"link","label":"Causal Inference","href":"/ai/docs/concept-extras/causal-inference","docId":"concept-extras/causal-inference"},{"type":"link","label":"Cold Start","href":"/ai/docs/concept-extras/cold-start","docId":"concept-extras/cold-start"},{"type":"link","label":"Cross-domain","href":"/ai/docs/concept-extras/cross-domain","docId":"concept-extras/cross-domain"},{"type":"link","label":"Data Science","href":"/ai/docs/concept-extras/data-science","docId":"concept-extras/data-science"},{"type":"link","label":"Diversity","href":"/ai/docs/concept-extras/diversity","docId":"concept-extras/diversity"},{"type":"link","label":"Emerging Concepts in Recommender Systems","href":"/ai/docs/concept-extras/emerging-concepts-in-recommender-systems","docId":"concept-extras/emerging-concepts-in-recommender-systems"},{"type":"link","label":"Graph Embeddings","href":"/ai/docs/concept-extras/graph-embeddings","docId":"concept-extras/graph-embeddings"},{"type":"link","label":"Graph Networks","href":"/ai/docs/concept-extras/graph-networks","docId":"concept-extras/graph-networks"},{"type":"link","label":"Incremental Learning","href":"/ai/docs/concept-extras/incremental-learning","docId":"concept-extras/incremental-learning"},{"type":"link","label":"Jensen\u2013Shannon divergence","href":"/ai/docs/concept-extras/jensen-shannon-divergence","docId":"concept-extras/jensen-shannon-divergence"},{"type":"link","label":"Meta Learning","href":"/ai/docs/concept-extras/meta-learning","docId":"concept-extras/meta-learning"},{"type":"link","label":"MLOps","href":"/ai/docs/concept-extras/mlops","docId":"concept-extras/mlops"},{"type":"link","label":"Model Deployment","href":"/ai/docs/concept-extras/model-deployment","docId":"concept-extras/model-deployment"},{"type":"link","label":"Model Retraining","href":"/ai/docs/concept-extras/model-retraining","docId":"concept-extras/model-retraining"},{"type":"link","label":"Multi-Objective Optimization","href":"/ai/docs/concept-extras/multi-objective-optimization","docId":"concept-extras/multi-objective-optimization"},{"type":"link","label":"Multi-Task Learning","href":"/ai/docs/concept-extras/multi-task-learning","docId":"concept-extras/multi-task-learning"},{"type":"link","label":"Multi-task Learning","href":"/ai/docs/concept-extras/multitask-learning","docId":"concept-extras/multitask-learning"},{"type":"category","label":"NLP","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Chatbot","href":"/ai/docs/concept-extras/nlp/chatbot","docId":"concept-extras/nlp/chatbot"},{"type":"link","label":"Language Modeling","href":"/ai/docs/concept-extras/nlp/language-modeling","docId":"concept-extras/nlp/language-modeling"},{"type":"link","label":"Named Entity Recognition","href":"/ai/docs/concept-extras/nlp/named-entity-recognition","docId":"concept-extras/nlp/named-entity-recognition"},{"type":"link","label":"Text Analysis","href":"/ai/docs/concept-extras/nlp/text-analysis","docId":"concept-extras/nlp/text-analysis"},{"type":"link","label":"Text Classification","href":"/ai/docs/concept-extras/nlp/text-classification","docId":"concept-extras/nlp/text-classification"},{"type":"link","label":"Text Generation","href":"/ai/docs/concept-extras/nlp/text-generation","docId":"concept-extras/nlp/text-generation"},{"type":"link","label":"Text Similarity","href":"/ai/docs/concept-extras/nlp/text-similarity","docId":"concept-extras/nlp/text-similarity"},{"type":"link","label":"Text Style Transfer","href":"/ai/docs/concept-extras/nlp/text-style-transfer","docId":"concept-extras/nlp/text-style-transfer"},{"type":"link","label":"Text Summarization","href":"/ai/docs/concept-extras/nlp/text-summarization","docId":"concept-extras/nlp/text-summarization"},{"type":"link","label":"Topic Modeling","href":"/ai/docs/concept-extras/nlp/topic-modeling","docId":"concept-extras/nlp/topic-modeling"},{"type":"link","label":"Transformers","href":"/ai/docs/concept-extras/nlp/transformers","docId":"concept-extras/nlp/transformers"}]},{"type":"link","label":"Off-Policy Learning","href":"/ai/docs/concept-extras/offline-learning","docId":"concept-extras/offline-learning"},{"type":"link","label":"Scalarization","href":"/ai/docs/concept-extras/scalarization","docId":"concept-extras/scalarization"},{"type":"category","label":"Success Stories","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"1mg Prod2vec","href":"/ai/docs/concept-extras/success-stories/1mg-prod2vec","docId":"concept-extras/success-stories/1mg-prod2vec"},{"type":"link","label":"Airbnb Experiences","href":"/ai/docs/concept-extras/success-stories/airbnb-experiences","docId":"concept-extras/success-stories/airbnb-experiences"},{"type":"link","label":"Alibaba","href":"/ai/docs/concept-extras/success-stories/alipay-ctr","docId":"concept-extras/success-stories/alipay-ctr"},{"type":"link","label":"Doordash Contextual Bandit","href":"/ai/docs/concept-extras/success-stories/doordash-contextual-bandit","docId":"concept-extras/success-stories/doordash-contextual-bandit"},{"type":"link","label":"Etsy Personalization","href":"/ai/docs/concept-extras/success-stories/etsy-personalization","docId":"concept-extras/success-stories/etsy-personalization"},{"type":"link","label":"Huawei AppGallery","href":"/ai/docs/concept-extras/success-stories/huawei-appgallery","docId":"concept-extras/success-stories/huawei-appgallery"},{"type":"link","label":"LinkedIn GLMix","href":"/ai/docs/concept-extras/success-stories/linkedin-glmix","docId":"concept-extras/success-stories/linkedin-glmix"},{"type":"link","label":"MarketCloud Real-time","href":"/ai/docs/concept-extras/success-stories/marketcloud-real-time","docId":"concept-extras/success-stories/marketcloud-real-time"},{"type":"link","label":"Netflix Personalize Images","href":"/ai/docs/concept-extras/success-stories/netflix-personalize-images","docId":"concept-extras/success-stories/netflix-personalize-images"},{"type":"link","label":"Pinterest Multi-task Learning","href":"/ai/docs/concept-extras/success-stories/pinterest-multi-task-learning","docId":"concept-extras/success-stories/pinterest-multi-task-learning"},{"type":"link","label":"Santander Banking Products","href":"/ai/docs/concept-extras/success-stories/santander-banking-products","docId":"concept-extras/success-stories/santander-banking-products"},{"type":"link","label":"Scribd Real-time","href":"/ai/docs/concept-extras/success-stories/scribd-real-time","docId":"concept-extras/success-stories/scribd-real-time"},{"type":"link","label":"Spotify Contextual Bandits","href":"/ai/docs/concept-extras/success-stories/spotify-contextual-bandits","docId":"concept-extras/success-stories/spotify-contextual-bandits"},{"type":"link","label":"Spotify RL","href":"/ai/docs/concept-extras/success-stories/spotify-rl","docId":"concept-extras/success-stories/spotify-rl"},{"type":"link","label":"StitchFix Multi-armed Bandit","href":"/ai/docs/concept-extras/success-stories/stitchfix-multi-armed-bandit","docId":"concept-extras/success-stories/stitchfix-multi-armed-bandit"},{"type":"link","label":"Taobao BST","href":"/ai/docs/concept-extras/success-stories/taobao-bst","docId":"concept-extras/success-stories/taobao-bst"},{"type":"link","label":"The Long Tail","href":"/ai/docs/concept-extras/success-stories/the-long-tail","docId":"concept-extras/success-stories/the-long-tail"},{"type":"link","label":"UberEats Personalization","href":"/ai/docs/concept-extras/success-stories/ubereats-personalization","docId":"concept-extras/success-stories/ubereats-personalization"},{"type":"link","label":"Walmart Model Selection","href":"/ai/docs/concept-extras/success-stories/walmart-model-selection","docId":"concept-extras/success-stories/walmart-model-selection"}],"href":"/ai/docs/concept-extras/success-stories/"},{"type":"category","label":"Computer Vision","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Facial Analytics","href":"/ai/docs/concept-extras/vision/facial-analytics","docId":"concept-extras/vision/facial-analytics"},{"type":"link","label":"Image Segmentation","href":"/ai/docs/concept-extras/vision/image-segmentation","docId":"concept-extras/vision/image-segmentation"},{"type":"link","label":"Image Similarity","href":"/ai/docs/concept-extras/vision/image-similarity","docId":"concept-extras/vision/image-similarity"},{"type":"link","label":"Object Detection","href":"/ai/docs/concept-extras/vision/object-detection","docId":"concept-extras/vision/object-detection"},{"type":"link","label":"Object Tracking","href":"/ai/docs/concept-extras/vision/object-tracking","docId":"concept-extras/vision/object-tracking"},{"type":"link","label":"Pose Estimation","href":"/ai/docs/concept-extras/vision/pose-estimation","docId":"concept-extras/vision/pose-estimation"},{"type":"link","label":"Scene Text Recognition","href":"/ai/docs/concept-extras/vision/scene-text-recognition","docId":"concept-extras/vision/scene-text-recognition"},{"type":"link","label":"Video Action Recognition","href":"/ai/docs/concept-extras/vision/video-action-recognition","docId":"concept-extras/vision/video-action-recognition"}]}],"href":"/ai/docs/concept-extras/"},{"type":"category","label":"Models","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"A3C","href":"/ai/docs/models/a3c","docId":"models/a3c"},{"type":"link","label":"AFM","href":"/ai/docs/models/afm","docId":"models/afm"},{"type":"link","label":"AFN","href":"/ai/docs/models/afn","docId":"models/afn"},{"type":"link","label":"AR","href":"/ai/docs/models/ar","docId":"models/ar"},{"type":"link","label":"ASMG","href":"/ai/docs/models/asmg","docId":"models/asmg"},{"type":"link","label":"AttRec","href":"/ai/docs/models/attrec","docId":"models/attrec"},{"type":"link","label":"AUSH","href":"/ai/docs/models/aush","docId":"models/aush"},{"type":"link","label":"AutoInt","href":"/ai/docs/models/autoint","docId":"models/autoint"},{"type":"link","label":"BASR","href":"/ai/docs/models/basr","docId":"models/basr"},{"type":"link","label":"BCQ","href":"/ai/docs/models/bcq","docId":"models/bcq"},{"type":"link","label":"Behavior Propensity Modeling","href":"/ai/docs/models/beh-prop","docId":"models/beh-prop"},{"type":"link","label":"BiasOnly","href":"/ai/docs/models/biasonly","docId":"models/biasonly"},{"type":"link","label":"BigGraph","href":"/ai/docs/models/biggraph","docId":"models/biggraph"},{"type":"link","label":"BPR","href":"/ai/docs/models/bpr","docId":"models/bpr"},{"type":"link","label":"BST","href":"/ai/docs/models/bst","docId":"models/bst"},{"type":"link","label":"CASER","href":"/ai/docs/models/caser","docId":"models/caser"},{"type":"link","label":"CIGC","href":"/ai/docs/models/cigc","docId":"models/cigc"},{"type":"link","label":"CoKE","href":"/ai/docs/models/coke","docId":"models/coke"},{"type":"link","label":"DCN","href":"/ai/docs/models/dcn","docId":"models/dcn"},{"type":"link","label":"DDPG","href":"/ai/docs/models/ddpg","docId":"models/ddpg"},{"type":"link","label":"DeepCross","href":"/ai/docs/models/deepcross","docId":"models/deepcross"},{"type":"link","label":"DeepFM","href":"/ai/docs/models/deepfm","docId":"models/deepfm"},{"type":"link","label":"DeepWalk","href":"/ai/docs/models/deepwalk","docId":"models/deepwalk"},{"type":"link","label":"DGTN","href":"/ai/docs/models/dgtn","docId":"models/dgtn"},{"type":"link","label":"DM","href":"/ai/docs/models/dm","docId":"models/dm"},{"type":"link","label":"DMT","href":"/ai/docs/models/dmt","docId":"models/dmt"},{"type":"link","label":"DPADL","href":"/ai/docs/models/dpadl","docId":"models/dpadl"},{"type":"link","label":"DQN","href":"/ai/docs/models/dqn","docId":"models/dqn"},{"type":"link","label":"DRQN","href":"/ai/docs/models/drqn","docId":"models/drqn"},{"type":"link","label":"DRR","href":"/ai/docs/models/drr","docId":"models/drr"},{"type":"link","label":"Dueling DQN","href":"/ai/docs/models/dueling-dqn","docId":"models/dueling-dqn"},{"type":"link","label":"FFM","href":"/ai/docs/models/ffm","docId":"models/ffm"},{"type":"link","label":"FGNN","href":"/ai/docs/models/fgnn","docId":"models/fgnn"},{"type":"link","label":"FM","href":"/ai/docs/models/fm","docId":"models/fm"},{"type":"link","label":"GAT","href":"/ai/docs/models/gat","docId":"models/gat"},{"type":"link","label":"GC-SAN","href":"/ai/docs/models/gc-san","docId":"models/gc-san"},{"type":"link","label":"GCE-GNN","href":"/ai/docs/models/gce-gnn","docId":"models/gce-gnn"},{"type":"link","label":"GLMix","href":"/ai/docs/models/glmix","docId":"models/glmix"},{"type":"link","label":"GRU4Rec","href":"/ai/docs/models/gru4rec","docId":"models/gru4rec"},{"type":"link","label":"HMLET","href":"/ai/docs/models/hmlet","docId":"models/hmlet"},{"type":"link","label":"IncCTR","href":"/ai/docs/models/incctr","docId":"models/incctr"},{"type":"link","label":"IPW","href":"/ai/docs/models/ipw","docId":"models/ipw"},{"type":"link","label":"ItemPop","href":"/ai/docs/models/itempop","docId":"models/itempop"},{"type":"link","label":"KHGT","href":"/ai/docs/models/khgt","docId":"models/khgt"},{"type":"link","label":"LESSR","href":"/ai/docs/models/lessr","docId":"models/lessr"},{"type":"link","label":"LightFM WARP","href":"/ai/docs/models/lightfm-warp","docId":"models/lightfm-warp"},{"type":"link","label":"LightGCN","href":"/ai/docs/models/lightgcn","docId":"models/lightgcn"},{"type":"link","label":"LINE","href":"/ai/docs/models/line","docId":"models/line"},{"type":"link","label":"LIRD","href":"/ai/docs/models/lird","docId":"models/lird"},{"type":"link","label":"Markov Chains","href":"/ai/docs/models/markov-chains","docId":"models/markov-chains"},{"type":"link","label":"MATN","href":"/ai/docs/models/matn","docId":"models/matn"},{"type":"link","label":"MB-GMN","href":"/ai/docs/models/mb-gmn","docId":"models/mb-gmn"},{"type":"link","label":"MDP","href":"/ai/docs/models/mdp","docId":"models/mdp"},{"type":"link","label":"MF","href":"/ai/docs/models/mf","docId":"models/mf"},{"type":"link","label":"MIAN","href":"/ai/docs/models/mian","docId":"models/mian"},{"type":"link","label":"MMoE","href":"/ai/docs/models/mmoe","docId":"models/mmoe"},{"type":"link","label":"MPNN","href":"/ai/docs/models/mpnn","docId":"models/mpnn"},{"type":"link","label":"NeuMF","href":"/ai/docs/models/neumf","docId":"models/neumf"},{"type":"link","label":"NGCF","href":"/ai/docs/models/ngcf","docId":"models/ngcf"},{"type":"link","label":"NMRN","href":"/ai/docs/models/nmrn","docId":"models/nmrn"},{"type":"link","label":"Node2vec","href":"/ai/docs/models/node2vec","docId":"models/node2vec"},{"type":"link","label":"PNN","href":"/ai/docs/models/pnn","docId":"models/pnn"},{"type":"link","label":"PPO","href":"/ai/docs/models/ppo","docId":"models/ppo"},{"type":"link","label":"Q-learning","href":"/ai/docs/models/q-learning","docId":"models/q-learning"},{"type":"link","label":"Random Walk","href":"/ai/docs/models/random_walk","docId":"models/random_walk"},{"type":"link","label":"RKMF","href":"/ai/docs/models/rkmf","docId":"models/rkmf"},{"type":"link","label":"SAC","href":"/ai/docs/models/sac","docId":"models/sac"},{"type":"link","label":"SARSA","href":"/ai/docs/models/sarsa","docId":"models/sarsa"},{"type":"link","label":"SASRec","href":"/ai/docs/models/sasrec","docId":"models/sasrec"},{"type":"link","label":"SDNE","href":"/ai/docs/models/sdne","docId":"models/sdne"},{"type":"link","label":"SGL","href":"/ai/docs/models/sgl","docId":"models/sgl"},{"type":"link","label":"Shared Bottom","href":"/ai/docs/models/shared-bottom","docId":"models/shared-bottom"},{"type":"link","label":"SiReN","href":"/ai/docs/models/siren","docId":"models/siren"},{"type":"link","label":"SLIST","href":"/ai/docs/models/slist","docId":"models/slist"},{"type":"link","label":"SML","href":"/ai/docs/models/sml","docId":"models/sml"},{"type":"link","label":"SPop","href":"/ai/docs/models/spop","docId":"models/spop"},{"type":"link","label":"SR-GNN","href":"/ai/docs/models/sr-gnn","docId":"models/sr-gnn"},{"type":"link","label":"SR-SAN","href":"/ai/docs/models/sr-san","docId":"models/sr-san"},{"type":"link","label":"SSE-PT","href":"/ai/docs/models/sse-pt","docId":"models/sse-pt"},{"type":"link","label":"STAMP","href":"/ai/docs/models/stamp","docId":"models/stamp"},{"type":"link","label":"Struc2Vec","href":"/ai/docs/models/struc2vec","docId":"models/struc2vec"},{"type":"link","label":"SVAE","href":"/ai/docs/models/svae","docId":"models/svae"},{"type":"link","label":"TAaMR","href":"/ai/docs/models/taamr","docId":"models/taamr"},{"type":"link","label":"TAGNN-PP","href":"/ai/docs/models/tagnn-pp","docId":"models/tagnn-pp"},{"type":"link","label":"TAGNN","href":"/ai/docs/models/tagnn","docId":"models/tagnn"},{"type":"link","label":"TGIN","href":"/ai/docs/models/tgin","docId":"models/tgin"},{"type":"link","label":"VNCF","href":"/ai/docs/models/vncf","docId":"models/vncf"},{"type":"link","label":"VSKNN","href":"/ai/docs/models/vsknn","docId":"models/vsknn"},{"type":"link","label":"Wide and Deep","href":"/ai/docs/models/wide-and-deep","docId":"models/wide-and-deep"},{"type":"link","label":"Word2vec","href":"/ai/docs/models/word2vec","docId":"models/word2vec"},{"type":"link","label":"xDeepFM","href":"/ai/docs/models/xdeepfm","docId":"models/xdeepfm"}],"href":"/ai/docs/models/"},{"type":"category","label":"Tutorials","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Amazon Personalize","href":"/ai/docs/tutorials/amazon-personalize","docId":"tutorials/amazon-personalize"},{"type":"link","label":"Database Connections","href":"/ai/docs/tutorials/database-conn","docId":"tutorials/database-conn"},{"type":"link","label":"Email Classification","href":"/ai/docs/tutorials/email-classification","docId":"tutorials/email-classification"},{"type":"link","label":"Graph-based Modeling","href":"/ai/docs/tutorials/graph","docId":"tutorials/graph"},{"type":"link","label":"Mathematics","href":"/ai/docs/tutorials/mathematics","docId":"tutorials/mathematics"},{"type":"link","label":"Matrix Factorizations","href":"/ai/docs/tutorials/matrix-factorizations","docId":"tutorials/matrix-factorizations"},{"type":"link","label":"MLOps","href":"/ai/docs/tutorials/mlops","docId":"tutorials/mlops"},{"type":"link","label":"Multi-Touch Attribution","href":"/ai/docs/tutorials/multi-touch-attribution","docId":"tutorials/multi-touch-attribution"},{"type":"link","label":"Name and Address Parsing","href":"/ai/docs/tutorials/name-address-parsing","docId":"tutorials/name-address-parsing"},{"type":"link","label":"Negative Implicit Feedback in Recommendations","href":"/ai/docs/tutorials/negative-implicit-feedback-rec","docId":"tutorials/negative-implicit-feedback-rec"},{"type":"link","label":"PDF to WordCloud via Email","href":"/ai/docs/tutorials/pdf-to-wordcloud-mail","docId":"tutorials/pdf-to-wordcloud-mail"},{"type":"link","label":"Recommender System Evaluation","href":"/ai/docs/tutorials/recsys-evaluation","docId":"tutorials/recsys-evaluation"},{"type":"link","label":"Resale Price Prediction","href":"/ai/docs/tutorials/resale-price-prediction","docId":"tutorials/resale-price-prediction"},{"type":"link","label":"Vector Search","href":"/ai/docs/tutorials/vector-search","docId":"tutorials/vector-search"},{"type":"link","label":"Word2vec","href":"/ai/docs/tutorials/word2vec","docId":"tutorials/word2vec"}]},{"type":"category","label":"Snippets","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Python code","href":"/ai/docs/snippets/python-snippets","docId":"snippets/python-snippets"},{"type":"link","label":"Unix shell","href":"/ai/docs/snippets/unix-shell-snippets","docId":"snippets/unix-shell-snippets"}]},{"type":"category","label":"Tools","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"OBP","href":"/ai/docs/tools/obp","docId":"tools/obp"},{"type":"link","label":"Pachyderm","href":"/ai/docs/tools/pachyderm","docId":"tools/pachyderm"},{"type":"link","label":"PySpark","href":"/ai/docs/tools/pyspark","docId":"tools/pyspark"},{"type":"link","label":"River","href":"/ai/docs/tools/river","docId":"tools/river"}],"href":"/ai/docs/tools/"},{"type":"link","label":"Datasets","href":"/ai/docs/datasets","docId":"datasets"},{"type":"link","label":"Notebooks","href":"/ai/docs/notebooks","docId":"notebooks"},{"type":"link","label":"Projects","href":"/ai/docs/projects","docId":"projects"}]},"docs":{"concept-basics/challenges":{"id":"concept-basics/challenges","title":"Challenges","description":"The construction of effective Recommender Systems (RS) is a complex process, mainly due to the nature of RSs which involves large scale software-systems and human interactions. Iterative development processes require deep understanding of a current baseline as well as the ability to estimate the impact of changes in multiple variables of interest. Simulations are well suited to address both challenges and potentially leading to a high velocity construction process, a fundamental requirement in commercial contexts. Recently, there has been significant interest in RS Simulation Platforms, which allow RS developers to easily craft simulated environments where their systems can be analyzed.","sidebar":"tutorialSidebar"},"concept-basics/collaborative-filtering":{"id":"concept-basics/collaborative-filtering","title":"Collaborative Filtering","description":"Similarity methods","sidebar":"tutorialSidebar"},"concept-basics/concept-basics":{"id":"concept-basics/concept-basics","title":"Concept - Basics","description":"In the following sections, we will more systematically introduce the following concepts:","sidebar":"tutorialSidebar"},"concept-basics/evaluation":{"id":"concept-basics/evaluation","title":"Evaluation","description":"Online vs Offline Evaluation","sidebar":"tutorialSidebar"},"concept-basics/implicit-feedback":{"id":"concept-basics/implicit-feedback","title":"User Feedback","description":"Explicit vs. implicit users feedback","sidebar":"tutorialSidebar"},"concept-basics/processes":{"id":"concept-basics/processes","title":"Processes","description":"Retrieval and Ranking","sidebar":"tutorialSidebar"},"concept-basics/session-based-recommenders":{"id":"concept-basics/session-based-recommenders","title":"Session-based Recommenders","description":"Recommender systems help users find relevant items of interest, for example on e-commerce or media streaming sites. Most academic research is concerned with approaches that personalize the recommendations according to long-term user profiles. In many real-world applications, however, such long-term profiles often do not exist and recommendations, therefore, have to be made solely based on the observed behavior of a user during an ongoing session.","sidebar":"tutorialSidebar"},"concept-basics/tasks":{"id":"concept-basics/tasks","title":"Tasks","description":"Top-K Recommendation","sidebar":"tutorialSidebar"},"concept-basics/types-of-recommender-systems":{"id":"concept-basics/types-of-recommender-systems","title":"Types of Recommender Systems","description":"Group Recommender System","sidebar":"tutorialSidebar"},"concept-extras/amazon-personalize":{"id":"concept-extras/amazon-personalize","title":"Amazon Personalize","description":"This page is a random dump of my notes on Amazon Personalize. Proceed accordingly.","sidebar":"tutorialSidebar"},"concept-extras/apps":{"id":"concept-extras/apps","title":"Apps","description":"Streamlit","sidebar":"tutorialSidebar"},"concept-extras/bias-&-fairness":{"id":"concept-extras/bias-&-fairness","title":"Bias & Fairness","description":"It can\u2019t be denied that there is bias all around us. A bias is a prejudice against a person or group of people, including, but not limited to their gender, race, and beliefs. Many of these biases arise from emergent behavior in social interactions, events in history, and cultural and political views around the world. These biases affect the data that we collect. Because AI algorithms work with this data, it is an inherent problem that the machine will \u201clearn\u201d these biases. From a technical perspective, we can engineer the system perfectly, but at the end of the day, humans interact with these systems, and it\u2019s our responsibility to minimize bias and prejudice as much as possible. The algorithms we use are only as good as the data provided to them. Understanding the data and the context in which it is being used is the first step in battling bias, and this understanding will help you build better solutions\u2014because you will be well versed in the problem space. Providing balanced data with as little bias as possible should result in better solutions.","sidebar":"tutorialSidebar"},"concept-extras/causal-inference":{"id":"concept-extras/causal-inference","title":"Causal Inference","description":"Typical recommender systems frame the recommendation task as either a distance learning problem between pairs of products, or between pairs of users and products, or as a next item prediction problem. However, a recommender system should not only attempt to model organic user behavior but influence it. This is where causal techniques help, potentially via simple modifications of standard matrix factorization methods.","sidebar":"tutorialSidebar"},"concept-extras/cold-start":{"id":"concept-extras/cold-start","title":"Cold Start","description":"One long-standing challenge for Collaborative Filtering (CF) based recommendation methods is the cold start problem, i.e., to provide recommendations for new users or items who have no historical interaction record. The cold start problem is common in real world applications. For example, 500 hours of new videos are uploaded to YouTube every minute, 500,000 new users register in Facebook every day, and web/mobile apps face the daily challenge of onboarding new users and subscribers. To provide recommendations for these new users and items, many content-based methods and heuristic methods have been deployed, e.g., recommending popular items or geographically near items. However, recent research efforts that tackle the cold start problem from the perspective of machine learning have made promising strides.","sidebar":"tutorialSidebar"},"concept-extras/concept-extras":{"id":"concept-extras/concept-extras","title":"Concept - Extras","description":"In the following sections, we will more systematically introduce the following extra concepts:","sidebar":"tutorialSidebar"},"concept-extras/cross-domain":{"id":"concept-extras/cross-domain","title":"Cross-domain","description":"A common challenge for most current recommender systems is the cold-start problem. Due to the lack of user-item interactions, the fine-tuned recommender systems are unable to handle situations with new users or new items. Recently, some works introduce the meta-optimization idea into the recommendation scenarios, i.e. predicting the user preference by only a few of past interacted items. The core idea is learning a global sharing initialization parameter for all users and then learning the local parameters for each user separately. However, most meta-learning based recommendation approaches adopt model-agnostic meta-learning for parameter initialization, where the global sharing parameter may lead the model into local optima for some users.","sidebar":"tutorialSidebar"},"concept-extras/data-science":{"id":"concept-extras/data-science","title":"Data Science","description":"Data science is used in a variety of ways. Some data scientists focus on the analytics side of things, pulling out hidden patterns and insights from data, then communicating these results with visualizations and statistics. Others work on creating predictive models in order to predict future events, such as predicting whether someone will put solar panels on their house. Yet others work on models for classification; for example, classifying the make and model of a car in an image. One thing ties all applications of data science together: the data. Anywhere you have enough data, you can use data science to accomplish things that seem like magic to the casual observer.","sidebar":"tutorialSidebar"},"concept-extras/diversity":{"id":"concept-extras/diversity","title":"Diversity","description":"Individual-level diversity and System-level diversity","sidebar":"tutorialSidebar"},"concept-extras/emerging-concepts-in-recommender-systems":{"id":"concept-extras/emerging-concepts-in-recommender-systems","title":"Emerging Concepts in Recommender Systems","description":"Real-time Learning and Inference","sidebar":"tutorialSidebar"},"concept-extras/graph-embeddings":{"id":"concept-extras/graph-embeddings","title":"Graph Embeddings","description":"Due to their nature, graphs can be analyzed at different levels of granularity: at the node, edge, and graph level (the whole graph), as depicted in the following figure. For each of those levels, different problems could be faced and, as a consequence, specific algorithms should be used.","sidebar":"tutorialSidebar"},"concept-extras/graph-networks":{"id":"concept-extras/graph-networks","title":"Graph Networks","description":"Item-Item Co-Occurrence Graph","sidebar":"tutorialSidebar"},"concept-extras/incremental-learning":{"id":"concept-extras/incremental-learning","title":"Incremental Learning","description":"Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference.","sidebar":"tutorialSidebar"},"concept-extras/jensen-shannon-divergence":{"id":"concept-extras/jensen-shannon-divergence","title":"Jensen\u2013Shannon divergence","description":"In\xa0probability theory\xa0and\xa0statistics, the\xa0**Jensen)\u2013Shannon\xa0divergence\xa0is a method of measuring the similarity between two\xa0probability distributions. It is also known as\xa0information radius\xa0(IRad)[1]\xa0or\xa0total divergence to the average.[2]\xa0It is based on the\xa0Kullback\u2013Leibler divergence, with some notable (and useful) differences, including that it is symmetric and it always has a finite value. The square root of the Jensen\u2013Shannon divergence is a\xa0metric)\xa0often referred to as Jensen-Shannon distance.","sidebar":"tutorialSidebar"},"concept-extras/meta-learning":{"id":"concept-extras/meta-learning","title":"Meta Learning","description":"Meta learning covers a wide range of topics and has contributed to a booming study trend. Few-shot learning is one of successful branches of meta learning. We retrospect some representative meta-learning models with strong connections to our work.","sidebar":"tutorialSidebar"},"concept-extras/mlops":{"id":"concept-extras/mlops","title":"MLOps","description":"The boom in AI has seen a rising demand for better AI infrastructure \u2014 both in the compute hardware layer and AI framework optimizations that make optimal use of accelerated compute. Unfortunately, organizations often overlook the critical importance of a middle tier: infrastructure software that standardizes the ML life cycle, adding a common platform for teams of data scientists and researchers to standardize their approach and eliminate distracting DevOps work. This process of building the ML life cycle is increasingly known as MLOps, with end-to-end platforms being built to automate and standardize repeatable manual processes. Although dozens of MLOps platforms exist, adopting one can be confusing and cumbersome. What should be considered when employing MLOps? What are the core pillars to MLOps, and which features are most critical?","sidebar":"tutorialSidebar"},"concept-extras/model-deployment":{"id":"concept-extras/model-deployment","title":"Model Deployment","description":"modeldeployment","sidebar":"tutorialSidebar"},"concept-extras/model-retraining":{"id":"concept-extras/model-retraining","title":"Model Retraining","description":"Concept Drift","sidebar":"tutorialSidebar"},"concept-extras/multi-objective-optimization":{"id":"concept-extras/multi-objective-optimization","title":"Multi-Objective Optimization","description":"Recommender systems have been widely applied to several domains and applications. Traditional recommender systems usually deal with a single objective, such as minimizing the prediction errors or maximizing the ranking of the recommendation list. There is an emerging demand for multi-objective optimization so that the development of recommendation models can take multiple objectives into consideration, especially in the area of multi-stakeholder and multi-task recommender systems.","sidebar":"tutorialSidebar"},"concept-extras/multi-task-learning":{"id":"concept-extras/multi-task-learning","title":"Multi-Task Learning","description":"","sidebar":"tutorialSidebar"},"concept-extras/multitask-learning":{"id":"concept-extras/multitask-learning","title":"Multi-task Learning","description":"Generally, as soon as you find yourself optimizing more than one loss function, you are effectively doing multi-task learning (in contrast to single-task learning). In those scenarios, it helps to think about what you are trying to do explicitly in terms of MTL and to draw insights from it.","sidebar":"tutorialSidebar"},"concept-extras/nlp/chatbot":{"id":"concept-extras/nlp/chatbot","title":"Chatbot","description":"/img/content-concepts-raw-nlp-chatbot-img.png","sidebar":"tutorialSidebar"},"concept-extras/nlp/language-modeling":{"id":"concept-extras/nlp/language-modeling","title":"Language Modeling","description":"Language Models (LMs) estimate the probability of different linguistic units: symbols, tokens, token sequences.","sidebar":"tutorialSidebar"},"concept-extras/nlp/named-entity-recognition":{"id":"concept-extras/nlp/named-entity-recognition","title":"Named Entity Recognition","description":"/img/content-concepts-raw-nlp-named-entity-recognition-img.png","sidebar":"tutorialSidebar"},"concept-extras/nlp/text-analysis":{"id":"concept-extras/nlp/text-analysis","title":"Text Analysis","description":"Rapid text analysis can save lives. Let\u2019s consider a real-world incident when US soldiers stormed a terrorist compound. In the compound, they discovered a computer containing terabytes of archived data. The data included documents, text messages, and emails pertaining to terrorist activities. The documents were too numerous to be read by any single human being. Fortunately, the soldiers were equipped with special software that could perform very fast text analysis. The software allowed the soldiers to process all of the text data without even having to leave the compound. The onsite analysis immediately revealed an active terrorist plot in a nearby neighborhood. The soldiers instantly responded to the plot and prevented a terrorist attack.","sidebar":"tutorialSidebar"},"concept-extras/nlp/text-classification":{"id":"concept-extras/nlp/text-classification","title":"Text Classification","description":"/img/content-concepts-raw-text-classification-img.png","sidebar":"tutorialSidebar"},"concept-extras/nlp/text-generation":{"id":"concept-extras/nlp/text-generation","title":"Text Generation","description":"Natural language generation (NLG) can actually tell a story \u2013 exactly like that of a human analyst \u2013 by writing the sentences and paragraphs for you. It can also summarize reports.","sidebar":"tutorialSidebar"},"concept-extras/nlp/text-similarity":{"id":"concept-extras/nlp/text-similarity","title":"Text Similarity","description":"/img/content-concepts-raw-nlp-text-similarity-img.png","sidebar":"tutorialSidebar"},"concept-extras/nlp/text-style-transfer":{"id":"concept-extras/nlp/text-style-transfer","title":"Text Style Transfer","description":"How to adapt the text to different situations, audiences and purposes by making some changes? The style of the text usually includes many aspects such as morphology, grammar, emotion, complexity, fluency, tense, tone and so on.","sidebar":"tutorialSidebar"},"concept-extras/nlp/text-summarization":{"id":"concept-extras/nlp/text-summarization","title":"Text Summarization","description":"/img/content-concepts-raw-nlp-text-summarization-untitled.png","sidebar":"tutorialSidebar"},"concept-extras/nlp/topic-modeling":{"id":"concept-extras/nlp/topic-modeling","title":"Topic Modeling","description":"/img/content-concepts-raw-nlp-topic-modeling-img.png","sidebar":"tutorialSidebar"},"concept-extras/nlp/transformers":{"id":"concept-extras/nlp/transformers","title":"Transformers","description":"The old, obsolete, 1980 architecture of Recurrent Neural Networks(RNNs) including the LSTMs were simply not producing good results anymore. In less than two years, transformer models wiped RNNs off the map and even outperformed human baselines for many tasks.","sidebar":"tutorialSidebar"},"concept-extras/offline-learning":{"id":"concept-extras/offline-learning","title":"Off-Policy Learning","description":"Offline Reinforcement Learning","sidebar":"tutorialSidebar"},"concept-extras/scalarization":{"id":"concept-extras/scalarization","title":"Scalarization","description":"| Method | Idea | Scalarization | Characteristic |","sidebar":"tutorialSidebar"},"concept-extras/success-stories/1mg-prod2vec":{"id":"concept-extras/success-stories/1mg-prod2vec","title":"1mg Prod2vec","description":"Gurgaon (India) based 1mg is an online pharmacy and healthcare platform that offers medicines, lab tests, and doctor consultations. Launched in 2013 as Healthkartplus, 1mg initially focussed on the alternative medicine space with AYUSH products. Over the years, it rebranded itself as 1mg which is an online pharmacy and healthcare platform that offers medicines, lab tests, and doctor consultations. Today, 1mg provides a wide range of healthcare services. 1mg also provides information on medicines. It facilitates lab tests at home. At present, the platform has about 2,000 tests and 120 verified labs listed and users can consult a doctor across 20 specialties. The company earns from its services like diagnostics, sale of medicines, preventive healthcare, and online consultations, as well as through native ads on its platform.","sidebar":"tutorialSidebar"},"concept-extras/success-stories/airbnb-experiences":{"id":"concept-extras/success-stories/airbnb-experiences","title":"Airbnb Experiences","description":"Machine Learning-Powered Search Ranking of Airbnb Experiences","sidebar":"tutorialSidebar"},"concept-extras/success-stories/alipay-ctr":{"id":"concept-extras/success-stories/alipay-ctr","title":"Alibaba","description":"Alipay CTR","sidebar":"tutorialSidebar"},"concept-extras/success-stories/doordash-contextual-bandit":{"id":"concept-extras/success-stories/doordash-contextual-bandit","title":"Doordash Contextual Bandit","description":"Read here on Doordash\'s blog","sidebar":"tutorialSidebar"},"concept-extras/success-stories/etsy-personalization":{"id":"concept-extras/success-stories/etsy-personalization","title":"Etsy Personalization","description":"Two-sided marketplaces such as eBay, Etsy and Taobao have two distinct groups of customers: buyers who use the platform to seek the most relevant and interesting item to purchase and sellers who view the same platform as a tool to reach out to their audience and grow their business. Additionally, platforms have their own objectives ranging from growing both buyer and seller user bases to revenue maximization. It is not difficult to see that it would be challenging to obtain a globally favorable outcome for all parties. Taking the search experience as an example, any interventions are likely to impact either buyers or sellers unfairly to course correct for a greater perceived need.","sidebar":"tutorialSidebar"},"concept-extras/success-stories/huawei-appgallery":{"id":"concept-extras/success-stories/huawei-appgallery","title":"Huawei AppGallery","description":"Untitled","sidebar":"tutorialSidebar"},"concept-extras/success-stories/linkedin-glmix":{"id":"concept-extras/success-stories/linkedin-glmix","title":"LinkedIn GLMix","description":"A snapshot of the LinkedIn jobs homepage.","sidebar":"tutorialSidebar"},"concept-extras/success-stories/marketcloud-real-time":{"id":"concept-extras/success-stories/marketcloud-real-time","title":"MarketCloud Real-time","description":"Model Training and Deployment","sidebar":"tutorialSidebar"},"concept-extras/success-stories/netflix-personalize-images":{"id":"concept-extras/success-stories/netflix-personalize-images","title":"Netflix Personalize Images","description":"Read here on Netflix\'s blog","sidebar":"tutorialSidebar"},"concept-extras/success-stories/pinterest-multi-task-learning":{"id":"concept-extras/success-stories/pinterest-multi-task-learning","title":"Pinterest Multi-task Learning","description":"Multi-task Learning for Related Products Recommendations at Pinterest","sidebar":"tutorialSidebar"},"concept-extras/success-stories/santander-banking-products":{"id":"concept-extras/success-stories/santander-banking-products","title":"Santander Banking Products","description":"The goal of the bank is to predict which new products customers will purchase. The data starts on 2015\u201301\u201328, and has monthly records of the products each customer has, such as a credit card, savings account, etc. In addition, the dataset also records user personal data such as average income, age, gender, and so on. The monthly statistics are provided until 2015\u201305\u201328. Finally, the model predicts which additional products a customer will start using from the following month, 2016\u201306\u201328. Thus, the dataset spans 17 months from 2015\u201301\u201328 to 2016\u201305\u201328, and the output set contains only the timestamp corresponding to 2016\u201306\u201328. Models are therefore trained on sequences of 16 months to predict products acquired on their respective last month.","sidebar":"tutorialSidebar"},"concept-extras/success-stories/scribd-real-time":{"id":"concept-extras/success-stories/scribd-real-time","title":"Scribd Real-time","description":"Transformer based model architecture can be applied to recommendation applications as well but recommendation problems are a bit more complex than NLP domain so it needs to be adapted according to the business needs. Therefore, instead of predicting next word based on the past sequence of words, at Scribd, we are interested in predicting what user would like to read next based on rich user interaction history with multiple types of documents and multiple types of interactions, where position in sequence & relative time are both important factors.","sidebar":"tutorialSidebar"},"concept-extras/success-stories/spotify-contextual-bandits":{"id":"concept-extras/success-stories/spotify-contextual-bandits","title":"Spotify Contextual Bandits","description":"Read more in this paper","sidebar":"tutorialSidebar"},"concept-extras/success-stories/spotify-rl":{"id":"concept-extras/success-stories/spotify-rl","title":"Spotify RL","description":"https://towardsdatascience.com/recommendation-system-with-reinforcement-learning-3362cb4422c8","sidebar":"tutorialSidebar"},"concept-extras/success-stories/stitchfix-multi-armed-bandit":{"id":"concept-extras/success-stories/stitchfix-multi-armed-bandit","title":"StitchFix Multi-armed Bandit","description":"Read more on official blog","sidebar":"tutorialSidebar"},"concept-extras/success-stories/success-stories":{"id":"concept-extras/success-stories/success-stories","title":"Success Stories","description":"In the following sections, we will learn more about the following success stories:","sidebar":"tutorialSidebar"},"concept-extras/success-stories/taobao-bst":{"id":"concept-extras/success-stories/taobao-bst","title":"Taobao BST","description":"The BST has been deployed in rank stage for Taobao recommendation, which provides recommending service for hundreds of millions of consumers everyday. In this paper, Alibaba described its usage and technical details in Taobao platform.","sidebar":"tutorialSidebar"},"concept-extras/success-stories/the-long-tail":{"id":"concept-extras/success-stories/the-long-tail","title":"The Long Tail","description":"Here is an excerpt from the book The Long Tail by Chris Anderson: \\"In 1988, a British mountain climber named Joe Simpson wrote a book called Touching the Void, a harrowing account of near-death in the Peruvian Andes. It got good reviews but, only a modest success, it was soon forgotten. Then, a decade later, a strange thing happened. Jon Krakauer wrote Into Thin Air, another book about a mountain-c\xe4mbing tragedy, which became a publishing sensation. Suddenly Touching the Void started to sell again\\".","sidebar":"tutorialSidebar"},"concept-extras/success-stories/ubereats-personalization":{"id":"concept-extras/success-stories/ubereats-personalization","title":"UberEats Personalization","description":"Food Discovery with Uber Eats: Recommending for the Marketplace","sidebar":"tutorialSidebar"},"concept-extras/success-stories/walmart-model-selection":{"id":"concept-extras/success-stories/walmart-model-selection","title":"Walmart Model Selection","description":"In this paper, Walmart researchers briefly discussed their approach and experiment details.","sidebar":"tutorialSidebar"},"concept-extras/vision/facial-analytics":{"id":"concept-extras/vision/facial-analytics","title":"Facial Analytics","description":"/img/content-concepts-raw-computer-vision-facial-analytics-img.png","sidebar":"tutorialSidebar"},"concept-extras/vision/image-segmentation":{"id":"concept-extras/vision/image-segmentation","title":"Image Segmentation","description":"/img/content-concepts-raw-computer-vision-image-segmentation-slide46.png","sidebar":"tutorialSidebar"},"concept-extras/vision/image-similarity":{"id":"concept-extras/vision/image-similarity","title":"Image Similarity","description":"/img/content-concepts-raw-computer-vision-image-similarity-slide19.png","sidebar":"tutorialSidebar"},"concept-extras/vision/object-detection":{"id":"concept-extras/vision/object-detection","title":"Object Detection","description":"/img/content-concepts-raw-computer-vision-object-detection-slide29.png","sidebar":"tutorialSidebar"},"concept-extras/vision/object-tracking":{"id":"concept-extras/vision/object-tracking","title":"Object Tracking","description":"/img/content-concepts-raw-computer-vision-object-tracking-img.png","sidebar":"tutorialSidebar"},"concept-extras/vision/pose-estimation":{"id":"concept-extras/vision/pose-estimation","title":"Pose Estimation","description":"/img/content-concepts-raw-computer-vision-pose-estimation-slide52.png","sidebar":"tutorialSidebar"},"concept-extras/vision/scene-text-recognition":{"id":"concept-extras/vision/scene-text-recognition","title":"Scene Text Recognition","description":"/img/content-concepts-raw-computer-vision-scene-text-recognition-img.png","sidebar":"tutorialSidebar"},"concept-extras/vision/video-action-recognition":{"id":"concept-extras/vision/video-action-recognition","title":"Video Action Recognition","description":"/img/content-concepts-raw-computer-vision-video-action-recognition-img.png","sidebar":"tutorialSidebar"},"datasets":{"id":"datasets","title":"Datasets","description":"| Title | Link | Description |","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"Introduction","description":"It is evident that the pace that technology advances have been increased over the last decades. Scientific discoveries and technological growth introduced to people a huge variety of options and possibilities. One of the most important advantages that technology offers is the direct and easy access to information. Nowadays access to vast networks of information is easy and people can be informed about almost anything they desire. Even though ease of access provided people with the ability to acquire the needed information, they are now facing a new obstacle: this of easily finding what they need. On one hand, information abundance covers the majority of needs but on the other hinders accessibility to information truly valuable to the user. The term that describes this phenomenon is \u201cInformation Overload\u201d. Often users are presented with seemingly similar information to their inquiry but irrelevant to their actual needs, rendering this way the discovery of the desired knowledge a difficult task. Continuous expanding of information overload necessitated the development of systems that aim to alleviate such problems. Such systems were introduced in order to filter or retrieve the desired information. Recommendation systems is an example. Recommenders aim to filter out all the unnecessary and irrelevant information and present those that fit the user\u2019s needs. This way the user is relieved of the burden of discovering what he needs making this way information truly accessible.","sidebar":"tutorialSidebar"},"models/a3c":{"id":"models/a3c","title":"A3C","description":"A3C stands for Asynchronous Advantage Actor-Critic. The A3C algorithm builds upon the Actor-Critic class of algorithms by using a neural network to approximate the actor (and critic). The actor learns the policy function using a deep neural network, while the critic estimates the value function. The asynchronous nature of the algorithm allows the agent to learn from different parts of the state space, allowing parallel learning and faster convergence. Unlike DQN agents, which use an experience replay memory, the A3C agent uses multiple workers to gather more samples for learning.","sidebar":"tutorialSidebar"},"models/afm":{"id":"models/afm","title":"AFM","description":"AFM stands for Attentional Factorization Machines. It Improves FM by discriminating the importance of different feature interactions, and learns the importance of each feature interaction from data via a neural attention network. Empirically, it is shown on regression task that AFM performs betters than FM with a 8.6% relative improvement, and consistently outperforms the state-of-the-art deep learning methods Wide&Deep and DeepCross with a much simpler structure and fewer model parameters.","sidebar":"tutorialSidebar"},"models/afn":{"id":"models/afn","title":"AFN","description":"AFN stands for Adaptive Factorization Network.","sidebar":"tutorialSidebar"},"models/ar":{"id":"models/ar","title":"AR","description":"Simple Association Rules (AR) are a simplified version of the association rule mining technique [Agrawal et al. 1993] with a maximum rule size of two. The method is designed to capture the frequency of two co-occurring events, e.g., \u201cCustomers who bought . . . also bought\u201d.","sidebar":"tutorialSidebar"},"models/asmg":{"id":"models/asmg","title":"ASMG","description":"ASMG stands for Adaptive Sequential Model Generation.","sidebar":"tutorialSidebar"},"models/attrec":{"id":"models/attrec","title":"AttRec","description":"AttRec stands for Self-Attentive Sequential Recommendation.","sidebar":"tutorialSidebar"},"models/aush":{"id":"models/aush","title":"AUSH","description":"Attacking Recommender Systems with Augmented User Profiles","sidebar":"tutorialSidebar"},"models/autoint":{"id":"models/autoint","title":"AutoInt","description":"Song et. al., \u201cAutomatic Feature Interaction Learning via Self-Attentive Neural Networks\u201d. CIKM, 2018.","sidebar":"tutorialSidebar"},"models/basr":{"id":"models/basr","title":"BASR","description":"Black-Box Attacks on Sequential Recommenders via Data-Free Model Extraction","sidebar":"tutorialSidebar"},"models/bcq":{"id":"models/bcq","title":"BCQ","description":"Current off-policy deep reinforcement learning algorithms fail to address extrapolation error by selecting actions with respect to a learned value estimate, without consideration of the accuracy of the estimate. As a result, certain outof-distribution actions can be erroneously extrapolated to higher values. However, the value of an off-policy agent can be accurately evaluated in regions where data is available.","sidebar":"tutorialSidebar"},"models/beh-prop":{"id":"models/beh-prop","title":"Behavior Propensity Modeling","description":"behavior-propensity-modeling","sidebar":"tutorialSidebar"},"models/biasonly":{"id":"models/biasonly","title":"BiasOnly","description":"BiasOnly is a simple baseline that assumes no interactions between users and items. Formally, it learns: (1) a global bias \ud835\udefc; (2) scalar biases $\\\\betau$ for each user \ud835\udc62 \u2208 U; and (3) scalar biases $\\\\betai$ for each item \ud835\udc56 \u2208 I. Ultimately, the rating/relevance for user \ud835\udc62 and item \ud835\udc56 is modeled as $\\\\hati^u = \\\\alpha + \\\\betau + \\\\beta_i$.","sidebar":"tutorialSidebar"},"models/biggraph":{"id":"models/biggraph","title":"BigGraph","description":"PyTorch-BigGraph: A Large-scale Graph Embedding System","sidebar":"tutorialSidebar"},"models/bpr":{"id":"models/bpr","title":"BPR","description":"BPR stands for Bayesian Personalized Ranking. In matrix factorization (MF), to compute the prediction we have to multiply the user factors to the item factors:","sidebar":"tutorialSidebar"},"models/bst":{"id":"models/bst","title":"BST","description":"It stands for Behavior Sequence Transformer.","sidebar":"tutorialSidebar"},"models/caser":{"id":"models/caser","title":"CASER","description":"CASER stands for Convolutional Sequence Embedding Recommendation. Top-N sequential recommendation models each user as a sequence of items interacted in the past and aims to predict top-N ranked items that a user will likely interact in a \'near future\'. The order of interaction implies that sequential patterns play an important role where more recent items in a sequence have a larger impact on the next item. Convolutional Sequence Embedding Recommendation Model (Caser) address this requirement by embedding a sequence of recent items into an image\' in the time and latent spaces and learn sequential patterns as local features of the image using convolutional filters. This approach provides a unified and flexible network structure for capturing both general preferences and sequential patterns. In other words, Caser adopts convolutional neural networks capture the dynamic pattern influences of users\u2019 recent activities.","sidebar":"tutorialSidebar"},"models/cigc":{"id":"models/cigc","title":"CIGC","description":"To pursue high efficiency, we set the target as using only new data for model updating, meanwhile not sacrificing the recommendation accuracy compared with full model retraining. This is non-trivial to achieve, since the interaction data participates in both the graph structure for model construction and the loss function for model learning, whereas the old graph structure is not allowed to use in model updating. Causal Incremental Graph Convolution (CIGC) estimates the output of full graph convolution. Incremental Graph Convolution (IGC) ingeniously combine the old representations and the incremental graph and effectively fuse the long-term and short-term preference signals. Colliding Effect Distillation (CED) aims to avoid the out-of-date issue of inactive nodes that are not in the incremental graph, which connects the new data with inactive nodes through causal inference. In particular, CED estimates the causal effect of new data on the representation of inactive nodes through the control of their collider.","sidebar":"tutorialSidebar"},"models/coke":{"id":"models/coke","title":"CoKE","description":"Contextualized Knowledge Graph Embedding","sidebar":"tutorialSidebar"},"models/dcn":{"id":"models/dcn","title":"DCN","description":"DCN stands for Deep and Cross Network. Manual explicit feature crossing process is very laborious and inefficient. On the other hand, automatic implicit feature crossing methods like MLPs cannot efficiently approximate even 2nd or 3rd-order feature crosses. Deep-cross networks provides a solution to this problem. DCN was designed to learn explicit and bounded-degree cross features more effectively. It starts with an input layer (typically an embedding layer), followed by a cross network containing multiple cross layers that models explicit feature interactions, and then combines with a deep network that models implicit feature interactions.","sidebar":"tutorialSidebar"},"models/ddpg":{"id":"models/ddpg","title":"DDPG","description":"Deterministic Policy Gradient (DPG)\xa0is a type\xa0of Actor-Critic RL algorithm that uses two neural networks: one for estimating the action value function, and the other for estimating the optimal target policy. The\xa0Deep Deterministic Policy Gradient\xa0(DDPG) agent\xa0builds upon the idea of DPG and is quite efficient compared to vanilla Actor-Critic agents due\xa0to the use\xa0of deterministic action policies.","sidebar":"tutorialSidebar"},"models/deepcross":{"id":"models/deepcross","title":"DeepCross","description":"Shan, Y., Hoens, T., Jiao, J., Wang, H., Yu, D. and Mao, J., 2016.\xa0Deep Crossing: Web-Scale Modeling without Manually Crafted Combinatorial Features. [online] Kdd.org.","sidebar":"tutorialSidebar"},"models/deepfm":{"id":"models/deepfm","title":"DeepFM","description":"DeepFM stands for Deep Factorization Machines. It consists of an FM component and a deep component which are integrated in a parallel structure. The FM component is the same as the 2-way factorization machines which is used to model the low-order feature interactions. The deep component is a multi-layered perceptron that is used to capture high-order feature interactions and nonlinearities. These two components share the same inputs/embeddings and their outputs are summed up as the final prediction.","sidebar":"tutorialSidebar"},"models/deepwalk":{"id":"models/deepwalk","title":"DeepWalk","description":"DeepWalk learns representations of online social networks graphs. By performing random walks to generate sequences, the paper demonstrated that it was able to learn vector representations of nodes (e.g., profiles, content) in the graph.","sidebar":"tutorialSidebar"},"models/dgtn":{"id":"models/dgtn","title":"DGTN","description":"DGTN stands for Dual-channel Graph Transition Network.","sidebar":"tutorialSidebar"},"models/dm":{"id":"models/dm","title":"DM","description":"The direct method (DM) involves training a model on the historical data to predict the reward for each context-action instance. In this model, the reward is the target variable, while the context and action are the input features. Using this model, we can predict the rewards for each context-action instance associated with the target policy. This is illustrated below.","sidebar":"tutorialSidebar"},"models/dmt":{"id":"models/dmt","title":"DMT","description":"Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender Systems.","sidebar":"tutorialSidebar"},"models/dpadl":{"id":"models/dpadl","title":"DPADL","description":"Data Poisoning Attacks to Deep Learning Based Recommender Systems","sidebar":"tutorialSidebar"},"models/dqn":{"id":"models/dqn","title":"DQN","description":"The\xa0Q-learning component of DQN was invented in 1989 by Christopher Watkins in his PhD thesis titled \u201cLearning from Delayed Rewards\u201d. Experience replay quickly followed, invented by Long-Ji Lin in 1992. This played a major role in improving the efficiency of\xa0Q-learning. In the years that followed, however, there were no major success stories involving deep\xa0Q-learning. This is perhaps not surprising given the combination of limited computational power in the 1990s and early 2000s, data-hungry deep learning architectures, and the sparse, noisy, and delayed feedback signals experienced in RL. Progress had to wait for the emergence of general-purpose GPU programming, for example with the launch of CUDA in 2006, and the reignition of interest in deep learning within the machine learning community that began in the mid-2000s and rapidly accelerated after 2012.","sidebar":"tutorialSidebar"},"models/drqn":{"id":"models/drqn","title":"DRQN","description":"DRQN stands for Deep Recurrent Q-Learning. It is a combination of a recurrent neural network (RNN) and a deep Q-network (DQN). The idea being that the RNN will be able to retain information from states further back in time and incorporate that into predicting better Q values and thus performing better on games that require long term planning.","sidebar":"tutorialSidebar"},"models/drr":{"id":"models/drr","title":"DRR","description":"DRR Framework","sidebar":"tutorialSidebar"},"models/dueling-dqn":{"id":"models/dueling-dqn","title":"Dueling DQN","description":"A\xa0Dueling DQN agent explicitly estimates two quantities through a modified network architecture:","sidebar":"tutorialSidebar"},"models/ffm":{"id":"models/ffm","title":"FFM","description":"FFM stands for Field-aware Factorization Machines. In the official FFM paper, it is empirically proven that for large, sparse datasets with many categorical features, FFM performs better. Conversely, for small and dense datasets or numerical datasets, FFM may not be as effective as FM. FFM is also prone to overfitting on the training dataset, hence one should use a standalone validation set and use early stopping when the loss increases.","sidebar":"tutorialSidebar"},"models/fgnn":{"id":"models/fgnn","title":"FGNN","description":"Ruihong Qiu, Jingjing Li, Zi Huang and Hongzhi Yin, \u201cRethinking the Item Order in Session-based Recommendation with Graph Neural Networks\u201d. CIKM, 2019.","sidebar":"tutorialSidebar"},"models/fm":{"id":"models/fm","title":"FM","description":"Factorization Machines (FMs) are a supervised learning approach that enhances the linear regression model by incorporating the second-order feature interactions. Factorization Machine type algorithms are a combination of linear regression and matrix factorization, the cool idea behind this type of algorithm is it aims model interactions between features (a.k.a attributes, explanatory variables) using factorized parameters. By doing so it has the ability to estimate all interactions between features even with extremely sparse data.","sidebar":"tutorialSidebar"},"models/gat":{"id":"models/gat","title":"GAT","description":"GAT stands for Graph Attention Networks. This is a special GNN model that addresses several key challenges of spectral models, such as poor ability of generalization from a specific graph structure to another and sophisticated computation of matrix inverse. GAT utilizes attention mechanisms to aggregate neighborhood features (embeddings) by specifying different weights to different nodes.","sidebar":"tutorialSidebar"},"models/gc-san":{"id":"models/gc-san","title":"GC-SAN","description":"GC-SAN stands for Graph contextualized self-attention.","sidebar":"tutorialSidebar"},"models/gce-gnn":{"id":"models/gce-gnn","title":"GCE-GNN","description":"GCE-GNN stands for Global Context Enhanced Graph Neural Networks. It exploit item transitions over all sessions in a more subtle manner for better inferring the user preference of the current session.","sidebar":"tutorialSidebar"},"models/glmix":{"id":"models/glmix","title":"GLMix","description":"Generalized Linear Mixed Effects model (GLMix) decomposes a personalized recommender system into 2 submodels we first train the fixed effects model, and then train random effects models on the residuals after scoring the fixed effects model, and go back to fixed effects model training again until convergence (Zhang et al., 2016).","sidebar":"tutorialSidebar"},"models/gru4rec":{"id":"models/gru4rec","title":"GRU4Rec","description":"It uses session-parallel mini-batch approach where we first create an order for the sessions and then, we use the first event of the first X sessions to form the input of the first mini-batch (the desired output is the second events of our active sessions). The second mini-batch is formed from the second events and so on. If any of the sessions end, the next available session is put in its place. Sessions are assumed to be independent, thus we reset the appropriate hidden state when this switch occurs.","sidebar":"tutorialSidebar"},"models/hmlet":{"id":"models/hmlet","title":"HMLET","description":"HMLET stands for Hybrid Method of Linear and nonlinEar collaborative filTering (HMLET, pronounced as Hamlet). It is a GCN-based CF method.","sidebar":"tutorialSidebar"},"models/incctr":{"id":"models/incctr","title":"IncCTR","description":"Recently, various deep CTR models are proposed, such as DeepFM, Wide & Deep, PIN, DIN, and DIEN. Generally, deep CTR models include three parts: embedding layer, interaction layer, and prediction layer.","sidebar":"tutorialSidebar"},"models/ipw":{"id":"models/ipw","title":"IPW","description":"Inverse Propensity Weighting","sidebar":"tutorialSidebar"},"models/itempop":{"id":"models/itempop","title":"ItemPop","description":"Itempop is a na\xefve baseline that simply ranks items according to overall train-set popularity. Note that this method is unaffected by the user for which items are being recommended, and has the same global ranking of all items","sidebar":"tutorialSidebar"},"models/khgt":{"id":"models/khgt","title":"KHGT","description":"Knowledge-Enhanced Hierarchical Graph Transformer Network for Multi-Behavior Recommendation","sidebar":"tutorialSidebar"},"models/lessr":{"id":"models/lessr","title":"LESSR","description":"Tianwen Chen and Raymond Chi-Wing Wong, \u201cLESSR: Handling Information Loss of Graph Neural Networks for Session-based Recommendation\u201d. KDD, 2020.","sidebar":"tutorialSidebar"},"models/lightfm-warp":{"id":"models/lightfm-warp","title":"LightFM WARP","description":"LightFM is probably the only recommender package implementing the WARP (Weighted Approximate-Rank Pairwise) loss for implicit feedback learning-to-rank. Generally, it performs better than the more popular BPR (Bayesian Personalised Ranking) loss --- often by a large margin.","sidebar":"tutorialSidebar"},"models/lightgcn":{"id":"models/lightgcn","title":"LightGCN","description":"GCN is a representative model of graph neural networks that applies message passing to aggregate neighborhood information. The message passing layer with self-loops is defined as follows:","sidebar":"tutorialSidebar"},"models/line":{"id":"models/line","title":"LINE","description":"Large-scale Information Network Embedding","sidebar":"tutorialSidebar"},"models/lird":{"id":"models/lird","title":"LIRD","description":"Existing reinforcement learning recommender methods also could recommend a list of items. E.g. DQN can calculate Q-values of all recalled items separately, and recommend a list of items with highest Q-values. But these recommendations are similar in Euclidean space and we want to find similarity in associative space. For instance, for a bread \ud83c\udf5e, I want egg \ud83e\udd5a, milk \ud83e\udd5b in my recommendation list instead of white bread \ud83c\udf5e, brown bread \ud83e\udd6a, bun \ud83e\uded3 etc.","sidebar":"tutorialSidebar"},"models/markov-chains":{"id":"models/markov-chains","title":"Markov Chains","description":"Markov chains, named after Andrey Markov, are mathematical systems that hop from one \\"state\\" (a situation or set of values) to another. For example, if you made a Markov chain model of a baby\'s behavior, you might include \\"playing,\\" \\"eating\\", \\"sleeping,\\" and \\"crying\\" as states, which together with other behaviors could form a \'state space\': a list of all possible states. In addition, on top of the state space, a Markov chain tells you the probability of hopping, or \\"transitioning,\\" from one state to any other state---e.g., the chance that a baby currently playing will fall asleep in the next five minutes without crying first.","sidebar":"tutorialSidebar"},"models/matn":{"id":"models/matn","title":"MATN","description":"Multiplex Behavioral Relation Learning for Recommendation via Memory Augmented Transformer Network.","sidebar":"tutorialSidebar"},"models/mb-gmn":{"id":"models/mb-gmn","title":"MB-GMN","description":"MB-GMN stands for Multi-behavior pattern modeling with meta-knowledge learner.","sidebar":"tutorialSidebar"},"models/mdp":{"id":"models/mdp","title":"MDP","description":"The\xa0Markov decision process\xa0(MDP), a\xa0reinforcement learning\xa0(RL) algorithm, perfectly illustrates\xa0how machines have become intelligent in their own\xa0unique way. Humans build their decision process on experience. MDPs are memoryless. Humans use logic and reasoning to think problems through. MDPs apply random decisions 100% of the time. Humans think in words, labeling everything they perceive. MDPs have an unsupervised approach that uses no labels or training data. MDPs boost the machine thought process of self-driving cars (SDCs), translation tools, scheduling software, and more. This memoryless, random, and unlabeled machine thought process marks a historical change in the way a former human problem was solved.","sidebar":"tutorialSidebar"},"models/mf":{"id":"models/mf","title":"MF","description":"Matrix Factorization is an iterative approach of SVD called Regularized SVD. It uses the gradient-descent method to estimate the resulting matrices. The obtained model will not be a true SVD of the rating-matrix, as the component matrices are no longer orthogonal, but tends to be more accurate at predicting unseen preferences than the standard SVD [Ekstrand et al. 2011].","sidebar":"tutorialSidebar"},"models/mian":{"id":"models/mian","title":"MIAN","description":"MIAN stands for Multi-Interactive Attention Network. It aggregate multiple information, and gain latent representations through interactions between candidate items and other fine-grained features.","sidebar":"tutorialSidebar"},"models/mmoe":{"id":"models/mmoe","title":"MMoE","description":"MMoE stands for Multi-gate Mixture-of-Experts.","sidebar":"tutorialSidebar"},"models/models":{"id":"models/models","title":"Models","description":"In the following sections, we will more systematically introduce the following models:","sidebar":"tutorialSidebar"},"models/mpnn":{"id":"models/mpnn","title":"MPNN","description":"Message Passing Neural Networks","sidebar":"tutorialSidebar"},"models/neumf":{"id":"models/neumf","title":"NeuMF","description":"NMF Leverages the representation power of deep neural-networks to capture nonlinear correlations between user and item embeddings. Formally, the rating/relevance for user \ud835\udc62 and item \ud835\udc56 is modeled as $\\\\hati^u = \\\\alpha + \\\\betau + \\\\betai + f(\\\\gammau || \\\\gammai || \\\\gammau \\\\cdot \\\\gammai)$ where $\\\\gammau , \\\\gamma_i \\\\in \\\\mathbb{R}^d$, \u2018||\u2019 represents the concatenation operation, and $f: \\\\mathbb{R}^{3d} \\\\rightarrow \\\\mathbb{R}$ represents an arbitrarily complex neural network.","sidebar":"tutorialSidebar"},"models/ngcf":{"id":"models/ngcf","title":"NGCF","description":"NGCF stands for Neural Graph Collaborative Filtering. This GNN-based approach follows basic operations inherited from the standard GCN to explore the high-order connectivity information. More specifically, NGCF stacks embedding layers and concatenates embeddings obtained in all layers to constitute the final embeddings.","sidebar":"tutorialSidebar"},"models/nmrn":{"id":"models/nmrn","title":"NMRN","description":"NMRN is a streaming recommender model based on neural memory networks with external memories to capture and store both long-term stable interests and short-term dynamic interests in a unified way. An adaptive negative sampling framework based on Generative Adversarial Nets (GAN) is developed to optimize our proposed streaming recommender model, which effectively overcomes the limitations of classical negative sampling approaches and improves the effectiveness of the model parameter inference.","sidebar":"tutorialSidebar"},"models/node2vec":{"id":"models/node2vec","title":"Node2vec","description":"Nodes in networks could be organized based on communities they belong to (i.e., homophily); in other cases, the organization could be based on the structural roles of nodes in the network (i.e., structural equivalence). For instance, in the below figure, we observe nodes $u$ and $s1$ belonging to the same tightly knit community of nodes, while the nodes $u$ and $s6$ in the two distinct communities share the same structural role of a hub node. Real-world networks commonly exhibit a mixture of such equivalences.","sidebar":"tutorialSidebar"},"models/pnn":{"id":"models/pnn","title":"PNN","description":"PNN stands for Product-based Neural Network.","sidebar":"tutorialSidebar"},"models/ppo":{"id":"models/ppo","title":"PPO","description":"The PPO (Proximal Policy Optimization) algorithm was\xa0introduced by the OpenAI team in 2017\xa0and quickly became one of the most popular Reinforcement Learning method that pushed all other RL methods at that moment aside. PPO involves collecting a small batch of experiences interacting with the environment and using that batch to update its decision-making policy. Once the policy is updated with that batch, the experiences are thrown away and a newer batch is collected with the newly updated policy. This is the reason why it is an \u201con-policy learning\u201d approach where the experience samples collected are only useful for updating the current policy.","sidebar":"tutorialSidebar"},"models/q-learning":{"id":"models/q-learning","title":"Q-learning","description":"Q-learning can be applied to model-free RL problems. It supports off-policy learning and therefore provides a practical solution to problems where available experiences were/are collected using some other policy or by some other agent (even humans).","sidebar":"tutorialSidebar"},"models/random_walk":{"id":"models/random_walk","title":"Random Walk","description":"The term \\"random walk\\" was first mentioned by Karl Pearson in 1905 in a letter to Nature magazine titled\xa0The Problem of the Random Walk. Study of random walks date back even further to the\xa0Gambler\u2019s ruin\xa0problem, where it could be used to show that a gambler would eventually go bankrupt against an opponent with infinite wealth. It\u2019s only in the last couple of decades, however, that researchers have studied them with respect to networks.","sidebar":"tutorialSidebar"},"models/rkmf":{"id":"models/rkmf","title":"RKMF","description":"A kernel function allows to transform the product of the factor matrices. Kernels like the s-shaped logistic function allow to impose bounds on the prediction (e.g. one to five stars) while still being differentiable.","sidebar":"tutorialSidebar"},"models/sac":{"id":"models/sac","title":"SAC","description":"SAS stands for Soft Actor-Critic. It not only boasts of being more sample efficient than traditional RL algorithms but also promises to be robust to brittleness in convergence.","sidebar":"tutorialSidebar"},"models/sarsa":{"id":"models/sarsa","title":"SARSA","description":"The SARSA algorithm can be applied to model-free control problems and allows us to optimize the value function of an unknown MDP. SARSA is an on-policy\xa0temporal difference learning-based control algorithm. The SARSA algorithm can be summarized as follows:","sidebar":"tutorialSidebar"},"models/sasrec":{"id":"models/sasrec","title":"SASRec","description":"SASRec stands for Self-Attentive Sequential Recommendation. It relies on the sequence modeling capabilities of self-attentive neural networks to predict the occurence of the next item in a user\u2019s consumption sequence. To be precise, given a user \ud835\udc62 and their time-ordered consumption history $S^\ud835\udc62 = (S1^u, S2^u, \\\\dots, S_{|S^u|}^\ud835\udc62),$ SASRec first applies self-attention on $S^\ud835\udc62$ followed by a series of non-linear feed-forward layers to finally obtain the next item likelihood.","sidebar":"tutorialSidebar"},"models/sdne":{"id":"models/sdne","title":"SDNE","description":"Structural Deep Network Embedding","sidebar":"tutorialSidebar"},"models/sgl":{"id":"models/sgl","title":"SGL","description":"SGL is the latest baseline for top-k recommendations. It introduces self-supervised learning into the recommendation system based on the contrastive learning framework. It is implemented on LightGCN and uses a multitask approach that unites the contrastive loss and the BPR loss function. SGL mainly benefits from graph contrastive learning to reinforce user and item representations.","sidebar":"tutorialSidebar"},"models/shared-bottom":{"id":"models/shared-bottom","title":"Shared Bottom","description":"The shared-bottom model is the simplest and most common multi-task learning architecture. The model has a single base (the shared bottom) from which all of the task-specific subnetworks begin from. This means that this single representation is used for all tasks, and there is no way for individual tasks to adjust what information they get out of the shared bottom compared to other tasks.","sidebar":"tutorialSidebar"},"models/siren":{"id":"models/siren","title":"SiReN","description":"Existing literature often ignores the negative feedback e.g. dislikes on YouTube videos, and only capture the homophily (or assortativity) patterns by positive feedback. This is a missed opportunity situation. Performance of GNN-based Recommender Systems can be improved by including negative feedbacks. Disassortivity patterns can be learned by negative feedback. LightGCN can capture the assortativity patterns. and the MLP network can capture the disassortivity patterns.","sidebar":"tutorialSidebar"},"models/slist":{"id":"models/slist","title":"SLIST","description":"SLIST stands for Session-aware Linear Similarity/Transition. It is built by unifying two linear models - SLIS and SLIT.","sidebar":"tutorialSidebar"},"models/sml":{"id":"models/sml","title":"SML","description":"Problem formulation","sidebar":"tutorialSidebar"},"models/spop":{"id":"models/spop","title":"SPop","description":"SPop stands for Session-based Popularity. It is a session popularity predictor that gives higher scores to items with higher number of occurrences in the session. Ties are broken up by adding the popularity score of the item.","sidebar":"tutorialSidebar"},"models/sr-gnn":{"id":"models/sr-gnn","title":"SR-GNN","description":"SR-GNN stands for Session-based Recommendation with Graph Neural Networks.","sidebar":"tutorialSidebar"},"models/sr-san":{"id":"models/sr-san","title":"SR-SAN","description":"SR-SAN stands for Session-based Recommendation with Self-Attention Networks.","sidebar":"tutorialSidebar"},"models/sse-pt":{"id":"models/sse-pt","title":"SSE-PT","description":"Temporal information is crucial for recommendation problems because user preferences are naturally dynamic in the real world. Recent advances in deep learning, especially the discovery of various attention mechanisms and newer architectures in addition to widely used RNN and CNN in natural language processing, have allowed for better use of the temporal ordering of items that each user has engaged with. In particular, the SASRec model, inspired by the popular Transformer model in natural languages processing, has achieved state-of-the-art results. However, SASRec, just like the original Transformer model, is inherently an un-personalized model and does not include personalized user embeddings. SSE-PT overcomes this limitation by employing a Personalized Transformer.","sidebar":"tutorialSidebar"},"models/stamp":{"id":"models/stamp","title":"STAMP","description":"/img/content-models-raw-mp2-stamp-untitled.png","sidebar":"tutorialSidebar"},"models/struc2vec":{"id":"models/struc2vec","title":"Struc2Vec","description":"Learning Node Representations from Structural Identity","sidebar":"tutorialSidebar"},"models/svae":{"id":"models/svae","title":"SVAE","description":"SVAE stands for Sequential Variational Autoencoder.","sidebar":"tutorialSidebar"},"models/taamr":{"id":"models/taamr","title":"TAaMR","description":"Targeted Adversarial Attack against Multimedia Recommender Systems","sidebar":"tutorialSidebar"},"models/tagnn":{"id":"models/tagnn","title":"TAGNN","description":"TAGNN stands for Target Attentive Graph Neural Network. Session-based recommendations are challenging due to limited user-item interactions. Typical sequential models are not able to capture complex patterns from all previous interactions. SessionGraph (a graph representation of sessions) can capture the complex patterns from all previous interactions.","sidebar":"tutorialSidebar"},"models/tagnn-pp":{"id":"models/tagnn-pp","title":"TAGNN-PP","description":"TAGNN-PP models item interactions with GNN, and both local and global user interactions with  a Transformer.","sidebar":"tutorialSidebar"},"models/tgin":{"id":"models/tgin","title":"TGIN","description":"Jiang et. al., \u201cTriangle Graph Interest Network for Click-through Rate Prediction\u201d. WSDM, 2022.","sidebar":"tutorialSidebar"},"models/vncf":{"id":"models/vncf","title":"VNCF","description":"VNCF stands for Variational Neural Collaborative Filtering.","sidebar":"tutorialSidebar"},"models/vsknn":{"id":"models/vsknn","title":"VSKNN","description":"VSKNN stands for Vector Multiplication Session-Based kNN. The idea of this variant is to put more emphasis on the more recent events of a session when computing the similarities. Instead of encoding a session as a binary vector, we use real-valued vectors to encode the current session. Only the very last element of the session obtains a value of \u201c1\u201d; the weights of the other elements are determined using a linear decay function that depends on the position of the element within the session, where elements appearing earlier in the session obtain a lower weight. As a result, when using the dot product as a similarity function between the current weight-encoded session and a binary-encoded past session, more emphasis is given to elements that appear later in the sessions.","sidebar":"tutorialSidebar"},"models/wide-and-deep":{"id":"models/wide-and-deep","title":"Wide and Deep","description":"Wide and Deep Learning Model, proposed by Google, 2016, is a DNN-Linear mixed model, which combines the strength of memorization and generalization. It\'s useful for generic large-scale regression and classification problems with sparse input features (e.g., categorical features with a large number of possible feature values). It has been used for Google App Store for their app recommendation.","sidebar":"tutorialSidebar"},"models/word2vec":{"id":"models/word2vec","title":"Word2vec","description":"/img/content-models-raw-mp2-word2vec-untitled.png","sidebar":"tutorialSidebar"},"models/xdeepfm":{"id":"models/xdeepfm","title":"xDeepFM","description":"xDeepFM stands for Extreme Deep Factorization Machines.","sidebar":"tutorialSidebar"},"notebooks":{"id":"notebooks","title":"Notebooks","description":"|  Notebook                           |   Colab  |     nbviewer    |","sidebar":"tutorialSidebar"},"projects":{"id":"projects","title":"Projects","description":"| Project Id | Title | Links |","sidebar":"tutorialSidebar"},"snippets/python-snippets":{"id":"snippets/python-snippets","title":"Python code","description":"Clean filenames in a folder","sidebar":"tutorialSidebar"},"snippets/unix-shell-snippets":{"id":"snippets/unix-shell-snippets","title":"Unix shell","description":"Replace file extension of all files in a folder","sidebar":"tutorialSidebar"},"tools/obp":{"id":"tools/obp","title":"OBP","description":"Process flow","sidebar":"tutorialSidebar"},"tools/pachyderm":{"id":"tools/pachyderm","title":"Pachyderm","description":"If you are familiar with Git, a version control and life cycle system for code, you will find many similarities between the most important Git and Pachyderm concepts. Version control systems such as Git and its hosted version GitHub have become an industry standard for thousands of developers worldwide. Git enables you to keep a history of changes in your code and go back when needed. Data scientists deserve a platform that will let them track the versions of their experiments, reproduce results when needed, and investigate and correct bias that might crawl into one of the stages of the data science life cycle. Pachyderm provides benefits similar to Git that enable data scientists to reproduce their experiments and effortlessly manage the complete life cycle of the data science workflow.","sidebar":"tutorialSidebar"},"tools/pyspark":{"id":"tools/pyspark","title":"PySpark","description":"","sidebar":"tutorialSidebar"},"tools/river":{"id":"tools/river","title":"River","description":"River is a Python library for\xa0online machine learning. It is the result of a merger between\xa0creme\xa0and\xa0scikit-multiflow. River\'s ambition is to be the go-to library for doing machine learning on streaming data.","sidebar":"tutorialSidebar"},"tools/tools":{"id":"tools/tools","title":"Tools","description":"In the following sections, we will more systematically introduce the following tools:","sidebar":"tutorialSidebar"},"tutorials/amazon-personalize":{"id":"tutorials/amazon-personalize","title":"Amazon Personalize","description":"| Description | Notebook |","sidebar":"tutorialSidebar"},"tutorials/database-conn":{"id":"tutorials/database-conn","title":"Database Connections","description":"Mongodb","sidebar":"tutorialSidebar"},"tutorials/email-classification":{"id":"tutorials/email-classification","title":"Email Classification","description":"Fetching data from MS-Sql","sidebar":"tutorialSidebar"},"tutorials/graph":{"id":"tutorials/graph","title":"Graph-based Modeling","description":"General","sidebar":"tutorialSidebar"},"tutorials/mathematics":{"id":"tutorials/mathematics","title":"Mathematics","description":"Probability and Statistics","sidebar":"tutorialSidebar"},"tutorials/matrix-factorizations":{"id":"tutorials/matrix-factorizations","title":"Matrix Factorizations","description":"Neural Matrix Factorization (NMF)","sidebar":"tutorialSidebar"},"tutorials/mlops":{"id":"tutorials/mlops","title":"MLOps","description":"| Description | Notebook |","sidebar":"tutorialSidebar"},"tutorials/multi-touch-attribution":{"id":"tutorials/multi-touch-attribution","title":"Multi-Touch Attribution","description":"Abstract","sidebar":"tutorialSidebar"},"tutorials/name-address-parsing":{"id":"tutorials/name-address-parsing","title":"Name and Address Parsing","description":"Recognising Person names and Addresses in a text using NER and NLP modeling techniques","sidebar":"tutorialSidebar"},"tutorials/negative-implicit-feedback-rec":{"id":"tutorials/negative-implicit-feedback-rec","title":"Negative Implicit Feedback in Recommendations","description":"A tutorial to demonstrate the process of training and evaluating various recommender models on a online retail store data. Along with the positive feedbacks like view, add-to-cart, we also have a negative event \'remove-from-cart\'.","sidebar":"tutorialSidebar"},"tutorials/pdf-to-wordcloud-mail":{"id":"tutorials/pdf-to-wordcloud-mail","title":"PDF to WordCloud via Email","description":"Receive a pdf via outlook mail and send back the wordcloud of that pdf in the reply","sidebar":"tutorialSidebar"},"tutorials/recsys-evaluation":{"id":"tutorials/recsys-evaluation","title":"Recommender System Evaluation","description":"| Description | Notebook |","sidebar":"tutorialSidebar"},"tutorials/resale-price-prediction":{"id":"tutorials/resale-price-prediction","title":"Resale Price Prediction","description":"|   | BRAND | PARTNO           | QUANTITY | UNITRESALE | UNITCOST |","sidebar":"tutorialSidebar"},"tutorials/vector-search":{"id":"tutorials/vector-search","title":"Vector Search","description":"Faiss","sidebar":"tutorialSidebar"},"tutorials/word2vec":{"id":"tutorials/word2vec","title":"Word2vec","description":"| Description | Notebook |","sidebar":"tutorialSidebar"}}}')}}]);