<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.17">
<link rel="alternate" type="application/rss+xml" href="/ai/blog/rss.xml" title="AI RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/ai/blog/atom.xml" title="AI Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><title data-rh="true">Blog | AI</title><meta data-rh="true" property="og:title" content="Blog | AI"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" name="description" content="Blog"><meta data-rh="true" property="og:description" content="Blog"><meta data-rh="true" property="og:url" content="https://docs.recohut.com/ai/blog"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="blog_posts_list"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_posts_list"><link data-rh="true" rel="icon" href="/ai/img/logo.svg"><link data-rh="true" rel="canonical" href="https://docs.recohut.com/ai/blog"><link data-rh="true" rel="alternate" href="https://docs.recohut.com/ai/blog" hreflang="en"><link data-rh="true" rel="alternate" href="https://docs.recohut.com/ai/blog" hreflang="x-default"><link rel="stylesheet" href="/ai/assets/css/styles.fbf22338.css">
<link rel="preload" href="/ai/assets/js/runtime~main.92407a2e.js" as="script">
<link rel="preload" href="/ai/assets/js/main.7b4c40d1.js" as="script">
</head>
<body class="navigation-with-keyboard" data-theme="light">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai/"><div class="navbar__logo"><img src="/ai/img/logo.svg" alt="Recohut Logo" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/ai/img/logo.svg" alt="Recohut Logo" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title">Recohut</b></a><a class="navbar__item navbar__link" href="/ai/docs/intro">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/ai/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/sparsh-ai/ai" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_S7eR toggle_TdHA toggleDisabled_f9M3"><div class="toggleButton_rCf9" role="button" tabindex="-1"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_v35p"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_nQuB"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></div><input type="checkbox" class="toggleScreenReader_g2nN" aria-label="Switch between dark and light mode (currently light mode)"></div><div class="dsla-search-wrapper"><div class="dsla-search-field" data-tags="default,docs-default-current"></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-list-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_a9qW thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_uKok margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Kvuv"><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/ai/blog/2021/10/01/clinical-decision-making">Clinical Decision Making</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/ai/blog/2021/10/01/detectron-2">Detectron 2</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/ai/blog/2021/10/01/distributed-training-of-recommender-systems">Distributed Training of Recommender Systems</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/ai/blog/2021/10/01/document-recommendation">Document Recommendation</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/ai/blog/2021/10/01/fake-voice-detection">Fake Voice Detection</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/ai/blog/2021/10/01/clinical-decision-making">Clinical Decision Making</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2021-10-01T00:00:00.000Z" itemprop="datePublished">October 1, 2021</time> · <!-- -->2 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://avatars.githubusercontent.com/u/62965911?v=4" alt="Sparsh Agarwal"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sparsh Agarwal</span></a></div><small class="avatar__subtitle" itemprop="description">Principal Developer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>Health insurance can be complicated—especially when it comes to prior authorization (also referred to as pre-approval, pre-authorization, and pre-certification). The manual labor involved in obtaining prior authorizations (PAs) is a well-recognized burden among providers. Up to 46% of PA requests are still submitted by fax, and 60% require a telephone call, according to America’s Health Insurance Plans (AHIP). A 2018 survey by the American Medical Association (AMA) found that doctors and their staff spend an average of 2 days a week completing PAs. In addition to eating up time that physicians could spend with patients, PAs also contribute to burnout.</p><p>The objective was to identify the patterns from data to create clinical decision making in Pre-Auth and improve the accuracy in a clinical decision based on historical data analysis. </p><p>Two use cases were identified. Use Case 1 - <em>Supervised Learning Model - to aid clinicians in UM decision making. Tasks -</em> Ingest Pre-authorization data from Mongo DB into the analytical environment, Exploratory Data Analysis and Feature Engineering, Train supervised analytical models, model validation and model selection, Create a web service to be plugged into the case processing flow to call the model, and Display the recommendation from the model on UI on the authorization review screen. Use Case 2 - <em>Unsupervised Learning Model - to generate insights from the pre-authorization data. Tasks -</em> Ingest Pre-authorization data from Mongo DB into the analytical environment, Cluster analysis, univariate and multivariate analysis, and Generate insights and display insights on the dashboard.</p><p>Final Deliverables - Model re-training (batch mode), validation and deployment code (python scripts) with Unix command line support, Documentation - PPT, Recorded video, Technical document, Flask API backend system, HTML/PHP Web App frontend UI integration, and Plotly Dash Supervised/Unsupervised learning and insights generation dashboard.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai/blog/tags/classification">classification</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai/blog/tags/healthcare">healthcare</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/ai/blog/2021/10/01/detectron-2">Detectron 2</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2021-10-01T00:00:00.000Z" itemprop="datePublished">October 1, 2021</time> · <!-- -->7 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://avatars.githubusercontent.com/u/62965911?v=4" alt="Sparsh Agarwal"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sparsh Agarwal</span></a></div><small class="avatar__subtitle" itemprop="description">Principal Developer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="/img/content-blog-raw-blog-detectron-2-untitled.png" src="/ai/assets/images/content-blog-raw-blog-detectron-2-untitled-1b615c35cb13ff3ee0b6a399bde1a3c7.png" width="1347" height="901"></p><h1>Introduction</h1><p>Detectron 2 is a next-generation open-source object detection system from Facebook AI Research. With the repo you can use and train the various state-of-the-art models for detection tasks such as bounding-box detection, instance and semantic segmentation, and person keypoint detection.</p><p>The following is the directory tree of detectron 2:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">detectron2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├─checkpoint  &lt;- checkpointer and model catalog handlers</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├─config      &lt;- default configs and handlers</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├─data        &lt;- dataset handlers and data loaders</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├─engine      &lt;- predictor and trainer engines</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├─evaluation  &lt;- evaluator for each dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├─export      &lt;- converter of detectron2 models to caffe2 (ONNX)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├─layers      &lt;- custom layers e.g. deformable conv.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├─model_zoo   &lt;- pre-trained model links and handler</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├─modeling   </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│  ├─meta_arch &lt;- meta architecture e.g. R-CNN, RetinaNet</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│  ├─backbone  &lt;- backbone network e.g. ResNet, FPN</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│  ├─proposal_generator &lt;- region proposal network</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│  └─roi_heads &lt;- head networks for pooled ROIs e.g. box, mask heads</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├─solver       &lt;- optimizer and scheduler builders</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├─structures   &lt;- structure classes e.g. Boxes, Instances, etc</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">└─utils        &lt;- utility modules e.g. visualizer, logger, etc</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h1>Installation</h1><div class="codeBlockContainer_I0IT language-python theme-code-block"><div class="codeBlockContent_wNvx python"><pre tabindex="0" class="prism-code language-python codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token operator" style="color:#393A34">%</span><span class="token operator" style="color:#393A34">%</span><span class="token plain">time</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">!pip install </span><span class="token operator" style="color:#393A34">-</span><span class="token plain">U torch</span><span class="token operator" style="color:#393A34">==</span><span class="token number" style="color:#36acaa">1.4</span><span class="token operator" style="color:#393A34">+</span><span class="token plain">cu100 torchvision</span><span class="token operator" style="color:#393A34">==</span><span class="token number" style="color:#36acaa">0.5</span><span class="token operator" style="color:#393A34">+</span><span class="token plain">cu100 </span><span class="token operator" style="color:#393A34">-</span><span class="token plain">f https</span><span class="token punctuation" style="color:#393A34">:</span><span class="token operator" style="color:#393A34">//</span><span class="token plain">download</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">pytorch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">org</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">whl</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">torch_stable</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">html</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">!pip install cython pyyaml</span><span class="token operator" style="color:#393A34">==</span><span class="token number" style="color:#36acaa">5.1</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">!pip install </span><span class="token operator" style="color:#393A34">-</span><span class="token plain">U </span><span class="token string" style="color:#e3116c">&#x27;git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI&#x27;</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">!pip install detectron2 </span><span class="token operator" style="color:#393A34">-</span><span class="token plain">f https</span><span class="token punctuation" style="color:#393A34">:</span><span class="token operator" style="color:#393A34">//</span><span class="token plain">dl</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">fbaipublicfiles</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">com</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">detectron2</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">wheels</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">cu100</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">index</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">html</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> detectron2 </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> model_zoo</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> detectron2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">engine </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> DefaultPredictor</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> detectron2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">config </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> get_cfg</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> detectron2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">utils</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">visualizer </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> Visualizer</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> detectron2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">data </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> MetadataCatalog</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h1>Inference on pre-trained models</h1><p><img loading="lazy" alt="Original image" src="/ai/assets/images/content-blog-raw-blog-detectron-2-untitled-1-f67813931f498e5451c89d34fc53c18a.png" width="620" height="479"></p><p>Original image</p><p><img loading="lazy" alt="Object detection with Faster-RCNN-101" src="/ai/assets/images/content-blog-raw-blog-detectron-2-untitled-2-2d53fa263a2f78ba53e25191a5174f6a.png" width="744" height="574"></p><p>Object detection with Faster-RCNN-101</p><p><img loading="lazy" alt="Instance segmentation with Mask-RCNN-50" src="/ai/assets/images/content-blog-raw-blog-detectron-2-untitled-3-843f187808ce30f5f39ee16632bcc07e.png" width="744" height="574"></p><p>Instance segmentation with Mask-RCNN-50</p><p><img loading="lazy" alt="Keypoint estimation with Keypoint-RCNN-50" src="/ai/assets/images/content-blog-raw-blog-detectron-2-untitled-4-dc55d3ff169a8b338c25147d005a7357.png" width="744" height="574"></p><p>Keypoint estimation with Keypoint-RCNN-50</p><p><img loading="lazy" alt="Panoptic segmentation with Panoptic-FPN-101" src="/ai/assets/images/content-blog-raw-blog-detectron-2-untitled-5-400fd835e75e14ba61644847b378b47f.png" width="744" height="574"></p><p>Panoptic segmentation with Panoptic-FPN-101</p><p><img loading="lazy" alt="Default Mask R-CNN (top) vs. Mask R-CNN with PointRend (bottom) comparison" src="/ai/assets/images/content-blog-raw-blog-detectron-2-untitled-6-9497a38e9bed29af766a327244331cee.png" width="744" height="1148"></p><p>Default Mask R-CNN (top) vs. Mask R-CNN with PointRend (bottom) comparison</p><h1>Fine-tuning Balloons Dataset</h1><h3 class="anchor anchorWithStickyNavbar_mojV" id="load-the-data">Load the data<a class="hash-link" href="#load-the-data" title="Direct link to heading">​</a></h3><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain"># download, decompress the data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">!unzip balloon_dataset.zip &gt; /dev/null</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="convert-dataset-into-detectron2s-standard-format">Convert dataset into Detectron2&#x27;s standard format<a class="hash-link" href="#convert-dataset-into-detectron2s-standard-format" title="Direct link to heading">​</a></h3><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">from detectron2.structures import BoxMode</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># write a function that loads the dataset into detectron2&#x27;s standard format</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">def get_balloon_dicts(img_dir):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    json_file = os.path.join(img_dir, &quot;via_region_data.json&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    with open(json_file) as f:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        imgs_anns = json.load(f)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    dataset_dicts = []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    for _, v in imgs_anns.items():</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        record = {}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        filename = os.path.join(img_dir, v[&quot;filename&quot;])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        height, width = cv2.imread(filename).shape[:2]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        record[&quot;file_name&quot;] = filename</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        record[&quot;height&quot;] = height</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        record[&quot;width&quot;] = width</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        annos = v[&quot;regions&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        objs = []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        for _, anno in annos.items():</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            assert not anno[&quot;region_attributes&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            anno = anno[&quot;shape_attributes&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            px = anno[&quot;all_points_x&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            py = anno[&quot;all_points_y&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            poly = list(itertools.chain.from_iterable(poly))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            obj = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &quot;bbox&quot;: [np.min(px), np.min(py), np.max(px), np.max(py)],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &quot;bbox_mode&quot;: BoxMode.XYXY_ABS,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &quot;segmentation&quot;: [poly],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &quot;category_id&quot;: 0,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &quot;iscrowd&quot;: 0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            objs.append(obj)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        record[&quot;annotations&quot;] = objs</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        dataset_dicts.append(record)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    return dataset_dicts</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from detectron2.data import DatasetCatalog, MetadataCatalog</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">for d in [&quot;train&quot;, &quot;val&quot;]:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    DatasetCatalog.register(&quot;balloon/&quot; + d, lambda d=d: get_balloon_dicts(&quot;balloon/&quot; + d))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    MetadataCatalog.get(&quot;balloon/&quot; + d).set(thing_classes=[&quot;balloon&quot;])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">balloon_metadata = MetadataCatalog.get(&quot;balloon/train&quot;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="model-configuration-and-training">Model configuration and training<a class="hash-link" href="#model-configuration-and-training" title="Direct link to heading">​</a></h3><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">from detectron2.engine import DefaultTrainer</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from detectron2.config import get_cfg</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg = get_cfg()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.merge_from_file(model_zoo.get_config_file(&quot;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.DATASETS.TRAIN = (&quot;balloon/train&quot;,)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.DATASETS.TEST = ()   # no metrics implemented for this dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.DATALOADER.NUM_WORKERS = 2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(&quot;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.SOLVER.IMS_PER_BATCH = 2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.SOLVER.BASE_LR = 0.00025</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough, but you can certainly train longer</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">trainer = DefaultTrainer(cfg) </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">trainer.resume_or_load(resume=False)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">trainer.train()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="inference-and-visualization">Inference and Visualization<a class="hash-link" href="#inference-and-visualization" title="Direct link to heading">​</a></h3><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">from detectron2.utils.visualizer import ColorMode</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># load weights</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, &quot;model_final.pth&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Set training data-set path</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.DATASETS.TEST = (&quot;balloon/val&quot;, )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Create predictor (model for inference)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">predictor = DefaultPredictor(cfg)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">dataset_dicts = get_balloon_dicts(&quot;balloon/val&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">for d in random.sample(dataset_dicts, 3):    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    im = cv2.imread(d[&quot;file_name&quot;])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    outputs = predictor(im)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    v = Visualizer(im[:, :, ::-1],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                   metadata=balloon_metadata, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                   scale=0.8, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    v = v.draw_instance_predictions(outputs[&quot;instances&quot;].to(&quot;cpu&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    cv2_imshow(v.get_image()[:, :, ::-1])</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p><img loading="lazy" alt="/img/content-blog-raw-blog-detectron-2-untitled-7.png" src="/ai/assets/images/content-blog-raw-blog-detectron-2-untitled-7-2e7f9a8cba8cc49ee8ab648cf18bc33b.png" width="819" height="540"></p><p><img loading="lazy" alt="/img/content-blog-raw-blog-detectron-2-untitled-8.png" src="/ai/assets/images/content-blog-raw-blog-detectron-2-untitled-8-9d5d0a41fa5a618be93561523b131930.png" width="819" height="614"></p><p><img loading="lazy" alt="/img/content-blog-raw-blog-detectron-2-untitled-9.png" src="/ai/assets/images/content-blog-raw-blog-detectron-2-untitled-9-67a9478fd72c9f64dc0f058a5395edb8.png" width="548" height="819"></p><h1>Fine-tuning Chip Dataset</h1><h3 class="anchor anchorWithStickyNavbar_mojV" id="load-the-data-1">Load the data<a class="hash-link" href="#load-the-data-1" title="Direct link to heading">​</a></h3><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">#get the dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">!pip install -q kaggle</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">!pip install -q kaggle-cli</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">os.environ[&#x27;KAGGLE_USERNAME&#x27;] = &quot;sparshag&quot; </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">os.environ[&#x27;KAGGLE_KEY&#x27;] = &quot;1b1f894d1fa6febe9676681b44ad807b&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">!kaggle datasets download -d tannergi/microcontroller-detection</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">!unzip microcontroller-detection.zip</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="convert-dataset-into-detectron2s-standard-format-1">Convert dataset into Detectron2&#x27;s standard format<a class="hash-link" href="#convert-dataset-into-detectron2s-standard-format-1" title="Direct link to heading">​</a></h3><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain"># Registering the dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from detectron2.structures import BoxMode</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">def get_microcontroller_dicts(csv_file, img_dir):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    df = pd.read_csv(csv_file)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    df[&#x27;filename&#x27;] = df[&#x27;filename&#x27;].map(lambda x: img_dir+x)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    classes = [&#x27;Raspberry_Pi_3&#x27;, &#x27;Arduino_Nano&#x27;, &#x27;ESP8266&#x27;, &#x27;Heltec_ESP32_Lora&#x27;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    df[&#x27;class_int&#x27;] = df[&#x27;class&#x27;].map(lambda x: classes.index(x))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    dataset_dicts = []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    for filename in df[&#x27;filename&#x27;].unique().tolist():</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        record = {}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        height, width = cv2.imread(filename).shape[:2]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        record[&quot;file_name&quot;] = filename</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        record[&quot;height&quot;] = height</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        record[&quot;width&quot;] = width</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        objs = []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        for index, row in df[(df[&#x27;filename&#x27;]==filename)].iterrows():</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          obj= {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">              &#x27;bbox&#x27;: [row[&#x27;xmin&#x27;], row[&#x27;ymin&#x27;], row[&#x27;xmax&#x27;], row[&#x27;ymax&#x27;]],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">              &#x27;bbox_mode&#x27;: BoxMode.XYXY_ABS,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">              &#x27;category_id&#x27;: row[&#x27;class_int&#x27;],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">              &quot;iscrowd&quot;: 0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          objs.append(obj)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        record[&quot;annotations&quot;] = objs</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        dataset_dicts.append(record)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    return dataset_dicts</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">classes = [&#x27;Raspberry_Pi_3&#x27;, &#x27;Arduino_Nano&#x27;, &#x27;ESP8266&#x27;, &#x27;Heltec_ESP32_Lora&#x27;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">for d in [&quot;train&quot;, &quot;test&quot;]:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  DatasetCatalog.register(&#x27;microcontroller/&#x27; + d, lambda d=d: get_microcontroller_dicts(&#x27;Microcontroller Detection/&#x27; + d + &#x27;_labels.csv&#x27;, &#x27;Microcontroller Detection/&#x27; + d+&#x27;/&#x27;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  MetadataCatalog.get(&#x27;microcontroller/&#x27; + d).set(thing_classes=classes)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">microcontroller_metadata = MetadataCatalog.get(&#x27;microcontroller/train&#x27;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="model-configuration-and-training-1">Model configuration and training<a class="hash-link" href="#model-configuration-and-training-1" title="Direct link to heading">​</a></h3><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain"># Train the model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg = get_cfg()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.merge_from_file(model_zoo.get_config_file(&quot;COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.DATASETS.TRAIN = (&#x27;microcontroller/train&#x27;,)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.DATASETS.TEST = ()   # no metrics implemented for this dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.DATALOADER.NUM_WORKERS = 2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(&quot;COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.SOLVER.IMS_PER_BATCH = 2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.SOLVER.MAX_ITER = 1000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">trainer = DefaultTrainer(cfg) </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">trainer.resume_or_load(resume=False)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">trainer.train()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p><img loading="lazy" alt="/img/content-blog-raw-blog-detectron-2-untitled-10.png" src="/ai/assets/images/content-blog-raw-blog-detectron-2-untitled-10-8d5237190905a0e4fadc0b0ed691aef3.png" width="765" height="578"></p><p><img loading="lazy" alt="/img/content-blog-raw-blog-detectron-2-untitled-11.png" src="/ai/assets/images/content-blog-raw-blog-detectron-2-untitled-11-4da902dcc9cb45b630b4918e12ffd40b.png" width="640" height="480"></p><h3 class="anchor anchorWithStickyNavbar_mojV" id="inference-and-visualization-1">Inference and Visualization<a class="hash-link" href="#inference-and-visualization-1" title="Direct link to heading">​</a></h3><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, &quot;model_final.pth&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8   # set the testing threshold for this model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.DATASETS.TEST = (&#x27;microcontroller/test&#x27;, )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">predictor = DefaultPredictor(cfg)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df_test = pd.read_csv(&#x27;Microcontroller Detection/test_labels.csv&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">dataset_dicts = DatasetCatalog.get(&#x27;microcontroller/test&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">for d in random.sample(dataset_dicts, 3):    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    im = cv2.imread(d[&quot;file_name&quot;])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    outputs = predictor(im)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    v = Visualizer(im[:, :, ::-1], </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                   metadata=microcontroller_metadata, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                   scale=0.8</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                   )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    v = v.draw_instance_predictions(outputs[&quot;instances&quot;].to(&quot;cpu&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    cv2_imshow(v.get_image()[:, :, ::-1])</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="real-time-webcam-inference">Real-time Webcam inference<a class="hash-link" href="#real-time-webcam-inference" title="Direct link to heading">​</a></h3><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">from IPython.display import display, Javascript</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from google.colab.output import eval_js</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from base64 import b64decode</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">def take_photo(filename=&#x27;photo.jpg&#x27;, quality=0.8):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  js = Javascript(&#x27;&#x27;&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    async function takePhoto(quality) {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      const div = document.createElement(&#x27;div&#x27;);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      const capture = document.createElement(&#x27;button&#x27;);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      capture.textContent = &#x27;Capture&#x27;;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      div.appendChild(capture);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      const video = document.createElement(&#x27;video&#x27;);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      video.style.display = &#x27;block&#x27;;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      const stream = await navigator.mediaDevices.getUserMedia({video: true});</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      document.body.appendChild(div);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      div.appendChild(video);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      video.srcObject = stream;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      await video.play();</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      // Resize the output to fit the video element.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      // Wait for Capture to be clicked.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      await new Promise((resolve) =&gt; capture.onclick = resolve);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      const canvas = document.createElement(&#x27;canvas&#x27;);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      canvas.width = video.videoWidth;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      canvas.height = video.videoHeight;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      canvas.getContext(&#x27;2d&#x27;).drawImage(video, 0, 0);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      stream.getVideoTracks()[0].stop();</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      div.remove();</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      return canvas.toDataURL(&#x27;image/jpeg&#x27;, quality);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &#x27;&#x27;&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  display(js)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  data = eval_js(&#x27;takePhoto({})&#x27;.format(quality))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  binary = b64decode(data.split(&#x27;,&#x27;)[1])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  with open(filename, &#x27;wb&#x27;) as f:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    f.write(binary)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  return filename</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from IPython.display import Image</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">try:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  filename = take_photo()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  print(&#x27;Saved to {}&#x27;.format(filename))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Show the image which was just taken.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  display(Image(filename))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">except Exception as err:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Errors will be thrown if the user does not have a webcam or if they do not</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # grant the page permission to access it.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  print(str(err))</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">model_path = &#x27;/content/output/model_final.pth&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">config_path= model_zoo.get_config_file(&quot;COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Create config</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg = get_cfg()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.merge_from_file(config_path)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.MODEL.WEIGHTS = model_path</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">predictor = DefaultPredictor(cfg)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">im = cv2.imread(&#x27;photo.jpg&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">outputs = predictor(im)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">v = v.draw_instance_predictions(outputs[&quot;instances&quot;].to(&quot;cpu&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cv2_imshow(v.get_image()[:, :, ::-1])</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h1>Fine-tuning on Face dataset</h1><p>The process is same. Here is the output.</p><p><img loading="lazy" alt="/img/content-blog-raw-blog-detectron-2-untitled-12.png" src="/ai/assets/images/content-blog-raw-blog-detectron-2-untitled-12-8624038661cf0ab9a13ffd35bcea2092.png" width="1353" height="550"></p><p><img loading="lazy" alt="/img/content-blog-raw-blog-detectron-2-untitled-13.png" src="/ai/assets/images/content-blog-raw-blog-detectron-2-untitled-13-9ef0b07eeb23c36fea216ff6d8776168.png" width="700" height="400"></p><p><img loading="lazy" alt="/img/content-blog-raw-blog-detectron-2-untitled-14.png" src="/ai/assets/images/content-blog-raw-blog-detectron-2-untitled-14-4eb13d5f04434cff475a78207b96071c.png" width="700" height="400"></p><h3 class="anchor anchorWithStickyNavbar_mojV" id="behind-the-scenes">Behind the scenes<a class="hash-link" href="#behind-the-scenes" title="Direct link to heading">​</a></h3><p><img loading="lazy" alt="/img/content-blog-raw-blog-detectron-2-untitled-15.png" src="/ai/assets/images/content-blog-raw-blog-detectron-2-untitled-15-3bcd41db9aeb9c74ee059c1848fbc157.png" width="1071" height="563"></p><h3 class="anchor anchorWithStickyNavbar_mojV" id="references">References<a class="hash-link" href="#references" title="Direct link to heading">​</a></h3><ul><li><a href="https://medium.com/deepvisionguru/how-to-embed-detectron2-in-your-computer-vision-project-817f29149461" target="_blank" rel="noopener noreferrer">How to embed Detectron2 in your computer vision project - blogpost</a></li><li><a href="https://gilberttanner.com/blog/detectron2-train-a-instance-segmentation-model" target="_blank" rel="noopener noreferrer">Detectron2 Train a Instance Segmentation Model by Gilbert Tanner</a></li><li><a href="https://www.dlology.com/blog/how-to-train-detectron2-with-custom-coco-datasets/" target="_blank" rel="noopener noreferrer">How to train Detectron2 with Custom COCO Datasets - DLology</a></li><li><a href="https://towardsdatascience.com/character-recognition-and-segmentation-for-custom-data-using-detectron2-599de82b393c" target="_blank" rel="noopener noreferrer">Character Recognition and Segmentation For Custom Data Using Detectron2 - blogpost</a></li><li><a href="https://www.celantur.com/blog/panoptic-segmentation-in-detectron2/" target="_blank" rel="noopener noreferrer">Training models with Panoptic Segmentation in Detectron2</a></li><li><a href="https://www.kaggle.com/lewisgmorris/image-segmentation-using-detectron2" target="_blank" rel="noopener noreferrer">Image segmentation using Detectron2 - Kaggle</a></li><li><a href="https://towardsdatascience.com/a-beginners-guide-to-object-detection-and-computer-vision-with-facebook-s-detectron2-700b6273390e" target="_blank" rel="noopener noreferrer">A Beginner’s Guide To Object Detection And Computer Vision With Facebook’s Detectron2</a></li><li><a href="https://www.curiousily.com/posts/face-detection-on-custom-dataset-with-detectron2-in-python/" target="_blank" rel="noopener noreferrer">Face Detection on Custom Dataset with Detectron2 and PyTorch using Python</a></li><li><a href="https://www.notion.so/Detectron-2-d31ac9c14a8d4d9888882df14a4e0eee" target="_blank" rel="noopener noreferrer">My Experiment Notion</a></li><li><a href="https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5" target="_blank" rel="noopener noreferrer">Official Colab</a></li><li><a href="https://research.fb.com/wp-content/uploads/2019/12/4.-detectron2.pdf" target="_blank" rel="noopener noreferrer">Official Slide</a></li><li><a href="https://github.com/facebookresearch/detectron2" target="_blank" rel="noopener noreferrer">Official Git</a></li></ul></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai/blog/tags/tool">tool</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai/blog/tags/vision">vision</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/ai/blog/2021/10/01/distributed-training-of-recommender-systems">Distributed Training of Recommender Systems</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2021-10-01T00:00:00.000Z" itemprop="datePublished">October 1, 2021</time> · <!-- -->6 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://avatars.githubusercontent.com/u/62965911?v=4" alt="Sparsh Agarwal"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sparsh Agarwal</span></a></div><small class="avatar__subtitle" itemprop="description">Principal Developer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>The usage and importance of recommender systems are increasing at a fast pace. And deep learning is gaining traction as the preferred choice for model architecture. Giants like Google and Facebook are already using recommenders to earn billions of dollars.</p><p>Recently, Facebook shared its approach to maintain its 12 trillion parameter recommender. Building these large systems is challenging because it requires huge computation and memory resources. And we will soon enter into 100 trillion range. And SMEs will not be left behind due to open-source environment of software architectures and the decreasing cost of hardware, especially on the cloud infrastructure.</p><p>As per one estimate, a model with 100 trillion parameters would require at least 200TB just to store the model, even at 16-bit floating-point accuracy. So we need architectures that can support efficient and distributed training of recommendation models.</p><p><strong><em>Memory-intensive vs Computation-intensive</em></strong>: The increasing parameter comes mostly from the embedding layer which maps each entrance of an ID type feature (such as an user ID and a session ID) into a fixed length low-dimensional embedding vector. Consider the billion scale of entrances for the ID type features in a production recommender system and the wide utilization of feature crosses, the embedding layer usually domains the parameter space, which makes this component extremely <strong>memory-intensive</strong>. On the other hand, these low-dimensional embedding vectors are concatenated with diversified Non-ID type features (e.g., image, audio, video, social network, etc.) to feed a group of increasingly sophisticated neural networks (e.g., convolution, LSTM, multi-head attention) for prediction(s). Furthermore, in practice, multiple objectives can also be combined and optimized simultaneously for multiple tasks. These mechanisms make the rest neural network increasingly <strong>computation-intensive</strong>.</p><p><img loading="lazy" alt="An example of a recommender models with 100+ trillions of parameter in the embedding layer and 50+ TFLOP computation in the neural network." src="/ai/assets/images/content-blog-raw-blog-distributed-training-of-recommender-systems-untitled-76057748d7f785bcb03bc9fae4560fc3.png" width="881" height="562"></p><p>An example of a recommender models with 100+ trillions of parameter in the embedding layer and 50+ TFLOP computation in the neural network.</p><p><a href="https://github.com/alibaba/x-deeplearning" target="_blank" rel="noopener noreferrer">Alibaba&#x27;s XDL</a>, <a href="https://github.com/PaddlePaddle/PaddleRec" target="_blank" rel="noopener noreferrer">Baidu&#x27;s PaddleRec</a>, and <a href="https://github.com/persiaml/persia" target="_blank" rel="noopener noreferrer">Kwai&#x27;s Persia</a> are some open-source frameworks for this large-scale distributed training of recommender systems.</p><aside>📌 ***Synchronous vs Asynchronous Algorithms***: Synchronous algorithms always use the up-to-date gradient to update the model to ensure the model accuracy. However, the overhead of communications for synchronous algorithms starts to become too expensive to scale out the training procedure, causing inefficiency in running time. While asynchronous algorithm have better hardware efficiency, it often leads to a “significant” loss in model accuracy at this scale—for production recommender systems (e.g., Baidu’s search engine). Recall that even 0.1% drop of accuracy would lead to a noticeable loss in revenue.</aside><h3 class="anchor anchorWithStickyNavbar_mojV" id="parameter-server-framework">Parameter Server Framework<a class="hash-link" href="#parameter-server-framework" title="Direct link to heading">​</a></h3><p>Existing distributed systems for deep learning based recommender models are usually built on top of the parameter server (PS) framework, where one can add elastic distributed storage to hold the increasingly large amount of parameters of the embedding layer. On the other hand, the computation workload does not scale linearly with the increasing parameter scale of the embedding layer—in fact, with an efficient implementation, a lookup operation over a larger embedding table would introduce almost no additional computations.</p><p><img loading="lazy" alt="Left: deep learning based recommender model training workflow over a heterogeneous cluster. Right: Gantt charts to compare fully synchronous, fully asynchronous, raw hybrid and optimized hybrid modes of distributed training of the deep learning recommender model. [Source](https://arxiv.org/pdf/2111.05897v1.pdf)." src="/ai/assets/images/content-blog-raw-blog-distributed-training-of-recommender-systems-untitled-1-64afd6c4cb479b89e18f624461bb9641.png" width="1170" height="391"></p><p>Left: deep learning based recommender model training workflow over a heterogeneous cluster. Right: Gantt charts to compare fully synchronous, fully asynchronous, raw hybrid and optimized hybrid modes of distributed training of the deep learning recommender model. <a href="https://arxiv.org/pdf/2111.05897v1.pdf" target="_blank" rel="noopener noreferrer">Source</a>.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="persia">PERSIA<a class="hash-link" href="#persia" title="Direct link to heading">​</a></h3><p><strong>PERSIA</strong> (<strong>P</strong>arallel r<strong>E</strong>commendation t<strong>R</strong>aining <strong>S</strong>ystem with hybr<strong>I</strong>d <strong>A</strong>cceleration) is a PyTorch-based system for training deep learning recommendation models on commodity hardware. It supports models containing more than 100 trillion parameters.</p><p>It uses a hybrid training algorithm to tackle the embedding layer and dense neural network modules differently—the embedding layer is trained in an asynchronous fashion to improve the throughput of training samples, while the rest neural network is trained in a synchronous fashion to preserve the statistical efficiency.</p><p>It also uses a distributed system to manage the hybrid computation resources (CPUs and GPUs) to optimize the co-existence of asynchronicity and synchronicity in the training algorithm.</p><p><img loading="lazy" alt="Untitled" src="/ai/assets/images/content-blog-raw-blog-distributed-training-of-recommender-systems-untitled-2-f8f92456dbc99598ab43bdb238450ac0.png" width="588" height="509"></p><p><img loading="lazy" alt="Untitled" src="/ai/assets/images/content-blog-raw-blog-distributed-training-of-recommender-systems-untitled-3-c4a1ad9cdcc4ab78213fff42a2bf2d18.png" width="590" height="428"></p><p>Persia includes a data loader module, a embedding PS (Parameter Server) module, a group of embedding workers over CPU nodes, and a group of NN workers over GPU instances. Each module can be dynamically scaled for different model scales and desired training throughput:</p><ul><li>A data loader that fetches training data from distributed storages such as Hadoop, Kafka, etc;</li><li>A embedding parameter server (embedding PS for short) manages the storage and update of the parameters in the embedding layer $\mathrm{w}^{emb}$;</li><li>A group of embedding workers that runs Algorithm 1 for getting the embedding parameters from the embedding PS; aggregating embedding vectors (potentially) and putting embedding gradients back to embedding PS;</li><li>A group of NN workers that runs the forward-/backward- propagation of the neural network $\mathrm{NN_{w^{nn}}(·)}$.</li></ul><p><img loading="lazy" alt="The architecture of Persia." src="/ai/assets/images/content-blog-raw-blog-distributed-training-of-recommender-systems-untitled-4-717546b660d1b2fcaff856bf274d18e2.png" width="874" height="563"></p><p>The architecture of Persia.</p><p>Logically, the training procedure is conducted by Persia in a data dispatching based paradigm as below:</p><ol><li>The data loader will dispatch the ID type feature $\mathrm{x^{ID}}$ to an embedding worker—the embedding worker will generate an unique sample ID 𝜉 for this sample, buffer this sample ID with the ID type feature $\mathrm{x_\xi^{ID}}$ locally, and returns this ID 𝜉 back the data loader; the data loader will associate this sample’s Non-ID type features and labels with this unique ID.</li><li>Next, the data loader will dispatch the Non-ID type feature and label(s) $\mathrm{(x<em>\xi^{NID},y</em>\xi)}$ to a NN worker.</li><li>Once a NN worker receives this incomplete training sample, it will issue a request to pull the ID type features’ $\mathrm{(x<em>\xi^{ID})}$ embedding $\mathrm{w</em>\xi^{emb}}$ from some embedding worker according to the sample ID 𝜉—this would trigger the forward propagation in Algorithm 1, where the embedding worker will use the buffered ID type feature $\mathrm{x<em>\xi^{ID}}$ to get the corresponding $\mathrm{w</em>\xi^{emb}}$ from the embedding PS.</li><li>Then the embedding worker performs some potential aggregation of original embedding vectors. When this computation finishes, the aggregated embedding vector $\mathrm{w_\xi^{emb}}$ will be transmitted to the NN worker that issues the pull request.</li><li>Once the NN worker gets a group of complete inputs for the dense module, it will create a mini-batch and conduct the training computation of the NN according to Algorithm 2. Note that the parameter of the NN always locates in the device RAM of the NN worker, where the NN workers synchronize the gradients by the AllReduce Paradigm.</li><li>When the iteration of Algorithm 2 is finished, the NN worker will send the gradients of the embedding ($\mathrm{F_\xi^{emb&#x27;}}$) back to the embedding worker (also along with the sample ID 𝜉).</li><li>The embedding worker will query the buffered ID type feature $\mathrm{x<em>\xi^{ID}}$ according to the sample ID 𝜉; compute gradients $\mathrm{F</em>\xi^{emb&#x27;}}$ of the embedding parameters and send the gradients to the embedding PS, so that the embedding PS can finally compute the updates according the embedding parameter’s gradients by its SGD optimizer and update the embedding parameters.</li></ol></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai/blog/tags/distributed">distributed</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai/blog/tags/recsys">recsys</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/ai/blog/2021/10/01/document-recommendation">Document Recommendation</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2021-10-01T00:00:00.000Z" itemprop="datePublished">October 1, 2021</time> · <!-- -->2 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://avatars.githubusercontent.com/u/62965911?v=4" alt="Sparsh Agarwal"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sparsh Agarwal</span></a></div><small class="avatar__subtitle" itemprop="description">Principal Developer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="/img/content-blog-raw-blog-document-recommendation-untitled.png" src="/ai/assets/images/content-blog-raw-blog-document-recommendation-untitled-ccbcca01bf06db66ebeebcfe4da46778.png" width="1720" height="900"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="introduction"><strong>Introduction</strong><a class="hash-link" href="#introduction" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="business-objective">Business objective<a class="hash-link" href="#business-objective" title="Direct link to heading">​</a></h3><p>For the given user query, recommend relevant documents (BRM_ifam)</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="technical-objective">Technical objective<a class="hash-link" href="#technical-objective" title="Direct link to heading">​</a></h3><p>1-to-N mapping of given input text</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="proposed-framework-1--hybrid-recommender-system"><strong>Proposed Framework 1 — Hybrid Recommender System</strong><a class="hash-link" href="#proposed-framework-1--hybrid-recommender-system" title="Direct link to heading">​</a></h2><ul><li>Text → Vector (Universal Sentence Embedding with TF Hub)</li><li>Vector → Content-based Filtering Recommendation</li><li>Index → Interaction Matrix</li><li>Interaction Matrix → Collaborative Filtering Recommendation</li><li>Collaborative + Content-based → Hybrid Recommendation</li><li>Evaluation: Area-under-curve</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="proposed-framework-2--content-based-recommender-system"><strong>Proposed Framework 2 — Content-based Recommender System</strong><a class="hash-link" href="#proposed-framework-2--content-based-recommender-system" title="Direct link to heading">​</a></h2><ol><li>Find A most similar user → Cosine similarity</li><li>For each user in A, find TopK Most Similar Items → Map Argsort</li><li>For each item Find TopL Most Similar Items → Cosine similarity</li><li>Display</li><li>Implement an evaluation metric</li><li>Evaluate</li></ol><h2 class="anchor anchorWithStickyNavbar_mojV" id="results-and-discussion"><strong>Results and Discussion</strong><a class="hash-link" href="#results-and-discussion" title="Direct link to heading">​</a></h2><ul><li>build.py → this script will take the training data as input and save all the required files in the same working directory</li><li>recommend.py → this script will take the user query as input and predict top-K BRM recommendations</li></ul><p>Variables (during recommendation, you will be asked 2–3 choices, the meaning of those choices are as following)</p><ul><li>top-K — how many top items you want to get in recommendation</li><li>secondary items: this will determine how many similar items you would like to add in consideration, for each primary matching item</li><li>sorted by frequency: since multiple input queries might point to same output, therefore this option allows to take that frequence count of outputs in consideration and will move the more frequent items at the top.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="code"><strong>Code</strong><a class="hash-link" href="#code" title="Direct link to heading">​</a></h3><p><a href="https://gist.github.com/sparsh-ai/4e5f06ba3c55192b33a276ee67dbd42c#file-text-recommendations-ipynb" target="_blank" rel="noopener noreferrer">https://gist.github.com/sparsh-ai/4e5f06ba3c55192b33a276ee67dbd42c#file-text-recommendations-ipynb</a></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai/blog/tags/nlp">nlp</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai/blog/tags/similarity">similarity</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/ai/blog/2021/10/01/fake-voice-detection">Fake Voice Detection</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2021-10-01T00:00:00.000Z" itemprop="datePublished">October 1, 2021</time> · <!-- -->3 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://avatars.githubusercontent.com/u/62965911?v=4" alt="Sparsh Agarwal"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sparsh Agarwal</span></a></div><small class="avatar__subtitle" itemprop="description">Principal Developer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="/img/content-blog-raw-blog-fake-voice-detection-untitled.png" src="/ai/assets/images/content-blog-raw-blog-fake-voice-detection-untitled-f11b5243c4377e8265e1a521da103c79.png" width="1920" height="1080"></p><h1>Introduction</h1><p>Fake audio can be used for malicious purposes which affect directly or indirectly human life. The objective is to differentiate between fake and real voice. Python and deep learning has been used and implemented to achieve the objective. Audio files or video file are being used as an input of this work then model has been trained for uniquely identify features for voice creation and voice detection. Deep learning technique is used to find accuracy between real and fake.</p><p>Speaker recognition usually refers to both speaker identification and speaker verification. A speaker identification system identifies who the speaker is, while an automatic speaker verification (ASV) system decides if an identity claim is true or false.
A general ASV system is robust to zero-effort impostors, they are vulnerable to more sophisticated attacks. Such vulnerability represents one of the security concerns of ASV systems. Spoofing involves an adversary (attacker) who masquerades as the target speaker to gain the access to a system. Such spoofing attacks can happen to various biometric traits, such as fingerprints, iris, face, and voice patterns. We are focusing only on the voice-based spoofing and anti-spoofing techniques for ASV system. The spoofed speech samples can be obtained through speech synthesis, voice conversion, or replay of recorded speech. <strong>Imagine the following scenario…</strong>
Your phone rings, you pick up. It’s your spouse asking you for details about your savings account — they don’t have the account information on hand, but want to deposit money there this afternoon. Later, you realize a bunch of money has went missing! After investigating, you find out that the person masquerading as them on the other line was a voice 100% generated with AI. You’ve just been scammed, and on top of that, can’t believe the voice you thought belonged to your spouse was actually a fake.</p><p>To discern between real and fake audio, the detector uses visual representations of audio clips called spectrograms, which are also used to train speech synthesis models.
Google’s 2019 <a href="https://www.blog.google/outreach-initiatives/google-news-initiative/advancing-research-fake-audio-detection/" target="_blank" rel="noopener noreferrer">AVSSpoof dataset</a> contains over 25,000 clips of audio, featuring both real and fake clips of a variety of male and female speakers.<strong>Temporal Convolution Model</strong></p><h1>Modeling Approach</h1><p>First, raw audio is preprocessed and converted into a mel-frequency spectrogram — this is the input for the model. The model performs convolutions over the time dimension of the spectrogram, then uses masked pooling to prevent overfitting. Finally, the output is passed into a dense layer and a sigmoid activation function, which ultimately outputs a predicted probability between 0 (fake) and 1 (real).
The baseline model achieved 99%, 95%, and 85% accuracy on the train, validation, and test sets respectively. The differing performance is caused by differences between the three datasets. While all three datasets feature distinct and different speakers, the test set uses a different set of fake audio generating algorithms that were not present in the train or validation set.</p><h1>Proposed Framework</h1><h1>Process Flow</h1><ul><li>Voice detection<ul><li>Temporal Convolution model<ul><li>Install packages</li><li>Download pretrained models</li><li>Initialize the model</li><li>Load data</li><li>Detect DeepFakes</li></ul></li><li>GMM-UBG model<ul><li>Install packages</li><li>Train the model</li><li>Load data</li><li>Detect DeepFakes</li></ul></li><li>Convolutional VAE model<ul><li>Install packages</li><li>Train the model</li><li>Load data</li><li>Detect DeepFakes</li></ul></li><li>Voice Similarity<ul><li>Install packages</li><li>Load data</li><li>Voice similarity match</li><li>Embedding visualization</li></ul></li></ul></li></ul><h1>Models Algorithms</h1><ol><li>Temporal Convolution</li><li>ResNet</li><li>GMM</li><li>Light CNN</li><li>Fusion</li><li>SincNet</li><li>ASSERT</li><li>HOSA</li><li>CVAE</li></ol></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai/blog/tags/audio">audio</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai/blog/tags/deepfake">deepfake</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/ai/blog/2021/10/01/image-similarity-system">Image Similarity System</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2021-10-01T00:00:00.000Z" itemprop="datePublished">October 1, 2021</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://avatars.githubusercontent.com/u/62965911?v=4" alt="Sparsh Agarwal"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sparsh Agarwal</span></a></div><small class="avatar__subtitle" itemprop="description">Principal Developer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="/img/content-blog-raw-blog-image-similarity-system-untitled.png" src="/ai/assets/images/content-blog-raw-blog-image-similarity-system-untitled-59c540a72e9f8afbece0d5a9bd41f513.png" width="1289" height="545"></p><h1>Choice of variables</h1><h3 class="anchor anchorWithStickyNavbar_mojV" id="image-encoder">Image Encoder<a class="hash-link" href="#image-encoder" title="Direct link to heading">​</a></h3><p>We can select any pre-trained image classification model. These models are commonly known as encoders because their job is to encode an image into a feature vector. I analyzed four encoders named 1) MobileNet, 2) EfficientNet, 3) ResNet and 4) <a href="https://tfhub.dev/google/bit/m-r152x4/1" target="_blank" rel="noopener noreferrer">BiT</a>. After basic research, I decided to select BiT model because of its performance and state-of-the-art nature. I selected the BiT-M-50x3 variant of model which is of size 748 MB. More details about this architecture can be found on the official page <a href="https://tfhub.dev/google/bit/m-r50x3/1" target="_blank" rel="noopener noreferrer">here</a>. </p><h3 class="anchor anchorWithStickyNavbar_mojV" id="vector-similarity-system">Vector Similarity System<a class="hash-link" href="#vector-similarity-system" title="Direct link to heading">​</a></h3><p>Images are represented in a fixed-length feature vector format. For the given input vector, we need to find the TopK most similar vectors, keeping the memory efficiency and real-time retrival objective in mind. I explored the most popular techniques and listed down five of them: Annoy, Cosine distance, L1 distance, Locally Sensitive Hashing (LSH) and Image Deep Ranking. I selected Annoy because of its fast and efficient nature. More details about Annoy can be found on the official page <a href="https://github.com/spotify/annoy" target="_blank" rel="noopener noreferrer">here</a>.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="dataset">Dataset<a class="hash-link" href="#dataset" title="Direct link to heading">​</a></h3><p>I listed down 3 datasets from Kaggle that were best fitting the criteria of this use case: 1) <a href="https://www.kaggle.com/bhaskar2443053/fashion-small?" target="_blank" rel="noopener noreferrer">Fashion Product Images (Small)</a>, 2) <a href="https://www.kaggle.com/trolukovich/food11-image-dataset?" target="_blank" rel="noopener noreferrer">Food-11 image dataset</a> and 3) <a href="https://www.kaggle.com/jessicali9530/caltech256?" target="_blank" rel="noopener noreferrer">Caltech 256 Image Dataset</a>. I selected Fashion dataset and Foods dataset.</p><h1>Literature review</h1><ul><li>Determining Image similarity with Quasi-Euclidean Metric <a href="https://arxiv.org/abs/2006.14644v1" target="_blank" rel="noopener noreferrer">arxiv</a></li><li>CatSIM: A Categorical Image Similarity Metric <a href="https://arxiv.org/abs/2004.09073v1" target="_blank" rel="noopener noreferrer">arxiv</a></li><li>Central Similarity Quantization for Efficient Image and Video Retrieval <a href="https://arxiv.org/abs/1908.00347v5" target="_blank" rel="noopener noreferrer">arxiv</a></li><li>Improved Deep Hashing with Soft Pairwise Similarity for Multi-label Image Retrieval <a href="https://arxiv.org/abs/1803.02987v3" target="_blank" rel="noopener noreferrer">arxiv</a></li><li>Model-based Behavioral Cloning with Future Image Similarity Learning <a href="https://arxiv.org/abs/1910.03157v1" target="_blank" rel="noopener noreferrer">arxiv</a></li><li>Why do These Match? Explaining the Behavior of Image Similarity Models <a href="https://arxiv.org/abs/1905.10797v1" target="_blank" rel="noopener noreferrer">arxiv</a></li><li>Learning Non-Metric Visual Similarity for Image Retrieval <a href="https://arxiv.org/abs/1709.01353v2" target="_blank" rel="noopener noreferrer">arxiv</a></li></ul><h1>Process Flow</h1><h3 class="anchor anchorWithStickyNavbar_mojV" id="step-1-data-acquisition">Step 1: Data Acquisition<a class="hash-link" href="#step-1-data-acquisition" title="Direct link to heading">​</a></h3><p>Download the raw image dataset into a directory. Categorize these images into their respective category directories. Make sure that images are of the same type, JPEG recommended. We will also process the metadata and store it in a serialized file, CSV recommended. </p><h3 class="anchor anchorWithStickyNavbar_mojV" id="step-2-encoder-fine-tuning">Step 2: Encoder Fine-tuning<a class="hash-link" href="#step-2-encoder-fine-tuning" title="Direct link to heading">​</a></h3><p>Download the pre-trained image model and add two additional layers on top of that: the first layer is a feature vector layer and the second layer is the classification layer. We will only train these 2 layers on our data and after training, we will select the feature vector layer as the output of our fine-tuned encoder. After fine-tuning the model, we will save the feature extractor for later use.</p><p><img loading="lazy" alt="Fig: a screenshot of encoder fine-tuning process" src="/ai/assets/images/content-blog-raw-blog-image-similarity-system-untitled-1-66cd649b6c0e03c173f3e0734b2b3312.png" width="1195" height="333"></p><p>Fig: a screenshot of encoder fine-tuning process</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="step-3-image-vectorization">Step 3: Image Vectorization<a class="hash-link" href="#step-3-image-vectorization" title="Direct link to heading">​</a></h3><p>Now, we will use the encoder (prepared in step 2) to encode the images (prepared in step 1). We will save feature vector of each image as an array in a directory. After processing, we will save these embeddings for later use.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="step-4-metadata-and-indexing">Step 4: Metadata and Indexing<a class="hash-link" href="#step-4-metadata-and-indexing" title="Direct link to heading">​</a></h3><p>We will assign a unique id to each image and create dictionaries to locate information of this image: 1) Image id to Image name dictionary, 2) Image id to image feature vector dictionary, and 3) (optional) Image id to metadata product id dictionary. We will also create an image id to image feature vector indexing. Then we will save these dictionaries and index object for later use.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="step-5-api-call">Step 5: API Call<a class="hash-link" href="#step-5-api-call" title="Direct link to heading">​</a></h3><p>We will receive an image from user, encode it with our image encoder, find TopK similar vectors using Indexing object, and retrieve the image (and metadata) using dictionaries. We send these images (and metadata) back to the user.</p><h1>Deployment</h1><p>The API was deployed on AWS cloud infrastructure using AWS Elastic Beanstalk service.</p><p><img loading="lazy" alt="/img/content-blog-raw-blog-image-similarity-system-untitled-2.png" src="/ai/assets/images/content-blog-raw-blog-image-similarity-system-untitled-2-ca3d98690fca750590d55d9899c4d862.png" width="1883" height="593"></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai/blog/tags/aws-beanstalk">aws beanstalk</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai/blog/tags/flask">flask</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai/blog/tags/similarity">similarity</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai/blog/tags/vision">vision</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/ai/blog/2021/10/01/insurance-personalization">Insurance Personalization</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2021-10-01T00:00:00.000Z" itemprop="datePublished">October 1, 2021</time> · <!-- -->10 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://avatars.githubusercontent.com/u/62965911?v=4" alt="Sparsh Agarwal"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sparsh Agarwal</span></a></div><small class="avatar__subtitle" itemprop="description">Principal Developer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>Author: <a href="https://www.altexsoft.com/blog/personalized-insurance/" target="_blank" rel="noopener noreferrer">Alexsoft</a></p><p>In a hyper-connected world, where advanced analytics and smart devices constantly re-assess and monitor risks, the traditional once-a-year insurance policy looks increasingly irrelevant and static. Insurance will become a breathing and living thing that shrinks and scales with time to accommodate the changing risks in the clients’ daily lives. As technology continues to expand, real-time data from connected devices and predictive analysis from AIs and machine learning will enhance personalized insurance to benefit the client and insurer.</p><p>To satisfy the expectations of clients, insurers may need to go beyond the personalization of marketing communication and start personalizing product bundles for individuals.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="what-is-personalized-insurance">What is personalized insurance?<a class="hash-link" href="#what-is-personalized-insurance" title="Direct link to heading">​</a></h2><p><strong>Personalized insurance</strong> is the process of reaching insurance customers with targeted pricing, offers, and messages at the right time. Personalization spans across various types of insurance services, from health to property insurance.</p><p>Some insurers are already defining themselves as trusted advisors aiding people in navigating, anticipating, and eliminating risks rather than just paying the compensation when things go wrong.</p><p>For example, these companies use customer data from wearable and smart devices to monitor the user’s lifestyle. If the user’s data indicate the emergence of a serious medical condition, they can send the customer content designed to change their detrimental lifestyle or recommend immediate treatment. When the customer stays fit, healthy and does not carry out risky activities, their insurance cost will be decreased.</p><p><img loading="lazy" alt="/img/content-blog-raw-blog-insurance-personalization-untitled.png" src="/ai/assets/images/content-blog-raw-blog-insurance-personalization-untitled-a75cbc0f49ac3f2847972d8caea19e5d.png" width="2048" height="1933"></p><p>Insurers can provide personalization to customers at different levels:</p><ul><li><strong>Personalized product bundles.</strong> The insurer offers a wide range of products such as health, car, life, and property insurance. So, clients can choose the specific products they want and group them in a bundle.</li><li><strong>Personalized communications.</strong> Insurers use data collected from smart devices to notify customers about harmful activities and lifestyles. They also send recommendations on lifestyle changes. Some insurers take a step further to provide clients with incentives for a healthy lifestyle.</li><li><strong>Personalized insurance quote.</strong> Customers are able to adjust the price of their insurance premiums by turning off the ones they don’t need at any time. Some insurers enable automatic quote adjustments depending on customer’s behavior (e.g., driving habits) or lifestyle choices (e.g., exercising).</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="why-is-it-important">Why is it important?<a class="hash-link" href="#why-is-it-important" title="Direct link to heading">​</a></h3><p>Collecting and analyzing user data is vital in personalizing products based on individual behavior and preferences. In addition, insurers should use this data to enhance external relationships with their customers and guide their internal processes. This will eventually lead to delightful customer experiences and efficient operations.</p><p>Personalized insurance is important for many reasons:</p><p><strong>Customers expect personalized treatment.</strong> Every customer wants to feel special, and the personalization of your services and products will do just that. It will make them stay loyal to you. Moreover, customers are open for personalization. According to the <a href="https://www.accenture.com/_acnmedia/PDF-95/Accenture-2019-Global-Financial-Services-Consumer-Study.pdf#zoom=50" target="_blank" rel="noopener noreferrer">Accenture study</a>, 95 percent of new customers are ready to share their data in exchange for personalized insurance services. And about 58 percent of conservative users would be willing to do so.</p><p><strong>Driving more effective sales and increasing revenue.</strong> Personalization benefits your sales and income in two ways. First, lots of people are ready to share their data with you in exchange for incentives and reduced premiums. Secondly, having access to clients’ data gives you the ability to target people who are already interested in your product, thereby increasing sales and revenue at a lower cost. You will be able to reach your customers at the right time and with the product they need.</p><p><strong>Streamlining operations and working with customers more accurately.</strong> Having an insight into customer preferences and behavior is crucial if you want to provide personalized services. Data obtained from social media activity, fitness trackers, GPS, and other tech can help you serve customers better.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="success-stories">Success stories<a class="hash-link" href="#success-stories" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="lemonade">Lemonade<a class="hash-link" href="#lemonade" title="Direct link to heading">​</a></h3><p>Use of AI and chatbots to personalize communications. </p><p>Lemonade is a US insurance company that uses Maya – an AI-powered bot, to collect and analyze customer data. Maya acts as a virtual assistant that gets information, provides quotes, and handles payments. It also has the ability to provide customized answers to user’s questions and even help them make changes to existing policies. Lemonade uses Natural Action Synthesis and Natural Language Processing to ensure that Maya gets smarter the more it chats. This is possible because their machine learning model is retrained almost daily.</p><p><img loading="lazy" alt="/img/content-blog-raw-blog-insurance-personalization-untitled-1.png" src="/ai/assets/images/content-blog-raw-blog-insurance-personalization-untitled-1-fb24e59c7af671d2b31b2643e04ef647.png" width="1528" height="861"></p><p>On top of that, the company uses big data analytics to quantify losses and predict risks by placing the client into a risk group and quoting a relevant premium. Customers are grouped according to their risk behaviors. The groups are created using algorithms that collect extensive customer data, such as health conditions.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="cover">Cover<a class="hash-link" href="#cover" title="Direct link to heading">​</a></h3><p><strong><a href="https://cover.com/" target="_blank" rel="noopener noreferrer">Cover</a></strong> is a US-based insurance metasearch company that notifies its clients of price drops for their premiums. Their technology works by scanning the market, looking for discounted and lowered prices of insurance premiums for their clients. Cover blends automation, mobile technology, and expert advice to provide customers with high-quality insurance protection at the best prices.</p><p>Cover compares with policy data and prices from over 30 different insurers. From the start, the customers need to provide answers to some questions, which will be used to match the client with a policy that suits their needs.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="oscar">Oscar<a class="hash-link" href="#oscar" title="Direct link to heading">​</a></h3><p><strong><a href="https://www.hioscar.com/" target="_blank" rel="noopener noreferrer">Oscar</a></strong> is a health insurer that provides its clients with a concierge team of medical professionals who give health advice and help them know if they see the best specialist for their specific health condition. They also help with finding the best doctors that accept Oscar insurance and manage and treat chronic conditions. Also, they set aside a separate concierge team in cases of emergencies that helps with the patient’s discharge and follow-up care.</p><p>Oscar’s mobile app acts as an intermediary between the user and the health system. The platform facilitates the customer’s interaction with their healthcare professionals. Clients can receive their lab reports, medical records, physician recommendations, and virtual care from the app. Oscar has also improved its high-touch services, including telemedicine and an “Ask your concierge” feature that connects users with a health insurance advice team.</p><p><img loading="lazy" alt="/img/content-blog-raw-blog-insurance-personalization-untitled-2.png" src="/ai/assets/images/content-blog-raw-blog-insurance-personalization-untitled-2-cebc30b00f8a5fe5ae9ce0c699fbb183.png" width="413" height="683"></p><h3 class="anchor anchorWithStickyNavbar_mojV" id="alllstate">Alllstate<a class="hash-link" href="#alllstate" title="Direct link to heading">​</a></h3><p>Allstate is an auto insurance company that offers personalized car insurance to its customers using telematics programs called Drivewise and Milewise. Drivewise is offered through a mobile app that monitors the customers driving behavior and provides feedback after each drive. Customers also receive incentives for safe driving. From the app interface, clients can check their rewards and driving behavior for the last 100 trips. The customer’s premium is then calculated based on factors like speeding, abrupt braking, and time of the trip. One of the nice things about Drivewise is that even those who do not have an Allstate care insurance policy can participate in this program. Their Milewise program, as the name suggests, lets customers pay insurance based on the miles covered. So, the app monitors the distance covered by the car, and low-mileage drivers can save on insurance.</p><p><img loading="lazy" alt="/img/content-blog-raw-blog-insurance-personalization-untitled-3.png" src="/ai/assets/images/content-blog-raw-blog-insurance-personalization-untitled-3-2d0f3a78c9c7440706ffa188cd1f6282.png" width="700" height="356"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="how-to-approach-personalization">How to approach personalization?<a class="hash-link" href="#how-to-approach-personalization" title="Direct link to heading">​</a></h2><p><img loading="lazy" alt="/img/content-blog-raw-blog-insurance-personalization-untitled-4.png" src="/ai/assets/images/content-blog-raw-blog-insurance-personalization-untitled-4-d6098cfa3497355d80bdc132990f6615.png" width="1231" height="710"></p><p>Before fully investing in personalization, you need to carefully plan your approach. This will ensure you have all the pieces for success, and it will help you follow through with your plan.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="explore-existing-data">Explore existing data<a class="hash-link" href="#explore-existing-data" title="Direct link to heading">​</a></h3><p>Having customer data is the minimum requirement to provide personalized services. First, you need to envision the type of personalization you want to offer. Then, make sure you have data collection channels that provide you with relevant data needed for your tasks. For instance, some of your documents may contain the required information, and you have to digitize, structure those, or extract specific details for that. So, you should audit your current information and data collection mechanisms to estimate whether you’ll need any additional effort to gather this data. For instance, you may want to use <a href="https://www.altexsoft.com/blog/intelligent-document-processing/" target="_blank" rel="noopener noreferrer">intelligent document processing</a>.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="engage-data-scientists-to-make-the-proof-of-concept-and-carry-out-ab-tests">Engage data scientists to make the proof of concept and carry out A/B tests<a class="hash-link" href="#engage-data-scientists-to-make-the-proof-of-concept-and-carry-out-ab-tests" title="Direct link to heading">​</a></h3><p>Your vision on personalization may not work for every business model. Or your data quality may be low to reach project feasibility. We’ve talked about that while explaining how to approach <a href="https://www.altexsoft.com/blog/business/how-to-estimate-roi-and-costs-for-machine-learning-and-data-science-projects/" target="_blank" rel="noopener noreferrer">ROI calculations with machine learning projects</a>. So, you need to present the data you have to a data science team to run several experiments and build prototypes. Once they are ready, you can roll out your new algorithms for a subset of customers to run A/B tests. Their results may show that the conventional approaches work better for you or help iterate on your assumptions.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="invest-in-data-infrastructure">Invest in data infrastructure<a class="hash-link" href="#invest-in-data-infrastructure" title="Direct link to heading">​</a></h3><p>If the A/B tests show that personalization will work for your business model, that is where automation comes into play. You can start investing in data infrastructure and <a href="https://www.altexsoft.com/blog/data-pipeline-components-and-types/" target="_blank" rel="noopener noreferrer">analytical pipelines</a> to automate data collection and analysis mechanisms.</p><p>You’ll need a <a href="https://www.altexsoft.com/blog/datascience/what-is-data-engineering-explaining-data-pipeline-data-warehouse-and-data-engineer-role/" target="_blank" rel="noopener noreferrer">data engineering team</a> for that. These specialists set up connections with data sources, such as mobile, IoT, and telematics devices, enable automatic data preparation, configure storages, and integrate your infrastructure with business intelligence software that helps explore and visualize data.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="continuously-learn-your-customers-preferences-and-needs">Continuously learn your customers’ preferences and needs<a class="hash-link" href="#continuously-learn-your-customers-preferences-and-needs" title="Direct link to heading">​</a></h3><p>The data you collect is only as good as the insights gained from it. That is why it is vital to have a <a href="https://www.altexsoft.com/blog/business/complete-guide-to-business-intelligence-and-analytics-strategy-steps-processes-and-tools/" target="_blank" rel="noopener noreferrer">comprehensive analytic solution</a>. A high-quality analytic software will transform the data into your most valuable asset. This data will be used to improve product development, make more accurate decisions, and provide personalized services to your customers.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="iterate-on-your-infrastructure-and-algorithms">Iterate on your infrastructure and algorithms<a class="hash-link" href="#iterate-on-your-infrastructure-and-algorithms" title="Direct link to heading">​</a></h3><p>Personalization isn’t a one-time project. Whether you apply machine learning or build personalization based on rule-based systems, you still have to revisit your technology, continuously gather new data, and adapt your workflows.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="ensure-a-personalized-cross-channel-experience">Ensure a personalized cross-channel experience<a class="hash-link" href="#ensure-a-personalized-cross-channel-experience" title="Direct link to heading">​</a></h3><p>Since the data collected from IoT devices and other tech is vital for personalization, it is important to make the customer experience seamless across different communication channels. Therefore, the customer should always be provided with the same level of personalization regardless of the touchpoint.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="challenges">Challenges<a class="hash-link" href="#challenges" title="Direct link to heading">​</a></h2><p><strong>Personalization is financially intensive.</strong> The ability of insurers to personalize insurance differs only marginally between marketing communications and products. Most of them, especially startups, do not have the funds to implement advanced technologies like machine learning needed for personalized insurance. However, insurers do not need to start with all the levels of personalization. They can often start by customizing their customer service, gathering data and insights, and then gradually developing towards more complex systems.</p><p><strong>Complex process involving multiple parties.</strong> Also, it is difficult to balance personalization with financial targets, especially when establishing a price for risk. In-depth personalization of insurance must use data analytics from different sources to ensure that personalized offers reflect the client’s needs as well as the profitability and risks implications for the company.</p><p><strong>Customer data is heavily regulated.</strong> Customer data from different sources are subject to industry regulations and privacy concerns. It is often a difficult task to obtain approval from regulators to use this data. Also, customers are becoming more aware of how companies are using their data and approve strict regulations. That is why laws such as General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA) have been passed, which gives customers more control over their data. Insurers can address this barrier by explaining to people how their systems work and how personal data is used. Read more on <a href="https://www.altexsoft.com/blog/interpretability-machine-learning/" target="_blank" rel="noopener noreferrer">explainable machine learning</a> in our dedicated article. Besides being open, insurers can provide clients with incentives and other services for free in exchange for access to personal data.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai/blog/tags/insurance">insurance</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai/blog/tags/personalization">personalization</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/ai/blog/2021/10/01/name-&amp;-address-parsing">Name &amp; Address Parsing</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2021-10-01T00:00:00.000Z" itemprop="datePublished">October 1, 2021</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://avatars.githubusercontent.com/u/62965911?v=4" alt="Sparsh Agarwal"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sparsh Agarwal</span></a></div><small class="avatar__subtitle" itemprop="description">Principal Developer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="/img/content-blog-raw-blog-name-&amp;amp;-address-parsing-untitled.png" src="/ai/assets/images/content-blog-raw-blog-name-&amp;-address-parsing-untitled-35aab4c126484cf369b9cf212de0564a.png" width="700" height="342"></p><h1>Introduction</h1><p>Create an API that can parse and classify names and addresses given a string. We tried <a href="https://github.com/datamade/probablepeople" target="_blank" rel="noopener noreferrer">probablepeople</a> and <a href="https://github.com/datamade/usaddress" target="_blank" rel="noopener noreferrer">usaddress</a>. These work well separately but need the functionality of these packages combined, and better accuracy than what probablepeople provides.
For the API, I&#x27;d like to mimic <a href="https://parserator.datamade.us/api-docs/" target="_blank" rel="noopener noreferrer">this</a> with some minor modifications.
A few examples: </p><ul><li>&quot;KING JOHN A 5643 ROUTH CREEK PKWY #1314 RICHARDSON TEXAS 750820146 UNITED STATES OF AMERICA&quot; would return type: person; first_name: JOHN; last_name: KING; middle: A; street_address: 5643 ROUTH CREEK PKWY #1314; city: RICHARDSON; state: TEXAS; zip: 75082-0146; country: UNITED STATES OF AMERICA.</li><li>&quot;THRM NGUYEN LIVING TRUST 2720 SUMMERTREE CARROLLTON HOUSTON TEXAS 750062646 UNITED STATES OF AMERICA&quot; would return type: entity; name: THRM NGUYEN LIVING TRUST; street_address: 2720 SUMMERTREE CARROLLTON; state: TEXAS; city: HOUSTON; zip: 75006-2646; country: UNITED STATES OF AMERICA.</li></ul><h1>Modeling Approach</h1><h3 class="anchor anchorWithStickyNavbar_mojV" id="list-of-entities">List of Entities<a class="hash-link" href="#list-of-entities" title="Direct link to heading">​</a></h3><p>List of Entities A - Person, Household, Corporation</p><p>List of Entities B - Person First name, Person Middle name, Person Last name, Street address, City, State, Pincode, Country, Company name</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="endpoint-configuration">Endpoint Configuration<a class="hash-link" href="#endpoint-configuration" title="Direct link to heading">​</a></h3><p><strong>OOR Endpoint</strong></p><p>Input Instance: ANDERSON, EARLINE 1423 NEW YORK AVE FORT WORTH, TX 76104 7522</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">Output Tags:-</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&lt;Type&gt; - Person/Household/Corporation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&lt;GivenName&gt;, &lt;MiddleName&gt;, &lt;Surname&gt; - if Type Person/Household</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&lt;Name&gt; - Full Name - if Type Person </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&lt;Name&gt; - Household - if Type Household</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&lt;Name&gt; - Corporation - If Type Corporation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&lt;Address&gt; - Full Address</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&lt;StreetAddress&gt;, &lt;City&gt;, &lt;State&gt;, &lt;Zipcode&gt;, &lt;Country&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">~~NameConfidence, AddrConfidence~~</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p><strong>Name Endpoint</strong></p><p>Input Instance: ANDERSON, EARLINE</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">Output Tags:-</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- &lt;Type&gt; - Person/Household/Corporation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- &lt;GivenName&gt;, &lt;MiddleName&gt;, &lt;Surname&gt; - if Type Person/Household</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- &lt;Name&gt; - Full Name - if Type Person</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- &lt;Name&gt; - Household - if Type Household</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- &lt;Name&gt; - Corporation - If Type Corporation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- ~~NameConfidence~~</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p><strong>Address Endpoint</strong></p><p>Input Instance: 1423 NEW YORK AVE FORT WORTH, TX 76104 7522</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">Output Tags:-</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- &lt;Address&gt; - Full Address</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- &lt;StreetAddress&gt;, &lt;City&gt;, &lt;State&gt;, &lt;Zipcode&gt;, &lt;Country&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- ~~AddrConfidence~~</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="process-flow">Process Flow<a class="hash-link" href="#process-flow" title="Direct link to heading">​</a></h3><ul><li>Pytorch Flair NER model</li><li>Pre trained word embeddings</li><li>Additional parsing models on top of name tags</li><li>Tagging of 1000+ records to create training data</li><li>Deployment as REST api with 3 endpoints - name parse, address parse and whole string parse</li></ul><h1>Framework</h1><p><img loading="lazy" alt="/img/content-blog-raw-blog-name-&amp;amp;-address-parsing-untitled-1.png" src="/ai/assets/images/content-blog-raw-blog-name-&amp;-address-parsing-untitled-1-0c56bfea9f214ccebee859fd1f6d5b33.png" width="1299" height="447"></p><p><img loading="lazy" alt="/img/content-blog-raw-blog-name-&amp;amp;-address-parsing-untitled-2.png" src="/ai/assets/images/content-blog-raw-blog-name-&amp;-address-parsing-untitled-2-bf22e8e0a5e1f39628fdb8d05e2e14b7.png" width="1302" height="534"></p><h1>Tagging process</h1><p>I used Doccano (<a href="https://github.com/doccano/doccano" target="_blank" rel="noopener noreferrer">https://github.com/doccano/doccano</a>) for labeling the dataset. This tool is open-source and free to use. I deployed it with a one-click Heroku service (fig 1). After launching the app, log in with the provided credentials, and create a project (fig 2). Create the labels and upload the dataset (fig 3). Start the annotation process (fig 4). Now after enough annotations (you do not need complete all annotations in one go), go back to projects &gt; edit section and export the data (fig 5). Bring the exported JSON file in python and run the model training code. The whole model will automatically get trained on the new annotations. To make the training faster, you can use Nvidia GPU support.</p><p><img loading="lazy" alt="fig 1: screenshot taken from Doccano&amp;#39;s github page" src="/ai/assets/images/content-blog-raw-blog-name-&amp;-address-parsing-untitled-3-ad825b4a9077c68bb1069d9c595af800.png" width="435" height="323"></p><p>fig 1: screenshot taken from Doccano&#x27;s github page</p><p><img loading="lazy" alt="fig 2: Doccano&amp;#39;s deployed app homepage" src="/ai/assets/images/content-blog-raw-blog-name-&amp;-address-parsing-untitled-4-3b2954d0a30a62e7231f231eeccd0604.png" width="1166" height="675"></p><p>fig 2: Doccano&#x27;s deployed app homepage</p><p><img loading="lazy" alt="fig 3: create the labels. I defined these labels for my project" src="/ai/assets/images/content-blog-raw-blog-name-&amp;-address-parsing-untitled-5-c02e457933ab867810fe666c34fb4e76.png" width="1349" height="623"></p><p>fig 3: create the labels. I defined these labels for my project</p><p><img loading="lazy" alt="fig 5: export the annotations" src="/ai/assets/images/content-blog-raw-blog-name-&amp;-address-parsing-untitled-6-6de4153eff54d52c35fd5c0bed123a3d.png" width="1342" height="260"></p><p>fig 5: export the annotations</p><h1>Model</h1><p>I first tried the Spacy NER blank model but it was not giving high-quality results. So I moved to the PyTorch Flair NER model. This model was a way faster (5 min training because of GPU compatibility comparing to 1-hour Spacy training time) and also much more accurate. F1 results for all tags were near perfect (score of 1).  This score will increase further with more labeled data. This model is production-ready.</p><h1>Inference</h1><p>For OOR, I directly used the model&#x27;s output for core tagging and created the aggregated tags like recipient (aggregation of name tags) and address (aggregation of address tags like city and state) using simple conditional concatenation. For only Name and only Address inference, I added the dummy address in name text and dummy name in address text. This way, I passed the text in same model and later on filtered the required tags as output. </p><h3 class="anchor anchorWithStickyNavbar_mojV" id="api">API<a class="hash-link" href="#api" title="Direct link to heading">​</a></h3><p>I used Flask REST framework in Python to build the API with 3 endpoints. This API is production-ready.</p><h1>Results and Discussion</h1><ul><li>0.99 F1 score on 6 out of 8 tags &amp; 0.95+ F1 score on other 2 tags</li><li>API inference time of less than 1 second on single CPU</li></ul></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai/blog/tags/app">app</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai/blog/tags/flask">flask</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai/blog/tags/ner">ner</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai/blog/tags/nlp">nlp</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/ai/blog/2021/10/01/object-detection-hands-on-exercises">Object Detection Hands-on Exercises</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2021-10-01T00:00:00.000Z" itemprop="datePublished">October 1, 2021</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://avatars.githubusercontent.com/u/62965911?v=4" alt="Sparsh Agarwal"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sparsh Agarwal</span></a></div><small class="avatar__subtitle" itemprop="description">Principal Developer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>We are going to discuss the following 4 use cases:</p><ol><li>Detect faces, eyes, pedestrians, cars, and number plates using OpenCV haar cascade classifiers</li><li>Streamlit app for MobileNet SSD Caffe Pre-trained model</li><li>Streamlit app for various object detection models and use cases</li><li>Detect COCO-80 class objects in videos using TFHub MobileNet SSD model</li></ol><h3 class="anchor anchorWithStickyNavbar_mojV" id="use-case-1----object-detection-with-opencv">Use Case 1 -  <strong>Object detection with OpenCV</strong><a class="hash-link" href="#use-case-1----object-detection-with-opencv" title="Direct link to heading">​</a></h3><p><strong>Face detection</strong> - We will use the frontal face Haar cascade classifier model to detect faces in the given image. The following function first passes the given image into the classifier model to detect a list of face bounding boxes and then runs a loop to draw a red rectangle box around each detected face in the image:</p><div class="codeBlockContainer_I0IT language-python theme-code-block"><div class="codeBlockContent_wNvx python"><pre tabindex="0" class="prism-code language-python codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">detect_faces</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">fix_img</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    face_rects </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> face_classifier</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">detectMultiScale</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">fix_img</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> y</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> w</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> h</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> face_rects</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        cv2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">rectangle</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">fix_img</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                     </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">y</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                     </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token operator" style="color:#393A34">+</span><span class="token plain">w</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> y</span><span class="token operator" style="color:#393A34">+</span><span class="token plain">h</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                     </span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">255</span><span class="token punctuation" style="color:#393A34">,</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">,</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                     </span><span class="token number" style="color:#36acaa">10</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> fix_img</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p><strong>Eyes detection</strong> - The process is almost similar to the face detection process. Instead of frontal face Haar cascade, we will use the eye detection Haar cascade model.</p><p><img loading="lazy" alt="Input image" src="/ai/assets/images/content-blog-raw-blog-object-detection-with-opencv-untitled-44fc11078a43a07dc3e4ec7f84197d25.png" width="381" height="223"></p><p>Input image</p><p><img loading="lazy" alt="detected faces and eyes in the image" src="/ai/assets/images/content-blog-raw-blog-object-detection-with-opencv-untitled-1-d023975b118cde980a874199f93b9035.png" width="381" height="223"></p><p>detected faces and eyes in the image</p><p><strong>Pedestrian detection</strong> - We will use the full-body Haar cascade classifier model for pedestrian detection. We will apply this model to a video this time. The following function will run the model on each frame of the video to detect the pedestrians:</p><div class="codeBlockContainer_I0IT language-python theme-code-block"><div class="codeBlockContent_wNvx python"><pre tabindex="0" class="prism-code language-python codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># While Loop</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">while</span><span class="token plain"> cap</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">isOpened</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Read the capture</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        ret</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> frame </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> cap</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">read</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Pass the Frame to the Classifier</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        bodies </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> body_classifier</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">detectMultiScale</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">frame</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1.2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">3</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># if Statement</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> ret </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Bound Boxes to Identified Bodies</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">y</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">w</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">h</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> bodies</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            cv2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">rectangle</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">frame</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                         </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">y</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                         </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token operator" style="color:#393A34">+</span><span class="token plain">w</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> y</span><span class="token operator" style="color:#393A34">+</span><span class="token plain">h</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                         </span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">25</span><span class="token punctuation" style="color:#393A34">,</span><span class="token number" style="color:#36acaa">125</span><span class="token punctuation" style="color:#393A34">,</span><span class="token number" style="color:#36acaa">225</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                         </span><span class="token number" style="color:#36acaa">5</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            cv2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">imshow</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&#x27;Pedestrians&#x27;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> frame</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Exit with Esc button</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> cv2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">waitKey</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">27</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token keyword" style="color:#00009f">break</span><span class="token plain">  </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># else Statement</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">else</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">break</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Release the Capture &amp; Destroy All Windows</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cap</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">release</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cv2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">destroyAllWindows</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p><strong>Car detection</strong> - The process is almost similar to the pedestrian detection process. Again, we will use this model on a video. Instead of people Haar cascade, we will use the car cascade model.</p><p><strong>Car number plate detection</strong> - The process is almost similar to the face and eye detection process. We will use the car number plate cascade model.</p><p><em>You can find the code <a href="https://github.com/sparsh-ai/0D7ACA15" target="_blank" rel="noopener noreferrer">here</a> on Github.</em></p><h3 class="anchor anchorWithStickyNavbar_mojV" id="use-case-2---mobilenet-ssd-caffe-pre-trained-model">Use Case 2 - MobileNet SSD Caffe Pre-trained model<a class="hash-link" href="#use-case-2---mobilenet-ssd-caffe-pre-trained-model" title="Direct link to heading">​</a></h3><p><em>You can play with the live app <a href="https://share.streamlit.io/sparsh-ai/streamlit-5a407279/app.py" target="_blank" rel="noopener noreferrer">here</a>. Souce code is available</em> <a href="https://github.com/sparsh-ai/streamlit-489fbbb7" target="_blank" rel="noopener noreferrer">here</a> <em>on Github.</em></p><h3 class="anchor anchorWithStickyNavbar_mojV" id="use-case-3---yolo-object-detection-app">Use Case 3 - YOLO Object Detection App<a class="hash-link" href="#use-case-3---yolo-object-detection-app" title="Direct link to heading">​</a></h3><p><em>You can play with the live app</em> <a href="https://share.streamlit.io/sparsh-ai/streamlit-489fbbb7/app.py" target="_blank" rel="noopener noreferrer">*here</a>. Source code is available <a href="https://github.com/sparsh-ai/streamlit-5a407279/tree/master" target="_blank" rel="noopener noreferrer">here</a> on Github.*</p><p>This app can detect COCO 80-classes using three different models - Caffe MobileNet SSD, Yolo3-tiny, and Yolo3. It can also detect faces using two different models - SSD Res10 and OpenCV face detector.  Yolo3-tiny can also detect fires.</p><p><img loading="lazy" alt="/img/content-blog-raw-blog-object-detection-with-yolo3-untitled.png" src="/ai/assets/images/content-blog-raw-blog-object-detection-with-yolo3-untitled-7c24204b83a8bd10c57c53b4b2423899.png" width="1366" height="645"></p><p><img loading="lazy" alt="/img/content-blog-raw-blog-object-detection-with-yolo3-untitled-1.png" src="/ai/assets/images/content-blog-raw-blog-object-detection-with-yolo3-untitled-1-c0c4b05a1cf3256f1a9d0ebdb5f4bfb5.png" width="1366" height="640"></p><h3 class="anchor anchorWithStickyNavbar_mojV" id="use-case-4---tfhub-mobilenet-ssd-on-videos">Use Case 4 - TFHub MobileNet SSD on Videos<a class="hash-link" href="#use-case-4---tfhub-mobilenet-ssd-on-videos" title="Direct link to heading">​</a></h3><p>In this section, we will use the MobileNet SSD object detection model from TFHub. We will apply it to videos. We can load the model using the following command:</p><div class="codeBlockContainer_I0IT language-python theme-code-block"><div class="codeBlockContent_wNvx python"><pre tabindex="0" class="prism-code language-python codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">module_handle </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">detector </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> hub</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">load</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">module_handle</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">signatures</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">&#x27;default&#x27;</span><span class="token punctuation" style="color:#393A34">]</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>After loading the model, we will capture frames using OpenCV video capture method, and pass each frame through the detection model:</p><div class="codeBlockContainer_I0IT language-python theme-code-block"><div class="codeBlockContent_wNvx python"><pre tabindex="0" class="prism-code language-python codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">cap </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> cv2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">VideoCapture</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&#x27;/content/Spectre_opening_highest_for_a_James_Bond_film_in_India.mp4&#x27;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> i </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">total_frames</span><span class="token punctuation" style="color:#393A34">,</span><span class="token number" style="color:#36acaa">200</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    cap</span><span class="token punctuation" style="color:#393A34">.</span><span class="token builtin">set</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">cv2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">CAP_PROP_POS_FRAMES</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">i</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ret</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">frame </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> cap</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">read</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    frame </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> cv2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">cvtColor</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">frame</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> cv2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">COLOR_BGR2RGB</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    run_detector</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">detector</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">frame</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Here are some detected objects in frames: </p><p><img loading="lazy" alt="/img/content-blog-raw-blog-object-detection-hands-on-exercises-untitled.png" src="/ai/assets/images/content-blog-raw-blog-object-detection-hands-on-exercises-untitled-7bd46bee77946cff46d6a834699b3685.png" width="1156" height="662"></p><p><img loading="lazy" alt="/img/content-blog-raw-blog-object-detection-hands-on-exercises-untitled-1.png" src="/ai/assets/images/content-blog-raw-blog-object-detection-hands-on-exercises-untitled-1-0a9f162ff084467ebd27ea587d055584.png" width="1156" height="662"></p><p><img loading="lazy" alt="/img/content-blog-raw-blog-object-detection-hands-on-exercises-untitled-2.png" src="/ai/assets/images/content-blog-raw-blog-object-detection-hands-on-exercises-untitled-2-24de87f6b567ef8d72bccc20bfc2328f.png" width="1156" height="662"></p><p><em>You can find the code <a href="https://gist.github.com/sparsh-ai/32ff6fe8c073f6be5d893029e4dc2960" target="_blank" rel="noopener noreferrer">here</a> on Github.</em></p><hr><p><em>Congrats! In the next post of this series, we will cover 5 exciting use cases - 1) detectron 2 object detection fine-tuning on custom class, 2) Tensorflow Object detection API inference, fine-tuning, and few-shot learning, 3) Inference with 6 pre-trained models, 4) Mask R-CNN object detection app, and 5) Logo detection app deployment as a Rest API using AWS elastic Beanstalk.</em></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai/blog/tags/object-detection">object detection</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai/blog/tags/vision">vision</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/ai/blog/2021/10/01/object-detection-with-opencv">Object detection with OpenCV</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2021-10-01T00:00:00.000Z" itemprop="datePublished">October 1, 2021</time> · <!-- -->2 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://avatars.githubusercontent.com/u/62965911?v=4" alt="Sparsh Agarwal"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sparsh Agarwal</span></a></div><small class="avatar__subtitle" itemprop="description">Principal Developer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithStickyNavbar_mojV" id="face-detection"><strong>Face detection</strong><a class="hash-link" href="#face-detection" title="Direct link to heading">​</a></h2><p>We will use the frontal face Haar cascade classifier model to detect faces in the given image. The following function first passes the given image into the classifier model to detect a list of face bounding boxes and then runs a loop to draw a red rectangle box around each detected face in the image:</p><div class="codeBlockContainer_I0IT language-python theme-code-block"><div class="codeBlockContent_wNvx python"><pre tabindex="0" class="prism-code language-python codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">detect_faces</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">fix_img</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    face_rects </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> face_classifier</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">detectMultiScale</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">fix_img</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> y</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> w</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> h</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> face_rects</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        cv2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">rectangle</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">fix_img</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                     </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">y</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                     </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token operator" style="color:#393A34">+</span><span class="token plain">w</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> y</span><span class="token operator" style="color:#393A34">+</span><span class="token plain">h</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                     </span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">255</span><span class="token punctuation" style="color:#393A34">,</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">,</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                     </span><span class="token number" style="color:#36acaa">10</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> fix_img</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h2 class="anchor anchorWithStickyNavbar_mojV" id="eyes-detection"><strong>Eyes detection</strong><a class="hash-link" href="#eyes-detection" title="Direct link to heading">​</a></h2><p>The process is almost similar to the face detection process. Instead of frontal face Haar cascade, we will use the eye detection Haar cascade model.</p><p><img loading="lazy" alt="Input image" src="/ai/assets/images/content-blog-raw-blog-object-detection-with-opencv-untitled-44fc11078a43a07dc3e4ec7f84197d25.png" width="381" height="223"></p><p>Input image</p><p><img loading="lazy" alt="detected faces and eyes in the image" src="/ai/assets/images/content-blog-raw-blog-object-detection-with-opencv-untitled-1-d023975b118cde980a874199f93b9035.png" width="381" height="223"></p><p>detected faces and eyes in the image</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="pedestrian-detection"><strong>Pedestrian detection</strong><a class="hash-link" href="#pedestrian-detection" title="Direct link to heading">​</a></h2><p>We will use the full-body Haar cascade classifier model for pedestrian detection. We will apply this model to a video this time. The following function will run the model on each frame of the video to detect the pedestrians:</p><div class="codeBlockContainer_I0IT language-python theme-code-block"><div class="codeBlockContent_wNvx python"><pre tabindex="0" class="prism-code language-python codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># While Loop</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">while</span><span class="token plain"> cap</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">isOpened</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Read the capture</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        ret</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> frame </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> cap</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">read</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Pass the Frame to the Classifier</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        bodies </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> body_classifier</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">detectMultiScale</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">frame</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1.2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">3</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># if Statement</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> ret </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Bound Boxes to Identified Bodies</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">y</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">w</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">h</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> bodies</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            cv2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">rectangle</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">frame</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                         </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">y</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                         </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token operator" style="color:#393A34">+</span><span class="token plain">w</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> y</span><span class="token operator" style="color:#393A34">+</span><span class="token plain">h</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                         </span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">25</span><span class="token punctuation" style="color:#393A34">,</span><span class="token number" style="color:#36acaa">125</span><span class="token punctuation" style="color:#393A34">,</span><span class="token number" style="color:#36acaa">225</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                         </span><span class="token number" style="color:#36acaa">5</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            cv2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">imshow</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&#x27;Pedestrians&#x27;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> frame</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Exit with Esc button</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> cv2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">waitKey</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">27</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token keyword" style="color:#00009f">break</span><span class="token plain">  </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># else Statement</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">else</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">break</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Release the Capture &amp; Destroy All Windows</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cap</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">release</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cv2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">destroyAllWindows</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h2 class="anchor anchorWithStickyNavbar_mojV" id="car-detection"><strong>Car detection</strong><a class="hash-link" href="#car-detection" title="Direct link to heading">​</a></h2><p>The process is almost similar to the pedestrian detection process. Again, we will use this model on a video. Instead of people Haar cascade, we will use the car cascade model.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="car-number-plate-detection"><strong>Car number plate detection</strong><a class="hash-link" href="#car-number-plate-detection" title="Direct link to heading">​</a></h2><p>The process is almost similar to the face and eye detection process. We will use the car number plate cascade model.</p><p><em>You can find the code <a href="https://github.com/sparsh-ai/0D7ACA15" target="_blank" rel="noopener noreferrer">here</a> on Github.</em></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai/blog/tags/object-detection">object detection</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai/blog/tags/opencv">opencv</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai/blog/tags/vision">vision</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/ai/blog/page/2"><div class="pagination-nav__label">Older Entries</div></a></div></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Learn</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/ai/docs/intro">Introduction</a></li><li class="footer__item"><a class="footer__link-item" href="/ai/docs/concept-basics">Concepts</a></li><li class="footer__item"><a class="footer__link-item" href="/ai/docs/tutorials">Tutorials</a></li><li class="footer__item"><a class="footer__link-item" href="/ai/docs/projects">Projects</a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/ai/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/sparsh-ai/ai" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://nb.recohut.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Jupyter Notebooks<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://step.recohut.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Interactive Stories<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2022 Recohut Docs, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/ai/assets/js/runtime~main.92407a2e.js"></script>
<script src="/ai/assets/js/main.7b4c40d1.js"></script>
</body>
</html>