{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYYEpcq4wiv7"
      },
      "outputs": [],
      "source": [
        "# default_exp utils.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCjHII33wiwB"
      },
      "source": [
        "# Text utils\n",
        "> Implementation of text utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmUWFn1VwiwD"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "from nbdev.showdoc import *\n",
        "from fastcore.nb_imports import *\n",
        "from fastcore.test import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICb5KX9mwiwE"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KYJ63IkwiwG"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "nltk.download('stopwords', quiet=True)\n",
        "stopword = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL_r0AfTwiwH"
      },
      "source": [
        "## text_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBto_N0IwiwI"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "def text_clean(txt):\n",
        "  txt = re.sub('[^A-Za-z0-9]+', ' ', txt)\n",
        "  txt = txt.lower()\n",
        "  pattern = re.compile(r'\\b(' + r'|'.join(stopword) + r')\\b\\s*')\n",
        "  txt = pattern.sub(' ', txt)\n",
        "  txt = ' '.join(txt.split())\n",
        "  return txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIfkAezEwiwN"
      },
      "source": [
        "Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doz4yoR0wiwO",
        "outputId": "b0549af7-392c-4adb-eded-133dfaf8839d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'guns n roses popular card tee shirt pretty lady xxl red'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_clean(\"Guns N' Roses Popular Card Tee Shirt for Pretty Lady XXL Red\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus.reader.wordnet import NOUN, VERB, ADJ, ADV\n",
        "from nltk.corpus import brown, stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "from operator import itemgetter\n",
        "\n",
        "class TextCleaner():\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words(\"english\"))\n",
        "        self.punctuations = set(string.punctuation)\n",
        "        self.pos_tags = {\n",
        "                NOUN: ['NN', 'NNS', 'NNP', 'NNPS', 'PRP', 'PRP$', 'WP', 'WP$'],\n",
        "                VERB: ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'],\n",
        "                ADJ: ['JJ', 'JJR', 'JJS'],\n",
        "                ADV: ['RB', 'RBR', 'RBS', 'WRB']\n",
        "        }\n",
        "\n",
        "\n",
        "    def _remove_stop_words(self, words):\n",
        "        return [w for w in words if w not in self.stop_words]\n",
        "     \n",
        "    \n",
        "    def _remove_regex(self):\n",
        "        self.input_sent = \" \".join([w.lower() for w in self.input_sent])\n",
        "        self.input_sent = re.sub(r\"i'm\", \"i am\", self.input_sent)\n",
        "        self.input_sent = re.sub(r\"he's\", \"he is\", self.input_sent)\n",
        "        self.input_sent = re.sub(r\"she's\", \"she is\", self.input_sent)\n",
        "        self.input_sent = re.sub(r\"that's\", \"that is\", self.input_sent)\n",
        "        self.input_sent = re.sub(r\"what's\", \"what is\", self.input_sent)\n",
        "        self.input_sent = re.sub(r\"where's\", \"where is\", self.input_sent)\n",
        "        self.input_sent = re.sub(r\"\\'ll\", \" will\", self.input_sent)\n",
        "        self.input_sent = re.sub(r\"\\'ve\", \" have\", self.input_sent)\n",
        "        self.input_sent = re.sub(r\"\\'re\", \" are\", self.input_sent)\n",
        "        self.input_sent = re.sub(r\"\\'d\", \" would\", self.input_sent)\n",
        "        self.input_sent = re.sub(r\"won't\", \"will not\", self.input_sent)\n",
        "        self.input_sent = re.sub(r\"can't\", \"cannot\", self.input_sent)\n",
        "        self.input_sent = re.sub(r\"don't\", \"do not\", self.input_sent)\n",
        "        patterns = re.finditer(\"#[\\w]*\", self.input_sent)\n",
        "        for pattern in patterns:\n",
        "            self.input_sent = re.sub(pattern.group().strip(), \"\", self.input_sent)\n",
        "        self.input_sent = \"\".join(ch for ch in self.input_sent if ch not in self.punctuations)\n",
        "    \n",
        "    \n",
        "    def _tokenize(self):\n",
        "        return word_tokenize(self.input_sent)\n",
        "    \n",
        "    \n",
        "    def _process_content_for_pos(self, words):\n",
        "        tagged_words = pos_tag(words)\n",
        "        pos_words = []\n",
        "        for word in tagged_words:\n",
        "            flag = False\n",
        "            for key, value in self.pos_tags.items():\n",
        "                if word[1] in value:\n",
        "                    pos_words.append((word[0], key))\n",
        "                    flag = True\n",
        "                    break\n",
        "            if not flag:\n",
        "                pos_words.append((word[0], NOUN))\n",
        "        return pos_words\n",
        "       \n",
        "                 \n",
        "    def _remove_noise(self):\n",
        "        self._remove_regex()\n",
        "        words = self._tokenize()\n",
        "        noise_free_words = self._remove_stop_words(words)\n",
        "        return noise_free_words\n",
        "    \n",
        "    \n",
        "    def _normalize_text(self, words):\n",
        "        lem = WordNetLemmatizer()\n",
        "        pos_words = self._process_content_for_pos(words)\n",
        "        normalized_words = [lem.lemmatize(w, pos=p) for w, p in pos_words]\n",
        "        return normalized_words\n",
        "    \n",
        "    \n",
        "    def clean_up(self, input_sent):\n",
        "        self.input_sent = input_sent\n",
        "        cleaned_words = self._remove_noise()\n",
        "        cleaned_words = self._normalize_text(cleaned_words)\n",
        "        return cleaned_words"
      ],
      "metadata": {
        "id": "noCfOdRDwlze"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "name": "utils.text.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}