{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# default_exp models.deepwalk"
      ],
      "metadata": {
        "id": "gbApHj4FZd0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EqGRvLcQ_5Z"
      },
      "source": [
        "# DeepWalk\n",
        "> Implementation of Deepwalk graph embedding-based recommender model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import Word2Vec\n",
        "import networkx as nx\n",
        "import random\n",
        "import pickle"
      ],
      "metadata": {
        "id": "Oe4Tatl0tiGw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rating_df = pd.read_csv('https://raw.githubusercontent.com/sparsh-ai/rec-data-public/master/ml-other/ml100k_ratings.csv', sep=',', header=0)"
      ],
      "metadata": {
        "id": "5xEe3rbbti9-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class DeepWalk:\n",
        "    def __init__(self, is_directed=False, p=1, q=1, num_walks=10, walk_length=80, n_factors=50):\n",
        "        # p=q=1 for DeeWalk as the random walks are completely unbiased.\n",
        "        self.is_directed = is_directed\n",
        "        self.p = p\n",
        "        self.q = q\n",
        "        self.num_walks = num_walks\n",
        "        self.walk_length = walk_length\n",
        "        self.n_factors = n_factors\n",
        "                \n",
        "    def fit(self, df, save_path=None, user_col='user_id', item_col='item_id', rating_col='rating'):\n",
        "        try:\n",
        "            dataset = pickle.load(open(save_path, 'rb'))\n",
        "            self.node_vecs = dataset['node_vecs']\n",
        "            self.user2dict = dataset['user2dict']\n",
        "            self.item2dict = dataset['item2dict']\n",
        "            self.reverse_user2dict = dataset['reverse_user2dict']\n",
        "            self.reverse_item2dict = dataset['reverse_item2dict']\n",
        "        except:\n",
        "            user_item_graph = nx.Graph()\n",
        "\n",
        "            user2dict = dict()\n",
        "            item2dict = dict()\n",
        "            cnt = 0\n",
        "\n",
        "            df = df[[user_col, item_col, rating_col]].copy()\n",
        "\n",
        "            for x in df.values:\n",
        "                usr = (x[0], 'user')\n",
        "                item = (x[1], 'item')\n",
        "                if usr in user2dict:\n",
        "                    pass\n",
        "                else:\n",
        "                    user2dict[usr] = cnt\n",
        "                    cnt += 1\n",
        "                if item in item2dict:\n",
        "                    pass\n",
        "                else:\n",
        "                    item2dict[item] = cnt\n",
        "                    cnt += 1\n",
        "\n",
        "            # create a user-movie weighted graph using python library networkx\n",
        "            for x in df.values:\n",
        "                usr = (x[0], 'user')\n",
        "                item = (x[1], 'item')\n",
        "                user_item_graph.add_node(user2dict[usr])\n",
        "                user_item_graph.add_node(item2dict[item])\n",
        "                user_item_graph.add_edge(user2dict[usr], item2dict[item], weight=float(x[2]))\n",
        "            self.user_item_graph = user_item_graph\n",
        "\n",
        "            # Compute the transition probabilities based on the edge weights. \n",
        "            self.preprocess_transition_probs()\n",
        "            walks = self.simulate_walks()\n",
        "            node_embeddings = self.learn_embeddings(walks)\n",
        "\n",
        "            self.item2dict = item2dict\n",
        "            self.user2dict = user2dict\n",
        "            self.reverse_item2dict = {k:v for v,k in item2dict.items()}\n",
        "            self.reverse_user2dict = {k:v for v,k in user2dict.items()}\n",
        "\n",
        "            node_vecs = [node_embeddings[str(i)] for i in range(cnt)]\n",
        "            self.node_vecs = np.array(node_vecs)\n",
        "\n",
        "            with open(save_path, 'wb') as f:\n",
        "                dataset = {\n",
        "                    'node_vecs': self.node_vecs,\n",
        "                    'user2dict': self.user2dict,\n",
        "                    'item2dict': self.item2dict,\n",
        "                    'reverse_user2dict': self.reverse_user2dict,\n",
        "                    'reverse_item2dict': self.reverse_item2dict\n",
        "                }\n",
        "                pickle.dump(dataset, f)\n",
        "\n",
        "    def recommend(self, user_id=None, item_id=None, top_k=5):\n",
        "\n",
        "        if item_id is not None:\n",
        "            item_idx = self.item2dict[item_id]\n",
        "            query = self.node_vecs[item_idx].reshape(1,-1)\n",
        "        elif user_id is not None:\n",
        "            \"\"\"\n",
        "            items are ranked for a given user in terms of the cosine similarities \n",
        "            of their corresponding embeddings with the embedding of the user.\n",
        "            \"\"\"\n",
        "            user_idx = self.user2dict[user_id]\n",
        "            query = self.node_vecs[user_idx].reshape(1,-1)\n",
        "\n",
        "        ranking = cosine_similarity(query, self.node_vecs)\n",
        "        top_ids = np.argsort(-ranking)[0]\n",
        "        top_item_ids = [self.reverse_item2dict[j] for j in top_ids if j in self.reverse_item2dict][:top_k]\n",
        "        top_item_ids = [int(x[0]) for x in top_item_ids]\n",
        "        return top_item_ids\n",
        "\n",
        "    def node2vec_walk(self, start_node):\n",
        "        '''\n",
        "        Simulate a random walk starting from start node.\n",
        "        '''\n",
        "        G = self.user_item_graph\n",
        "        alias_nodes = self.alias_nodes\n",
        "        alias_edges = self.alias_edges\n",
        "\n",
        "        walk = [start_node]\n",
        "\n",
        "        while len(walk) < self.walk_length:\n",
        "            cur = walk[-1]\n",
        "            cur_nbrs = sorted(G.neighbors(cur))\n",
        "            if len(cur_nbrs) > 0:\n",
        "                if len(walk) == 1:\n",
        "                    walk.append(cur_nbrs[self.alias_draw(alias_nodes[cur][0], alias_nodes[cur][1])])\n",
        "                else:\n",
        "                    prev = walk[-2]\n",
        "                    next = cur_nbrs[self.alias_draw(alias_edges[(prev, cur)][0], \n",
        "                        alias_edges[(prev, cur)][1])]\n",
        "                    walk.append(next)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return walk\n",
        "\n",
        "    def simulate_walks(self):\n",
        "        '''\n",
        "        Repeatedly simulate random walks from each node.\n",
        "        '''\n",
        "        G = self.user_item_graph\n",
        "        walks = []\n",
        "        nodes = list(G.nodes())\n",
        "        print('Walk iteration:')\n",
        "        for walk_iter in range(self.num_walks):\n",
        "            random.shuffle(nodes)\n",
        "            for node in nodes:\n",
        "                walks.append(self.node2vec_walk(start_node=node))\n",
        "\n",
        "        return walks\n",
        "\n",
        "    def learn_embeddings(self, walks):\n",
        "        '''\n",
        "        Learn embeddings by optimizing the Skipgram objective using SGD.\n",
        "        Uses Gensim Word2Vec.\n",
        "        '''\n",
        "        walks = [list(map(str, walk)) for walk in walks]\n",
        "        model = Word2Vec(walks, size=50, window=10, min_count=0, sg=1, workers=8, iter=1)\n",
        "        return model.wv\n",
        "\n",
        "    def get_alias_edge(self, src, dst):\n",
        "        '''\n",
        "        Get the alias edge setup lists for a given edge.\n",
        "        '''\n",
        "        G = self.user_item_graph\n",
        "        p = self.p\n",
        "        q = self.q\n",
        "\n",
        "        unnormalized_probs = []\n",
        "        for dst_nbr in sorted(G.neighbors(dst)):\n",
        "            if dst_nbr == src:\n",
        "                unnormalized_probs.append(G[dst][dst_nbr]['weight']/p)\n",
        "            elif G.has_edge(dst_nbr, src):\n",
        "                unnormalized_probs.append(G[dst][dst_nbr]['weight'])\n",
        "            else:\n",
        "                unnormalized_probs.append(G[dst][dst_nbr]['weight']/q)\n",
        "        norm_const = sum(unnormalized_probs)\n",
        "        try:\n",
        "            normalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
        "        except:\n",
        "            normalized_probs =  [0.0 for u_prob in unnormalized_probs]\n",
        "\n",
        "        return self.alias_setup(normalized_probs)\n",
        "\n",
        "    def preprocess_transition_probs(self):\n",
        "        '''\n",
        "        Preprocessing of transition probabilities for guiding the random walks.\n",
        "        '''\n",
        "        G = self.user_item_graph\n",
        "        is_directed = self.is_directed\n",
        "\n",
        "        alias_nodes = {}\n",
        "        for node in G.nodes():\n",
        "            unnormalized_probs = [G[node][nbr]['weight'] for nbr in sorted(G.neighbors(node))]\n",
        "            norm_const = sum(unnormalized_probs)\n",
        "            try:\n",
        "                normalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
        "            except:\n",
        "                print(node)\n",
        "                normalized_probs =  [0.0 for u_prob in unnormalized_probs]\n",
        "            alias_nodes[node] = self.alias_setup(normalized_probs)\n",
        "\n",
        "        alias_edges = {}\n",
        "        triads = {}\n",
        "\n",
        "        if is_directed:\n",
        "            for edge in G.edges():\n",
        "                alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
        "        else:\n",
        "            for edge in G.edges():\n",
        "                alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
        "                alias_edges[(edge[1], edge[0])] = self.get_alias_edge(edge[1], edge[0])\n",
        "\n",
        "        self.alias_nodes = alias_nodes\n",
        "        self.alias_edges = alias_edges\n",
        "\n",
        "        return\n",
        "\n",
        "    def alias_setup(self, probs):\n",
        "        '''\n",
        "        Compute utility lists for non-uniform sampling from discrete distributions.\n",
        "        Refer to https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\n",
        "        for details\n",
        "        '''\n",
        "        K = len(probs)\n",
        "        q = np.zeros(K)\n",
        "        J = np.zeros(K, dtype=np.int)\n",
        "\n",
        "        smaller = []\n",
        "        larger = []\n",
        "        for kk, prob in enumerate(probs):\n",
        "            q[kk] = K*prob\n",
        "            if q[kk] < 1.0:\n",
        "                smaller.append(kk)\n",
        "            else:\n",
        "                larger.append(kk)\n",
        "\n",
        "        while len(smaller) > 0 and len(larger) > 0:\n",
        "            small = smaller.pop()\n",
        "            large = larger.pop()\n",
        "\n",
        "            J[small] = large\n",
        "            q[large] = q[large] + q[small] - 1.0\n",
        "            if q[large] < 1.0:\n",
        "                smaller.append(large)\n",
        "            else:\n",
        "                larger.append(large)\n",
        "\n",
        "        return J, q\n",
        "\n",
        "    def alias_draw(self, J, q):\n",
        "        '''\n",
        "        Draw sample from a non-uniform discrete distribution using alias sampling.\n",
        "        '''\n",
        "        K = len(J)\n",
        "\n",
        "        kk = int(np.floor(np.random.rand()*K))\n",
        "        if np.random.rand() < q[kk]:\n",
        "            return kk\n",
        "        else:\n",
        "            return J[kk]"
      ],
      "metadata": {
        "id": "wMLZzaUOBoMU"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example"
      ],
      "metadata": {
        "id": "RPA6zbpxaskP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DeepWalk()\n",
        "\n",
        "model.fit(save_path='./data.pkl', df=rating_df, user_col='userId', item_col='movieId', rating_col='rating')"
      ],
      "metadata": {
        "id": "TLHLuzFgJXLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = DeepWalk()\n",
        "\n",
        "model2.fit(df=rating_df, save_path='./data.pkl', user_col='userId', item_col='movieId', rating_col='rating')\n",
        "model2.recommend(item_id=(260, 'item'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sop9OydrJZCP",
        "outputId": "08fdbf0f-c28c-4b6d-aa4c-38bd7d06fa47"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[260, 1196, 1210, 1198, 1291]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext watermark\n",
        "%watermark -a \"Sparsh A.\" -m -iv -u -t -d -p gensim"
      ],
      "metadata": {
        "id": "ft2w9qnPa-aA",
        "outputId": "b6db0e43-b507-42ea-c732-27772efbf5c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author: Sparsh A.\n",
            "\n",
            "Last updated: 2022-01-28 11:22:48\n",
            "\n",
            "gensim: 3.6.0\n",
            "\n",
            "Compiler    : GCC 7.5.0\n",
            "OS          : Linux\n",
            "Release     : 5.4.144+\n",
            "Machine     : x86_64\n",
            "Processor   : x86_64\n",
            "CPU cores   : 2\n",
            "Architecture: 64bit\n",
            "\n",
            "networkx: 2.6.3\n",
            "IPython : 5.5.0\n",
            "pandas  : 1.1.5\n",
            "numpy   : 1.19.5\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}