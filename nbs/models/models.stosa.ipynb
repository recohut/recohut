{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.stosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOSA\n",
    "> Stochastic Self-Attention model for Sequential Item Recommendation.\n",
    "\n",
    "SR encodes usersâ€™ dynamic interests by modeling item-item transition relationships in sequences. Recent advancements in Transformer introduce the self-attention mechanism to reveal the position-wise item-item relationships, which leads to the state-of-the-art performance in SR. SASRec is the pioneering work in proposing Transformer for sequential recommendation, which applies scaled dot-product self-attention to learn item-item correlation weights. BERT4Rec adopts bi-directional modeling in sequences. TiSASRec and SSE-PT extend SASRec with additional time interval information and user regularization, respectively.\n",
    "\n",
    "**Sto**chastic **S**elf-**A**ttention (STOSA) embeds each item as a stochastic Gaussian distribution, the covariance of which encodes the uncertainty. **Wasserstein Self-Attention module** characterize item-item position-wise relationships in sequences, which effectively incorporates uncertainty into model training. Wasserstein attentions also enlighten the collaborative transitivity learning as it satisfies triangle inequality. **Regularization term** to the ranking loss assures the dissimilarity between positive and the negative items.\n",
    "\n",
    "<img src='https://github.com/recohut/reco-static/raw/master/media/images/models/stosa.png'>\n",
    "\n",
    "Model Architecture of the proposed STOSA. $ð‘ _ð‘–$ denotes the item in the position ð‘– and $\\hat{ð‘ }_{ð‘–+1}$ indicates the output inferred next item in (ð‘– +1)-th position. We first represent items as stochastic embeddings with Elliptical Gaussian distributions, comprised of the mean embedding and covariance embedding. Then we develop a novel Wasserstein self-attention module based on the Wasserstein distance to infer the stochastic sequence embeddings. A Wasserstein distance is adopted to measure the dissimilarity between items in the sequence with uncertainty signals. Finally, we incorporate a novel regularization term measuring the distance between positive and negative items into the standard BPR loss.\n",
    "\n",
    "**Why Wasserstein distance?** There are several advantages of using Wasserstein distance. First, Wasserstein distance measures the distance between distributions, with the capability of measuring the dissimilarity of items with uncertainty information. Secondly, Wasserstein distance satisfies triangle inequality and can capture collaborative transitivity inductively in sequence modeling. Finally, Wasserstein distance also enjoys the advantage of a more stable training process as it provides a smoother measurement when two distributions are non-overlapping, which in SR means two items are far away from each other. However, KL divergence will produce an infinity distance, causing numerical instability.\n",
    "\n",
    "For more information:\n",
    "1. https://arxiv.org/pdf/2201.06035v1.pdf\n",
    "2. https://github.com/RecoHut-Stanzas/STOSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.nb_imports import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Any, Iterable, List, Optional, Tuple, Union, Callable\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "from recohut.models.layers.encoding import DistSAEncoder, DistMeanSAEncoder\n",
    "from recohut.evaluation.metrics import get_eval_metrics_v2, recall_at_k, ndcg_at_k, cal_mrr\n",
    "from recohut.utils.distances import wasserstein_distance, kl_distance, wasserstein_distance_matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class STOSA(LightningModule):\n",
    "    def __init__(self,\n",
    "                 item_size,\n",
    "                 num_users,\n",
    "                 hidden_size,\n",
    "                 max_seq_length,\n",
    "                 num_attention_heads,\n",
    "                 optimizer='adamw',\n",
    "                 learning_rate = 0.003,\n",
    "                 hidden_dropout_prob=0.2,\n",
    "                 attention_probs_dropout_prob=0.2,\n",
    "                 num_hidden_layers=2,\n",
    "                 distance_metric='wasserstein',\n",
    "                 initializer_range=0.02,\n",
    "                 pvn_weight=0.1,\n",
    "                 kernel_param=1.0,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.item_size = item_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.num_users = num_users\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.distance_metric = distance_metric\n",
    "        self.initializer_range = initializer_range\n",
    "        self.pvn_weight = pvn_weight\n",
    "        self.kernel_param = kernel_param\n",
    "\n",
    "        self.item_mean_embeddings = nn.Embedding(item_size, hidden_size, padding_idx=0)\n",
    "        self.item_cov_embeddings = nn.Embedding(item_size, hidden_size, padding_idx=0)\n",
    "        self.position_mean_embeddings = nn.Embedding(max_seq_length, hidden_size)\n",
    "        self.position_cov_embeddings = nn.Embedding(max_seq_length, hidden_size)\n",
    "        self.user_margins = nn.Embedding(num_users, 1)\n",
    "        self.item_encoder = DistSAEncoder(hidden_size, num_attention_heads, hidden_dropout_prob, \n",
    "                                          attention_probs_dropout_prob, num_hidden_layers, \n",
    "                                          distance_metric)\n",
    "        self.layernorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def add_position_mean_embedding(self, sequence):\n",
    "\n",
    "        seq_length = sequence.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=sequence.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(sequence)\n",
    "        item_embeddings = self.item_mean_embeddings(sequence)\n",
    "        position_embeddings = self.position_mean_embeddings(position_ids)\n",
    "        sequence_emb = item_embeddings + position_embeddings\n",
    "        sequence_emb = self.layernorm(sequence_emb)\n",
    "        sequence_emb = self.dropout(sequence_emb)\n",
    "        elu_act = torch.nn.ELU()\n",
    "        sequence_emb = elu_act(sequence_emb)\n",
    "\n",
    "        return sequence_emb\n",
    "\n",
    "    def add_position_cov_embedding(self, sequence):\n",
    "\n",
    "        seq_length = sequence.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=sequence.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(sequence)\n",
    "        item_embeddings = self.item_cov_embeddings(sequence)\n",
    "        position_embeddings = self.position_cov_embeddings(position_ids)\n",
    "        sequence_emb = item_embeddings + position_embeddings\n",
    "        sequence_emb = self.layernorm(sequence_emb)\n",
    "        elu_act = torch.nn.ELU()\n",
    "        sequence_emb = elu_act(self.dropout(sequence_emb)) + 1\n",
    "\n",
    "        return sequence_emb\n",
    "\n",
    "    def forward(self, input_ids, user_ids):\n",
    "\n",
    "        attention_mask = (input_ids > 0).long()\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2) # torch.int64\n",
    "        max_len = attention_mask.size(-1)\n",
    "        attn_shape = (1, max_len, max_len)\n",
    "        subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1) # torch.uint8\n",
    "        subsequent_mask = (subsequent_mask == 0).unsqueeze(1)\n",
    "        subsequent_mask = subsequent_mask.long()\n",
    "\n",
    "        extended_attention_mask = extended_attention_mask * subsequent_mask\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * (-2 ** 32 + 1)\n",
    "\n",
    "        mean_sequence_emb = self.add_position_mean_embedding(input_ids)\n",
    "        cov_sequence_emb = self.add_position_cov_embedding(input_ids)\n",
    "\n",
    "        item_encoded_layers = self.item_encoder(mean_sequence_emb,\n",
    "                                                cov_sequence_emb,\n",
    "                                                extended_attention_mask,\n",
    "                                                output_all_encoded_layers=True)\n",
    "\n",
    "        mean_sequence_output, cov_sequence_output, att_scores = item_encoded_layers[-1]\n",
    "\n",
    "        margins = self.user_margins(user_ids)\n",
    "        return mean_sequence_output, cov_sequence_output, att_scores, margins\n",
    "\n",
    "    def init_weights(self, module):\n",
    "        \"\"\" Initialize the weights.\n",
    "        \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            module.weight.data.normal_(mean=0.01, std=self.initializer_range)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        user_ids, input_ids, target_pos, target_neg, _ = batch\n",
    "        sequence_mean_output, sequence_cov_output, att_scores, margins = self(input_ids, user_ids)\n",
    "        loss, batch_auc, pvn_loss = self.bpr_optimization(sequence_mean_output, sequence_cov_output, target_pos, target_neg)\n",
    "        #loss, batch_auc, pvn_loss = self.margin_optimization(sequence_mean_output, sequence_cov_output, target_pos, target_neg, margins)\n",
    "        # loss, batch_auc = self.ce_optimization(sequence_mean_output, sequence_cov_output, target_pos, target_neg)\n",
    "        self.log(\"train_loss\", loss.item())\n",
    "        self.log(\"train_pvn_loss\", pvn_loss.item())\n",
    "        self.log(\"train_auc\", batch_auc.item())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        user_ids, input_ids, target_pos, target_neg, answers = batch\n",
    "        recommend_mean_output, recommend_cov_output, _, _ = self(input_ids, user_ids)\n",
    "        loss, batch_auc, pvn_loss = self.bpr_optimization(recommend_mean_output, recommend_cov_output, target_pos, target_neg)\n",
    "        recommend_mean_output = recommend_mean_output[:, -1, :]\n",
    "        recommend_cov_output = recommend_cov_output[:, -1, :]\n",
    "        if self.distance_metric == 'kl':\n",
    "            rating_pred = self.kl_predict_full(recommend_mean_output, recommend_cov_output)\n",
    "        else:\n",
    "            rating_pred = self.dist_predict_full(recommend_mean_output, recommend_cov_output)\n",
    "        rating_pred = rating_pred.cpu().data.numpy().copy()\n",
    "\n",
    "        batch_user_index = user_ids.cpu().numpy()\n",
    "        rating_pred[args.valid_matrix[batch_user_index].toarray() > 0] = 1e+24\n",
    "        # reference: https://stackoverflow.com/a/23734295, https://stackoverflow.com/a/20104162\n",
    "        ind = np.argpartition(rating_pred, 40)[:, :40]\n",
    "        #ind = np.argpartition(rating_pred, -40)[:, -40:]\n",
    "        arr_ind = rating_pred[np.arange(len(rating_pred))[:, None], ind]\n",
    "        # ascending order\n",
    "        arr_ind_argsort = np.argsort(arr_ind)[np.arange(len(rating_pred)), ::]\n",
    "        #arr_ind_argsort = np.argsort(arr_ind)[np.arange(len(rating_pred)), ::-1]\n",
    "        batch_pred_list = ind[np.arange(len(rating_pred))[:, None], arr_ind_argsort]\n",
    "\n",
    "        pred_list = batch_pred_list\n",
    "        answer_list = answers.cpu().data.numpy()\n",
    "        val_scores = self.get_full_sort_score(answer_list, pred_list)\n",
    "\n",
    "        # val_scores = self.get_sample_scores(rating_pred)\n",
    "        self.log(\"valid_loss\", loss.item())\n",
    "        self.log(\"valid_pvn_loss\", pvn_loss.item())\n",
    "        self.log(\"valid_auc\", batch_auc.item())\n",
    "        self.log(\"valid_scores\", val_scores)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        user_ids, input_ids, target_pos, target_neg, answers = batch\n",
    "        recommend_mean_output, recommend_cov_output, _, _ = self(input_ids, user_ids)\n",
    "        loss, batch_auc, pvn_loss = self.bpr_optimization(recommend_mean_output, recommend_cov_output, target_pos, target_neg)\n",
    "        recommend_mean_output = recommend_mean_output[:, -1, :]\n",
    "        recommend_cov_output = recommend_cov_output[:, -1, :]\n",
    "        if self.distance_metric == 'kl':\n",
    "            rating_pred = self.kl_predict_full(recommend_mean_output, recommend_cov_output)\n",
    "        else:\n",
    "            rating_pred = self.dist_predict_full(recommend_mean_output, recommend_cov_output)\n",
    "        rating_pred = rating_pred.cpu().data.numpy().copy()\n",
    "\n",
    "        batch_user_index = user_ids.cpu().numpy()\n",
    "        rating_pred[args.test_matrix[batch_user_index].toarray() > 0] = 1e+24\n",
    "        # reference: https://stackoverflow.com/a/23734295, https://stackoverflow.com/a/20104162\n",
    "        ind = np.argpartition(rating_pred, 40)[:, :40]\n",
    "        #ind = np.argpartition(rating_pred, -40)[:, -40:]\n",
    "        arr_ind = rating_pred[np.arange(len(rating_pred))[:, None], ind]\n",
    "        # ascending order\n",
    "        arr_ind_argsort = np.argsort(arr_ind)[np.arange(len(rating_pred)), ::]\n",
    "        #arr_ind_argsort = np.argsort(arr_ind)[np.arange(len(rating_pred)), ::-1]\n",
    "        batch_pred_list = ind[np.arange(len(rating_pred))[:, None], arr_ind_argsort]\n",
    "\n",
    "        pred_list = batch_pred_list\n",
    "        answer_list = answers.cpu().data.numpy()\n",
    "        test_scores = self.get_full_sort_score(answer_list, pred_list)\n",
    "\n",
    "        # test_scores = self.get_sample_scores(rating_pred)\n",
    "        self.log(\"test_loss\", loss.item())\n",
    "        self.log(\"test_pvn_loss\", pvn_loss.item())\n",
    "        self.log(\"test_auc\", batch_auc.item())\n",
    "        self.log(\"test_scores\", test_scores)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # betas = (self.adam_beta1, self.adam_beta2)\n",
    "        # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, betas=betas, weight_decay=self.weight_decay)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, patience=10, factor=0.1\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler,\n",
    "            \"monitor\": \"train_loss\",\n",
    "        }\n",
    "\n",
    "    def get_sample_scores(self, pred_list):\n",
    "        pred_list = pred_list.argsort().argsort()[:, 0]\n",
    "        HIT_1, NDCG_1, MRR = get_eval_metrics_v2(pred_list, 1)\n",
    "        HIT_5, NDCG_5, MRR = get_eval_metrics_v2(pred_list, 5)\n",
    "        HIT_10, NDCG_10, MRR = get_eval_metrics_v2(pred_list, 10)\n",
    "        HIT_20, NDCG_20, MRR = get_eval_metrics_v2(pred_list, 20)\n",
    "        HIT_40, NDCG_40, MRR = get_eval_metrics_v2(pred_list, 40)\n",
    "        post_fix = {\n",
    "            \"HIT@1\": HIT_1, \"NDCG@1\": NDCG_1,\n",
    "            \"HIT@5\": HIT_5, \"NDCG@5\": NDCG_5,\n",
    "            \"HIT@10\": HIT_10, \"NDCG@10\": NDCG_10,\n",
    "            \"HIT@20\": HIT_20, \"NDCG@20\": NDCG_20,\n",
    "            \"HIT@40\": HIT_40, \"NDCG@40\": NDCG_40,\n",
    "            \"MRR\": MRR,\n",
    "        }\n",
    "        return post_fix\n",
    "\n",
    "    def get_full_sort_score(self, answers, pred_list):\n",
    "        recall, ndcg, mrr = [], [], 0\n",
    "        recall_dict_list = []\n",
    "        ndcg_dict_list = []\n",
    "        for k in [1, 5, 10, 15, 20, 40]:\n",
    "            recall_result, recall_dict_k = recall_at_k(answers, pred_list, k)\n",
    "            recall.append(recall_result)\n",
    "            recall_dict_list.append(recall_dict_k)\n",
    "            ndcg_result, ndcg_dict_k = ndcg_at_k(answers, pred_list, k)\n",
    "            ndcg.append(ndcg_result)\n",
    "            ndcg_dict_list.append(ndcg_dict_k)\n",
    "        mrr, mrr_dict = cal_mrr(answers, pred_list)\n",
    "        post_fix = {\n",
    "            \"HIT@1\": recall[0], \"NDCG@1\": ndcg[0],\n",
    "            \"HIT@5\": recall[1], \"NDCG@5\": ndcg[1],\n",
    "            \"HIT@10\": recall[2], \"NDCG@10\": ndcg[2],\n",
    "            \"HIT@15\": recall[3], \"NDCG@15\": ndcg[3],\n",
    "            \"HIT@20\": recall[4], \"NDCG@20\": ndcg[4],\n",
    "            \"HIT@40\": recall[5], \"NDCG@40\": ndcg[5],\n",
    "            \"MRR\": mrr\n",
    "       }\n",
    "        return post_fix\n",
    "\n",
    "    # def get_pos_items_ranks(self, batch_pred_lists, answers):\n",
    "    #     num_users = len(batch_pred_lists)\n",
    "    #     batch_pos_ranks = defaultdict(list)\n",
    "    #     for i in range(num_users):\n",
    "    #         pred_list = batch_pred_lists[i]\n",
    "    #         true_set = set(answers[i])\n",
    "    #         for ind, pred_item in enumerate(pred_list):\n",
    "    #             if pred_item in true_set:\n",
    "    #                 batch_pos_ranks[pred_item].append(ind+1)\n",
    "    #     return batch_pos_ranks\n",
    "\n",
    "    # def cross_entropy(self, seq_out, pos_ids, neg_ids):\n",
    "    #     # [batch seq_len hidden_size]\n",
    "    #     pos_emb = self.item_embeddings(pos_ids)\n",
    "    #     neg_emb = self.item_embeddings(neg_ids)\n",
    "    #     # [batch*seq_len hidden_size]\n",
    "    #     pos = pos_emb.view(-1, pos_emb.size(2))\n",
    "    #     neg = neg_emb.view(-1, neg_emb.size(2))\n",
    "    #     seq_emb = seq_out.view(-1, self.hidden_size) # [batch*seq_len hidden_size]\n",
    "    #     pos_logits = torch.sum(pos * seq_emb, -1) # [batch*seq_len]\n",
    "    #     neg_logits = torch.sum(neg * seq_emb, -1)\n",
    "    #     istarget = (pos_ids > 0).view(pos_ids.size(0) * self.max_seq_length).float() # [batch*seq_len]\n",
    "    #     loss = torch.sum(\n",
    "    #         - torch.log(torch.sigmoid(pos_logits) + 1e-24) * istarget -\n",
    "    #         torch.log(1 - torch.sigmoid(neg_logits) + 1e-24) * istarget\n",
    "    #     ) / torch.sum(istarget)\n",
    "\n",
    "    #     auc = torch.sum(\n",
    "    #         ((torch.sign(pos_logits - neg_logits) + 1) / 2) * istarget\n",
    "    #     ) / torch.sum(istarget)\n",
    "\n",
    "    #     return loss, auc\n",
    "\n",
    "    def predict_sample(self, seq_out, test_neg_sample):\n",
    "        # [batch 100 hidden_size]\n",
    "        test_item_emb = self.item_embeddings(test_neg_sample)\n",
    "        # [batch hidden_size]\n",
    "        test_logits = torch.bmm(test_item_emb, seq_out.unsqueeze(-1)).squeeze(-1)  # [B 100]\n",
    "        return test_logits\n",
    "\n",
    "    def predict_full(self, seq_out):\n",
    "        # [item_num hidden_size]\n",
    "        test_item_emb = self.item_embeddings.weight\n",
    "        # [batch hidden_size ]\n",
    "        rating_pred = torch.matmul(seq_out, test_item_emb.transpose(0, 1))\n",
    "        return rating_pred\n",
    "\n",
    "    def bpr_optimization(self, seq_mean_out, seq_cov_out, pos_ids, neg_ids):\n",
    "        # [batch seq_len hidden_size]\n",
    "        activation = nn.ELU()\n",
    "        pos_mean_emb = self.item_mean_embeddings(pos_ids)\n",
    "        pos_cov_emb = activation(self.item_cov_embeddings(pos_ids)) + 1\n",
    "        neg_mean_emb = self.item_mean_embeddings(neg_ids)\n",
    "        neg_cov_emb = activation(self.item_cov_embeddings(neg_ids)) + 1\n",
    "\n",
    "        # [batch*seq_len hidden_size]\n",
    "        pos_mean = pos_mean_emb.view(-1, pos_mean_emb.size(2))\n",
    "        pos_cov = pos_cov_emb.view(-1, pos_cov_emb.size(2))\n",
    "        neg_mean = neg_mean_emb.view(-1, neg_mean_emb.size(2))\n",
    "        neg_cov = neg_cov_emb.view(-1, neg_cov_emb.size(2))\n",
    "        seq_mean_emb = seq_mean_out.view(-1, self.hidden_size) # [batch*seq_len hidden_size]\n",
    "        seq_cov_emb = seq_cov_out.view(-1, self.hidden_size) # [batch*seq_len hidden_size]\n",
    "\n",
    "        if self.distance_metric == 'wasserstein':\n",
    "            pos_logits = wasserstein_distance(seq_mean_emb, seq_cov_emb, pos_mean, pos_cov)\n",
    "            neg_logits = wasserstein_distance(seq_mean_emb, seq_cov_emb, neg_mean, neg_cov)\n",
    "            pos_vs_neg = wasserstein_distance(pos_mean, pos_cov, neg_mean, neg_cov)\n",
    "        else:\n",
    "            pos_logits = kl_distance(seq_mean_emb, seq_cov_emb, pos_mean, pos_cov)\n",
    "            neg_logits = kl_distance(seq_mean_emb, seq_cov_emb, neg_mean, neg_cov)\n",
    "            pos_vs_neg = kl_distance(pos_mean, pos_cov, neg_mean, neg_cov)\n",
    "\n",
    "        istarget = (pos_ids > 0).view(pos_ids.size(0) * self.max_seq_length).float() # [batch*seq_len]\n",
    "        loss = torch.sum(-torch.log(torch.sigmoid(neg_logits - pos_logits + 1e-24)) * istarget) / torch.sum(istarget)\n",
    "\n",
    "        pvn_loss = self.pvn_weight * torch.sum(torch.clamp(pos_logits - pos_vs_neg, 0) * istarget) / torch.sum(istarget)\n",
    "        auc = torch.sum(\n",
    "            ((torch.sign(neg_logits - pos_logits) + 1) / 2) * istarget\n",
    "        ) / torch.sum(istarget)\n",
    "\n",
    "        return loss, auc, pvn_loss\n",
    "\n",
    "    def ce_optimization(self, seq_mean_out, seq_cov_out, pos_ids, neg_ids):\n",
    "        # [batch seq_len hidden_size]\n",
    "        activation = nn.ELU()\n",
    "        pos_mean_emb = self.item_mean_embeddings(pos_ids)\n",
    "        pos_cov_emb = activation(self.item_cov_embeddings(pos_ids)) + 1\n",
    "        neg_mean_emb = self.item_mean_embeddings(neg_ids)\n",
    "        neg_cov_emb = activation(self.item_cov_embeddings(neg_ids)) + 1\n",
    "\n",
    "        # [batch*seq_len hidden_size]\n",
    "        pos_mean = pos_mean_emb.view(-1, pos_mean_emb.size(2))\n",
    "        pos_cov = pos_cov_emb.view(-1, pos_cov_emb.size(2))\n",
    "        neg_mean = neg_mean_emb.view(-1, neg_mean_emb.size(2))\n",
    "        neg_cov = neg_cov_emb.view(-1, neg_cov_emb.size(2))\n",
    "        seq_mean_emb = seq_mean_out.view(-1, self.hidden_size) # [batch*seq_len hidden_size]\n",
    "        seq_cov_emb = seq_cov_out.view(-1, self.hidden_size) # [batch*seq_len hidden_size]\n",
    "\n",
    "\n",
    "        #pos_logits = d2s_gaussiannormal(wasserstein_distance(seq_mean_emb, seq_cov_emb, pos_mean, pos_cov), self.kernel_param)\n",
    "        pos_logits = -wasserstein_distance(seq_mean_emb, seq_cov_emb, pos_mean, pos_cov)\n",
    "        #neg_logits = d2s_gaussiannormal(wasserstein_distance(seq_mean_emb, seq_cov_emb, neg_mean, neg_cov), self.kernel_param)\n",
    "        neg_logits = -wasserstein_distance(seq_mean_emb, seq_cov_emb, neg_mean, neg_cov)\n",
    "\n",
    "        istarget = (pos_ids > 0).view(pos_ids.size(0) * self.max_seq_length).float() # [batch*seq_len]\n",
    "\n",
    "        loss = torch.sum(\n",
    "            - torch.log(torch.sigmoid(neg_logits) + 1e-24) * istarget -\n",
    "            torch.log(1 - torch.sigmoid(pos_logits) + 1e-24) * istarget\n",
    "        ) / torch.sum(istarget)\n",
    "\n",
    "        auc = torch.sum(\n",
    "            ((torch.sign(neg_logits - pos_logits) + 1) / 2) * istarget\n",
    "        ) / torch.sum(istarget)\n",
    "\n",
    "        return loss, auc\n",
    "\n",
    "    def margin_optimization(self, seq_mean_out, seq_cov_out, pos_ids, neg_ids, margins):\n",
    "        # [batch seq_len hidden_size]\n",
    "        activation = nn.ELU()\n",
    "        pos_mean_emb = self.item_mean_embeddings(pos_ids)\n",
    "        pos_cov_emb = activation(self.item_cov_embeddings(pos_ids)) + 1\n",
    "        neg_mean_emb = self.item_mean_embeddings(neg_ids)\n",
    "        neg_cov_emb = activation(self.item_cov_embeddings(neg_ids)) + 1\n",
    "\n",
    "        # [batch*seq_len hidden_size]\n",
    "        pos_mean = pos_mean_emb.view(-1, pos_mean_emb.size(2))\n",
    "        pos_cov = pos_cov_emb.view(-1, pos_cov_emb.size(2))\n",
    "        neg_mean = neg_mean_emb.view(-1, neg_mean_emb.size(2))\n",
    "        neg_cov = neg_cov_emb.view(-1, neg_cov_emb.size(2))\n",
    "        seq_mean_emb = seq_mean_out.view(-1, self.hidden_size) # [batch*seq_len hidden_size]\n",
    "        seq_cov_emb = seq_cov_out.view(-1, self.hidden_size) # [batch*seq_len hidden_size]\n",
    "\n",
    "        if self.distance_metric == 'wasserstein':\n",
    "            pos_logits = wasserstein_distance(seq_mean_emb, seq_cov_emb, pos_mean, pos_cov)\n",
    "            neg_logits = wasserstein_distance(seq_mean_emb, seq_cov_emb, neg_mean, neg_cov)\n",
    "            pos_vs_neg = wasserstein_distance(pos_mean, pos_cov, neg_mean, neg_cov)\n",
    "        else:\n",
    "            pos_logits = kl_distance(seq_mean_emb, seq_cov_emb, pos_mean, pos_cov)\n",
    "            neg_logits = kl_distance(seq_mean_emb, seq_cov_emb, neg_mean, neg_cov)\n",
    "            pos_vs_neg = kl_distance(pos_mean, pos_cov, neg_mean, neg_cov)\n",
    "\n",
    "        istarget = (pos_ids > 0).view(pos_ids.size(0) * self.max_seq_length).float() # [batch*seq_len]\n",
    "        loss = torch.sum(torch.clamp(pos_logits - neg_logits, min=0) * istarget) / torch.sum(istarget)\n",
    "        pvn_loss = self.pvn_weight * torch.sum(torch.clamp(pos_logits - pos_vs_neg, 0) * istarget) / torch.sum(istarget)\n",
    "        auc = torch.sum(\n",
    "            ((torch.sign(neg_logits - pos_logits) + 1) / 2) * istarget\n",
    "        ) / torch.sum(istarget)\n",
    "\n",
    "        return loss, auc, pvn_loss\n",
    "    \n",
    "    def dist_predict_full(self, seq_mean_out, seq_cov_out):\n",
    "        elu_activation = torch.nn.ELU()\n",
    "        test_item_mean_emb = self.item_mean_embeddings.weight\n",
    "        test_item_cov_emb = elu_activation(self.item_cov_embeddings.weight) + 1\n",
    "        #num_items, emb_size = test_item_cov_emb.shape\n",
    "\n",
    "        #seq_mean_out = seq_mean_out.unsqueeze(1).expand(-1, num_items, -1).reshape(-1, emb_size)\n",
    "        #seq_cov_out = seq_cov_out.unsqueeze(1).expand(-1, num_items, -1).reshape(-1, emb_size)\n",
    "\n",
    "        #if args.distance_metric == 'wasserstein':\n",
    "        #    return wasserstein_distance(seq_mean_out, seq_cov_out, test_item_mean_emb, test_item_cov_emb)\n",
    "        #else:\n",
    "        #    return kl_distance(seq_mean_out, seq_cov_out, test_item_mean_emb, test_item_cov_emb)\n",
    "        #return d2s_1overx(wasserstein_distance_matmul(seq_mean_out, seq_cov_out, test_item_mean_emb, test_item_cov_emb))\n",
    "        return wasserstein_distance_matmul(seq_mean_out, seq_cov_out, test_item_mean_emb, test_item_cov_emb)\n",
    "        #return d2s_gaussiannormal(wasserstein_distance_matmul(seq_mean_out, seq_cov_out, test_item_mean_emb, test_item_cov_emb))\n",
    "\n",
    "    def kl_predict_full(self, seq_mean_out, seq_cov_out):\n",
    "        elu_activation = torch.nn.ELU()\n",
    "        test_item_mean_emb = self.item_mean_embeddings.weight\n",
    "        test_item_cov_emb = elu_activation(self.item_cov_embeddings.weight) + 1\n",
    "\n",
    "        num_items = test_item_mean_emb.shape[0]\n",
    "        eval_batch_size = seq_mean_out.shape[0]\n",
    "        moded_num_items = eval_batch_size - num_items % eval_batch_size\n",
    "        fake_mean_emb = torch.zeros(moded_num_items, test_item_mean_emb.shape[1], dtype=torch.float32).to(self.device)\n",
    "        fake_cov_emb = torch.ones(moded_num_items, test_item_mean_emb.shape[1], dtype=torch.float32).to(self.device)\n",
    "\n",
    "        concated_mean_emb = torch.cat((test_item_mean_emb, fake_mean_emb), 0)\n",
    "        concated_cov_emb = torch.cat((test_item_cov_emb, fake_cov_emb), 0)\n",
    "\n",
    "        assert concated_mean_emb.shape[0] == test_item_mean_emb.shape[0] + moded_num_items\n",
    "\n",
    "        num_batches = int(num_items / eval_batch_size)\n",
    "        if moded_num_items > 0:\n",
    "            num_batches += 1\n",
    "\n",
    "        results = torch.zeros(seq_mean_out.shape[0], concated_mean_emb.shape[0], dtype=torch.float32)\n",
    "        start_i = 0\n",
    "        for i_batch in range(num_batches):\n",
    "            end_i = start_i + eval_batch_size\n",
    "\n",
    "            results[:, start_i:end_i] = kl_distance_matmul(seq_mean_out, seq_cov_out, concated_mean_emb[start_i:end_i, :], concated_cov_emb[start_i:end_i, :])\n",
    "            #results[:, start_i:end_i] = d2s_gaussiannormal(kl_distance_matmul(seq_mean_out, seq_cov_out, concated_mean_emb[start_i:end_i, :], concated_cov_emb[start_i:end_i, :]))\n",
    "            start_i += eval_batch_size\n",
    "\n",
    "        #print(results[:, :5])\n",
    "        return results[:, :num_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class STOSAMean(STOSA):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.item_encoder = DistMeanSAEncoder(self.hidden_size,\n",
    "                                              self.num_attention_heads,\n",
    "                                              self.hidden_dropout_prob,\n",
    "                                              self.attention_probs_dropout_prob,\n",
    "                                              self.num_hidden_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_size = 10\n",
    "num_users = 10\n",
    "hidden_size = 10\n",
    "max_seq_length = 20\n",
    "hidden_dropout_prob = 0.2\n",
    "initializer_range = 0.02\n",
    "num_hidden_layers = 2\n",
    "attention_probs_dropout_prob = 0.2\n",
    "num_attention_heads = 2\n",
    "distance_metric='wasserstein'\n",
    "initializer_range = 0.02\n",
    "optimizer = 'adamw'\n",
    "learning_rate = 0.003\n",
    "pvn_weight=0.1\n",
    "kernel_param=1.0\n",
    "\n",
    "model = STOSA(item_size, num_users, hidden_size, max_seq_length, num_attention_heads,\n",
    "              optimizer, learning_rate,\n",
    "              hidden_dropout_prob, attention_probs_dropout_prob, num_hidden_layers,\n",
    "              distance_metric, initializer_range, pvn_weight, kernel_param)\n",
    "\n",
    "input_ids = torch.randint(0, 5, (item_size, hidden_size))\n",
    "user_ids = torch.randint(0, 5, (item_size, hidden_size))\n",
    "\n",
    "output = model.forward(input_ids, user_ids)\n",
    "output_shapes = [list(x.shape) for x in [j for sub in output for j in sub]]\n",
    "\n",
    "test_eq(output_shapes[0], [10, 10])\n",
    "test_eq(output_shapes[21], [2, 10, 10])\n",
    "test_eq(output_shapes[31], [10, 1])\n",
    "\n",
    "model = STOSAMean(item_size, num_users, hidden_size, max_seq_length, num_attention_heads,\n",
    "                  optimizer, learning_rate,\n",
    "                  hidden_dropout_prob, attention_probs_dropout_prob, num_hidden_layers,\n",
    "                  distance_metric, initializer_range, pvn_weight, kernel_param)\n",
    "\n",
    "input_ids = torch.randint(0, 5, (item_size, hidden_size))\n",
    "user_ids = torch.randint(0, 5, (item_size, hidden_size))\n",
    "\n",
    "output = model.forward(input_ids, user_ids)\n",
    "output_shapes = [list(x.shape) for x in [j for sub in output for j in sub]]\n",
    "\n",
    "test_eq(output_shapes[0], [10, 10])\n",
    "test_eq(output_shapes[21], [2, 10, 10])\n",
    "test_eq(output_shapes[31], [10, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.data_dir = '/content/data'\n",
    "        self.min_seq_length = 10\n",
    "        self.max_seq_length = 200\n",
    "        self.sample_frac = 0.2\n",
    "        self.num_workers = 2\n",
    "\n",
    "        self.seed = 42\n",
    "        self.learning_rate = 1e-4\n",
    "        self.log_dir = '/content/recommender_logs'\n",
    "        self.model_dir = '/content/recommender_models'\n",
    "        self.batch_size = 256\n",
    "        self.max_epochs = 100\n",
    "        self.val_epoch = 10\n",
    "        self.gpus = None\n",
    "        self.monitor = 'valid_loss'\n",
    "        self.mode = 'min'\n",
    "\n",
    "        self.item_size = None\n",
    "        self.num_users = None\n",
    "        self.hidden_size = 64\n",
    "        self.hidden_dropout_prob = 0.5\n",
    "        self.initializer_range = 0.02\n",
    "        self.num_hidden_layers = 2\n",
    "        self.attention_probs_dropout_prob = 0.5\n",
    "        self.num_attention_heads = 2\n",
    "        self.distance_metric='wasserstein'\n",
    "        self.optimizer = 'adamw'\n",
    "        self.pvn_weight=0.1\n",
    "        self.kernel_param=1.0\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from recohut.datasets.amazon_beauty import AmazonBeautyDataModule\n",
    "\n",
    "dm = AmazonBeautyDataModule(**args.__dict__)\n",
    "dm.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.item_size = dm.data.item_size\n",
    "args.num_users = dm.data.num_users\n",
    "args.valid_matrix = dm.data.valid_rating_matrix\n",
    "args.test_matrix = dm.data.test_rating_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = STOSA(**args.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name                     | Type          | Params\n",
      "-----------------------------------------------------------\n",
      "0 | item_mean_embeddings     | Embedding     | 356 K \n",
      "1 | item_cov_embeddings      | Embedding     | 356 K \n",
      "2 | position_mean_embeddings | Embedding     | 12.8 K\n",
      "3 | position_cov_embeddings  | Embedding     | 12.8 K\n",
      "4 | user_margins             | Embedding     | 484   \n",
      "5 | item_encoder             | DistSAEncoder | 199 K \n",
      "6 | layernorm                | LayerNorm     | 128   \n",
      "7 | dropout                  | Dropout       | 0     \n",
      "-----------------------------------------------------------\n",
      "939 K     Trainable params\n",
      "0         Non-trainable params\n",
      "939 K     Total params\n",
      "3.756     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34962bfbda214e62a27281288f47ba4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0e22d36afa4a5a823d78878bd62552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e484e627e822421c9df7664bb1f927fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=args.log_dir)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor='valid_loss', mode='min', dirpath=args.model_dir, filename='STOSA')\n",
    "earlystop_callback = EarlyStopping(monitor=\"valid_loss\", patience=5, mode=\"min\")\n",
    "\n",
    "trainer = Trainer(max_epochs = args.max_epochs,\n",
    "                  check_val_every_n_epoch = args.val_epoch,\n",
    "                  logger = logger, \n",
    "                  callbacks = [checkpoint_callback, earlystop_callback],\n",
    "                  gpus = None)\n",
    "\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06dabc015e04c02801129107e57b720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_auc': 0.525363028049469,\n",
      " 'test_loss': 0.6991119384765625,\n",
      " 'test_pvn_loss': 4.98726224899292,\n",
      " 'test_scores': {'HIT@1': tensor(0.),\n",
      "                 'HIT@10': tensor(0.),\n",
      "                 'HIT@15': tensor(0.),\n",
      "                 'HIT@20': tensor(0.),\n",
      "                 'HIT@40': tensor(0.0041),\n",
      "                 'HIT@5': tensor(0.),\n",
      "                 'MRR': tensor(0.0001, dtype=torch.float64),\n",
      "                 'NDCG@1': tensor(0.0041),\n",
      "                 'NDCG@10': tensor(0.0041),\n",
      "                 'NDCG@15': tensor(0.0041),\n",
      "                 'NDCG@20': tensor(0.0041),\n",
      "                 'NDCG@40': tensor(0.0041),\n",
      "                 'NDCG@5': tensor(0.0041)}}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_auc': 0.525363028049469,\n",
       "  'test_loss': 0.6991119384765625,\n",
       "  'test_pvn_loss': 4.98726224899292,\n",
       "  'test_scores': {'HIT@1': tensor(0.),\n",
       "   'HIT@10': tensor(0.),\n",
       "   'HIT@15': tensor(0.),\n",
       "   'HIT@20': tensor(0.),\n",
       "   'HIT@40': tensor(0.0041),\n",
       "   'HIT@5': tensor(0.),\n",
       "   'MRR': tensor(0.0001, dtype=torch.float64),\n",
       "   'NDCG@1': tensor(0.0041),\n",
       "   'NDCG@10': tensor(0.0041),\n",
       "   'NDCG@15': tensor(0.0041),\n",
       "   'NDCG@20': tensor(0.0041),\n",
       "   'NDCG@40': tensor(0.0041),\n",
       "   'NDCG@5': tensor(0.0041)}}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = STOSAMean(**args.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "metadata": {},
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "Missing logger folder: /content/recommender_logs/default\n",
      "\n",
      "  | Name                     | Type              | Params\n",
      "---------------------------------------------------------------\n",
      "0 | item_mean_embeddings     | Embedding         | 356 K \n",
      "1 | item_cov_embeddings      | Embedding         | 356 K \n",
      "2 | position_mean_embeddings | Embedding         | 12.8 K\n",
      "3 | position_cov_embeddings  | Embedding         | 12.8 K\n",
      "4 | user_margins             | Embedding         | 484   \n",
      "5 | item_encoder             | DistMeanSAEncoder | 199 K \n",
      "6 | layernorm                | LayerNorm         | 128   \n",
      "7 | dropout                  | Dropout           | 0     \n",
      "---------------------------------------------------------------\n",
      "939 K     Trainable params\n",
      "0         Non-trainable params\n",
      "939 K     Total params\n",
      "3.756     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646f67f44e234a2c862e4ac875d4441f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b5ea88b23a4744b193064bb413217c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4255ed02b1844cb5a68a283a06f3d583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0637440c602448ee869f40242318057e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=args.log_dir)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor='valid_loss', mode='min', dirpath=args.model_dir, filename='STOSA')\n",
    "earlystop_callback = EarlyStopping(monitor=\"valid_loss\", patience=5, mode=\"min\")\n",
    "\n",
    "trainer = Trainer(max_epochs = 10,\n",
    "                  check_val_every_n_epoch = 5,\n",
    "                  logger = logger, \n",
    "                  callbacks = [checkpoint_callback, earlystop_callback],\n",
    "                  gpus = None)\n",
    "\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba0024734cb4a8b9fb74fa51bb0b48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_auc': 0.5465450286865234,\n",
      " 'test_loss': 0.6889303922653198,\n",
      " 'test_pvn_loss': 4.940025806427002,\n",
      " 'test_scores': {'HIT@1': tensor(0.),\n",
      "                 'HIT@10': tensor(0.),\n",
      "                 'HIT@20': tensor(0.),\n",
      "                 'HIT@40': tensor(0.),\n",
      "                 'HIT@5': tensor(0.),\n",
      "                 'MRR': tensor(0.),\n",
      "                 'NDCG@1': tensor(0.),\n",
      "                 'NDCG@10': tensor(0.),\n",
      "                 'NDCG@20': tensor(0.),\n",
      "                 'NDCG@40': tensor(0.),\n",
      "                 'NDCG@5': tensor(0.)}}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_auc': 0.5465450286865234,\n",
       "  'test_loss': 0.6889303922653198,\n",
       "  'test_pvn_loss': 4.940025806427002,\n",
       "  'test_scores': {'HIT@1': tensor(0.),\n",
       "   'HIT@10': tensor(0.),\n",
       "   'HIT@20': tensor(0.),\n",
       "   'HIT@40': tensor(0.),\n",
       "   'HIT@5': tensor(0.),\n",
       "   'MRR': tensor(0.),\n",
       "   'NDCG@1': tensor(0.),\n",
       "   'NDCG@10': tensor(0.),\n",
       "   'NDCG@20': tensor(0.),\n",
       "   'NDCG@40': tensor(0.),\n",
       "   'NDCG@5': tensor(0.)}}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Sparsh A.\n",
      "\n",
      "Last updated: 2022-01-23 05:56:19\n",
      "\n",
      "recohut: 0.0.11\n",
      "\n",
      "Compiler    : GCC 7.5.0\n",
      "OS          : Linux\n",
      "Release     : 5.4.144+\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 2\n",
      "Architecture: 64bit\n",
      "\n",
      "torch  : 1.10.0+cu111\n",
      "IPython: 5.5.0\n",
      "pandas : 1.1.5\n",
      "numpy  : 1.19.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Sparsh A.\" -m -iv -u -t -d -p recohut"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
