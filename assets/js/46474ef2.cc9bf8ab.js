"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[846],{3905:function(e,t,i){i.d(t,{Zo:function(){return m},kt:function(){return p}});var a=i(67294);function n(e,t,i){return t in e?Object.defineProperty(e,t,{value:i,enumerable:!0,configurable:!0,writable:!0}):e[t]=i,e}function r(e,t){var i=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),i.push.apply(i,a)}return i}function o(e){for(var t=1;t<arguments.length;t++){var i=null!=arguments[t]?arguments[t]:{};t%2?r(Object(i),!0).forEach((function(t){n(e,t,i[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(i)):r(Object(i)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(i,t))}))}return e}function s(e,t){if(null==e)return{};var i,a,n=function(e,t){if(null==e)return{};var i,a,n={},r=Object.keys(e);for(a=0;a<r.length;a++)i=r[a],t.indexOf(i)>=0||(n[i]=e[i]);return n}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)i=r[a],t.indexOf(i)>=0||Object.prototype.propertyIsEnumerable.call(e,i)&&(n[i]=e[i])}return n}var l=a.createContext({}),c=function(e){var t=a.useContext(l),i=t;return e&&(i="function"==typeof e?e(t):o(o({},t),e)),i},m=function(e){var t=c(e.components);return a.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var i=e.components,n=e.mdxType,r=e.originalType,l=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),u=c(i),p=n,f=u["".concat(l,".").concat(p)]||u[p]||d[p]||r;return i?a.createElement(f,o(o({ref:t},m),{},{components:i})):a.createElement(f,o({ref:t},m))}));function p(e,t){var i=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var r=i.length,o=new Array(r);o[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:n,o[1]=s;for(var c=2;c<r;c++)o[c]=i[c];return a.createElement.apply(null,o)}return a.createElement.apply(null,i)}u.displayName="MDXCreateElement"},35554:function(e,t,i){i.r(t),i.d(t,{assets:function(){return m},contentTitle:function(){return l},default:function(){return p},frontMatter:function(){return s},metadata:function(){return c},toc:function(){return d}});var a=i(87462),n=i(63366),r=(i(67294),i(3905)),o=["components"],s={},l="Image Similarity",c={unversionedId:"concept-extras/vision/image-similarity",id:"concept-extras/vision/image-similarity",title:"Image Similarity",description:"/img/content-concepts-raw-computer-vision-image-similarity-slide19.png",source:"@site/docs/concept-extras/vision/image-similarity.mdx",sourceDirName:"concept-extras/vision",slug:"/concept-extras/vision/image-similarity",permalink:"/recohut/docs/concept-extras/vision/image-similarity",editUrl:"https://github.com/sparsh-ai/recohut/docs/concept-extras/vision/image-similarity.mdx",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Image Segmentation",permalink:"/recohut/docs/concept-extras/vision/image-segmentation"},next:{title:"Object Detection",permalink:"/recohut/docs/concept-extras/vision/object-detection"}},m={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Models",id:"models",level:2},{value:"DeepRank",id:"deeprank",level:3},{value:"ConvNets",id:"convnets",level:3},{value:"FAISS",id:"faiss",level:3},{value:"Siamese Network",id:"siamese-network",level:3},{value:"Similarity Measures",id:"similarity-measures",level:3},{value:"Process flow",id:"process-flow",level:2},{value:"Use Cases",id:"use-cases",level:2},{value:"Multi-endpoint API Similarity System",id:"multi-endpoint-api-similarity-system",level:3},{value:"Beanstalk Image Similarity System",id:"beanstalk-image-similarity-system",level:3},{value:"Image + Text Similarity",id:"image--text-similarity",level:3},{value:"Siamese Network Image Similarity on MNIST",id:"siamese-network-image-similarity-on-mnist",level:3},{value:"Visual Recommendation",id:"visual-recommendation",level:3}],u={toc:d};function p(e){var t=e.components,s=(0,n.Z)(e,o);return(0,r.kt)("wrapper",(0,a.Z)({},u,s,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"image-similarity"},"Image Similarity"),(0,r.kt)("p",null,(0,r.kt)("img",{loading:"lazy",alt:"/img/content-concepts-raw-computer-vision-image-similarity-slide19.png",src:i(81555).Z,width:"960",height:"720"})),(0,r.kt)("h2",{id:"introduction"},"Introduction"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Definition:")," Image similarity is the measure of how similar two images are. In other words, it quantifies the degree of similarity between intensity patterns in two images."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Applications:")," Duplicate product detection, image clustering, visual search, product recommendations."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Scope:")," Fine-tuning on classes for greater accuracy"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Tools:")," TFHub")),(0,r.kt)("h2",{id:"models"},"Models"),(0,r.kt)("h3",{id:"deeprank"},"DeepRank"),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},(0,r.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1710.05649"},"DeepRank: A New Deep Architecture for Relevance Ranking in Information Retrieval. arXiv, 2017."))),(0,r.kt)("h3",{id:"convnets"},"ConvNets"),(0,r.kt)("p",null,"Pre-trained models like MobileNet, EfficientNet, BiT-L/BiT-M can be used to convert images into vectors. These models can be found on TFHub. For more accuracy, fine-tuning can be done. "),(0,r.kt)("h3",{id:"faiss"},"FAISS"),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},(0,r.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1702.08734"},"Billion-scale similarity search with GPUs. arXiv, 2017."))),(0,r.kt)("p",null,"Faiss is a library for efficient similarity search and clustering of dense vectors."),(0,r.kt)("h3",{id:"siamese-network"},"Siamese Network"),(0,r.kt)("p",null,"Siamese network is a neural network that contains two or more identical subnetwork. The purpose of this network is to find the similarity or comparing the relationship between two comparable things. Unlike the classification task that uses cross-entropy as the loss function, the siamese network usually uses contrastive loss or triplet loss."),(0,r.kt)("h3",{id:"similarity-measures"},"Similarity Measures"),(0,r.kt)("p",null,"L1 (Manhattan distance), L2 (Euclidean distance), Hinge Loss for Triplets."),(0,r.kt)("h2",{id:"process-flow"},"Process flow"),(0,r.kt)("p",null,"Step 1: Collect Images"),(0,r.kt)("p",null,"Download the raw image dataset into a directory. Categorize these images into their respective category directories. Make sure that images are of the same type, JPEG recommended. We will also process the metadata and store it in a serialized file, CSV recommended. "),(0,r.kt)("p",null,"Step 2: Encoder Fine-tuning"),(0,r.kt)("p",null,"Download the pre-trained image model and add two additional layers on top of that: the first layer is a feature vector layer and the second layer is the classification layer. We will only train these 2 layers on our data and after training, we will select the feature vector layer as the output of our fine-tuned encoder. After fine-tuning the model, we will save the feature extractor for later use."),(0,r.kt)("p",null,"Step 3: Image Vectorization"),(0,r.kt)("p",null,"Now, we will use the encoder (prepared in step 2) to encode the images (prepared in step 1). We will save the feature vector of each image as an array in a directory. After processing, we will save these embeddings for later use."),(0,r.kt)("p",null,"Step 4: Metadata and Indexing"),(0,r.kt)("p",null,"We will assign a unique id to each image and create dictionaries to locate information of this image: 1) Image id to Image name dictionary, 2) Image id to image feature vector dictionary, and 3) (optional) Image id to metadata product id dictionary. We will also create an image id to image feature vector indexing. Then we will save these dictionaries and index objects for later use."),(0,r.kt)("p",null,"Step 5: UAT Testing"),(0,r.kt)("p",null,"Wrap the model inference engine in API for client testing. We will receive an image from user, encode it with our image encoder, find Top-K similar vectors using Indexing object, and retrieve the image (and metadata) using dictionaries. We send these images (and metadata) back to the user."),(0,r.kt)("p",null,"Step 6: Deployment"),(0,r.kt)("p",null,"Deploy the model on cloud or edge as per the requirement."),(0,r.kt)("p",null,"Step 7: Documentation"),(0,r.kt)("p",null,"Prepare the documentation and transfer all assets to the client."),(0,r.kt)("h2",{id:"use-cases"},"Use Cases"),(0,r.kt)("h3",{id:"multi-endpoint-api-similarity-system"},"Multi-endpoint API Similarity System"),(0,r.kt)("p",null,"The task was to build an API that will support multiple endpoints. Each endpoint supports a separate similarity system. We built 2 endpoints: endpoint 1 would find tok-K most similar fashion images and endpoint 2 would find top-K most similar food images. Checkout the notion ",(0,r.kt)("a",{parentName:"p",href:"https://www.notion.so/Multi-endpoint-Image-Similarity-System-159b47b635ea42299a0214551630e740"},"here"),"."),(0,r.kt)("h3",{id:"beanstalk-image-similarity-system"},"Beanstalk Image Similarity System"),(0,r.kt)("p",null,"There are 2 endpoints in the API - one for training and the other for inference. During training, the system will receive a zipped file of images. At the time of inference, this trained system would receive an image over inference endpoint and send back top-K most similar images with a confidence score. The API was deployed on AWS beanstalk. Checkout the notion ",(0,r.kt)("a",{parentName:"p",href:"https://www.notion.so/Image-Similarity-AWS-b8f33261750047a69744e91a554eabff"},"here"),"."),(0,r.kt)("h3",{id:"image--text-similarity"},"Image + Text Similarity"),(0,r.kt)("p",null,"Use the textual details and images of products, find the exact similar product among different groups. Around 35 GB of retail product images was scraped and used to build the system. Checkout the notion ",(0,r.kt)("a",{parentName:"p",href:"https://www.notion.so/Image-Text-Similarity-fe5130324ae14ab48a30c93444348f4a"},"here"),"."),(0,r.kt)("h3",{id:"siamese-network-image-similarity-on-mnist"},"Siamese Network Image Similarity on MNIST"),(0,r.kt)("p",null,"Siamese networks are incredibly powerful networks, responsible for significant increases in face recognition, signature verification, and prescription pill identification applications. The objective was to build image pairs for the siamese network, train the siamese network with TF Keras, and then compare image similarity with this siamese network. "),(0,r.kt)("h3",{id:"visual-recommendation"},"Visual Recommendation"),(0,r.kt)("p",null,"Use image similarity to recommend users visually similar products based on what they searched. Checkout the notion ",(0,r.kt)("a",{parentName:"p",href:"https://www.notion.so/Image-Similarity-Detection-in-Action-with-Tensorflow-2-0-c2b4421d75dd42a3a1becf9c98251ccb"},"here"),"."))}p.isMDXComponent=!0},81555:function(e,t,i){t.Z=i.p+"assets/images/content-concepts-raw-computer-vision-image-similarity-slide19-233693a6002d2b19892db308415ef0d8.png"}}]);