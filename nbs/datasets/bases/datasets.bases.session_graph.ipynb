{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ms1RfbNVeRIb"
      },
      "outputs": [],
      "source": [
        "# default_exp datasets.bases.session_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAzceUi3eRIk"
      },
      "source": [
        "# Session Graph dataset\n",
        "> Base class for Session Graph dataset module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgdIIOUZeRIq"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "from nbdev.showdoc import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aDNCoBdAeRIs"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "from typing import List, Optional, Callable, Union, Any, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "\n",
        "from recohut.datasets.bases.common import Dataset\n",
        "from recohut.utils.common_utils import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zYQliJyteRJS"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class SessionGraphDataset(Dataset):\n",
        "    \"\"\"\n",
        "    References\n",
        "        1. COTREC session-based recommender model training. https://t.ly/cXTH.\n",
        "    \"\"\"\n",
        "    def __init__(self, root, shuffle=False, n_node=None):\n",
        "        super().__init__(root)\n",
        "        self.n_node = n_node\n",
        "        self.shuffle = shuffle\n",
        "        self.process()\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self) -> str:\n",
        "        return ['data.txt', 'all_seq.txt']\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self) -> str:\n",
        "        pass\n",
        "\n",
        "    def download(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def process(self):\n",
        "        import pickle\n",
        "        data = pickle.load(open(self.raw_paths[0], 'rb'))\n",
        "        all_seq = pickle.load(open(self.raw_paths[1], 'rb'))\n",
        "        self.raw = np.asarray(data[0])\n",
        "        self.targets = np.asarray(data[1])\n",
        "        self.length = len(self.raw)\n",
        "        adj = self.data_masks(all_seq, self.n_node)\n",
        "        self.adjacency = adj.multiply(1.0/adj.sum(axis=0).reshape(1, -1))\n",
        "\n",
        "\n",
        "    def get_overlap(self, sessions):\n",
        "        matrix = np.zeros((len(sessions), len(sessions)))\n",
        "        for i in range(len(sessions)):\n",
        "            seq_a = set(sessions[i])\n",
        "            seq_a.discard(0)\n",
        "            for j in range(i+1, len(sessions)):\n",
        "                seq_b = set(sessions[j])\n",
        "                seq_b.discard(0)\n",
        "                overlap = seq_a.intersection(seq_b)\n",
        "                ab_set = seq_a | seq_b\n",
        "                matrix[i][j] = float(len(overlap))/float(len(ab_set))\n",
        "                matrix[j][i] = matrix[i][j]\n",
        "        # matrix = self.dropout(matrix, 0.2)\n",
        "        matrix = matrix + np.diag([1.0]*len(sessions))\n",
        "        degree = np.sum(np.array(matrix), 1)\n",
        "        degree = np.diag(1.0/degree)\n",
        "        return matrix, degree\n",
        "\n",
        "    def generate_batch(self, batch_size):\n",
        "        if self.shuffle:\n",
        "            shuffled_arg = np.arange(self.length)\n",
        "            np.random.shuffle(shuffled_arg)\n",
        "            self.raw = self.raw[shuffled_arg]\n",
        "            self.targets = self.targets[shuffled_arg]\n",
        "        n_batch = int(self.length / batch_size)\n",
        "        if self.length % batch_size != 0:\n",
        "            n_batch += 1\n",
        "        slices = np.split(np.arange(n_batch * batch_size), n_batch)\n",
        "        slices[-1] = np.arange(self.length-batch_size, self.length)\n",
        "        return slices\n",
        "\n",
        "    def get_slice(self, index):\n",
        "        items, num_node = [], []\n",
        "        inp = self.raw[index]\n",
        "        for session in inp:\n",
        "            num_node.append(len(np.nonzero(session)[0]))\n",
        "        max_n_node = np.max(num_node)\n",
        "        session_len = []\n",
        "        reversed_sess_item = []\n",
        "        mask = []\n",
        "        # item_set = set()\n",
        "        for session in inp:\n",
        "            nonzero_elems = np.nonzero(session)[0]\n",
        "            # item_set.update(set([t-1 for t in session]))\n",
        "            session_len.append([len(nonzero_elems)])\n",
        "            items.append(session + (max_n_node - len(nonzero_elems)) * [0])\n",
        "            mask.append([1]*len(nonzero_elems) + (max_n_node - len(nonzero_elems)) * [0])\n",
        "            reversed_sess_item.append(list(reversed(session)) + (max_n_node - len(nonzero_elems)) * [0])\n",
        "        # item_set = list(item_set)\n",
        "        # index_list = [item_set.index(a) for a in self.targets[index]-1]\n",
        "        diff_mask = np.ones(shape=[100, self.n_node]) * (1/(self.n_node - 1))\n",
        "        for count, value in enumerate(self.targets[index]-1):\n",
        "            diff_mask[count][value] = 1\n",
        "        return self.targets[index]-1, session_len,items, reversed_sess_item, mask, diff_mask\n",
        "    \n",
        "    @staticmethod\n",
        "    def data_masks(all_sessions, n_node):\n",
        "        adj = dict()\n",
        "        for sess in all_sessions:\n",
        "            for i, item in enumerate(sess):\n",
        "                if i == len(sess)-1:\n",
        "                    break\n",
        "                else:\n",
        "                    if sess[i] - 1 not in adj.keys():\n",
        "                        adj[sess[i]-1] = dict()\n",
        "                        adj[sess[i]-1][sess[i]-1] = 1\n",
        "                        adj[sess[i]-1][sess[i+1]-1] = 1\n",
        "                    else:\n",
        "                        if sess[i+1]-1 not in adj[sess[i]-1].keys():\n",
        "                            adj[sess[i] - 1][sess[i + 1] - 1] = 1\n",
        "                        else:\n",
        "                            adj[sess[i]-1][sess[i+1]-1] += 1\n",
        "        row, col, data = [], [], []\n",
        "        for i in adj.keys():\n",
        "            item = adj[i]\n",
        "            for j in item.keys():\n",
        "                row.append(i)\n",
        "                col.append(j)\n",
        "                data.append(adj[i][j])\n",
        "        from scipy.sparse import coo_matrix\n",
        "        coo = coo_matrix((data, (row, col)), shape=(n_node, n_node))\n",
        "        return coo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examples"
      ],
      "metadata": {
        "id": "hd2F8vaelDZI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-SHNnBMeRJT"
      },
      "outputs": [],
      "source": [
        "class DigineticaDataset(SessionGraphDataset):\n",
        "    train_url = \"https://github.com/RecoHut-Datasets/diginetica/raw/v2/train.txt\"\n",
        "    test_url = \"https://github.com/RecoHut-Datasets/diginetica/raw/v2/test.txt\"\n",
        "    all_train_seq_url = \"https://github.com/RecoHut-Datasets/diginetica/raw/v2/all_train_seq.txt\"\n",
        "\n",
        "    def __init__(self, root, shuffle=False, n_node=43097, is_train=True):\n",
        "        self.n_node = n_node\n",
        "        self.shuffle = shuffle\n",
        "        self.is_train = is_train\n",
        "        super().__init__(root, shuffle, n_node)\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self) -> str:\n",
        "        if self.is_train:\n",
        "            return ['train.txt', 'all_train_seq.txt']\n",
        "        return ['test.txt', 'all_train_seq.txt']\n",
        "\n",
        "    def download(self):\n",
        "        download_url(self.all_train_seq_url, self.raw_dir)\n",
        "        if self.is_train:\n",
        "            download_url(self.train_url, self.raw_dir)\n",
        "        else:\n",
        "            download_url(self.test_url, self.raw_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqS_wQfGeRJT"
      },
      "outputs": [],
      "source": [
        "root = '/content/diginetica'\n",
        "\n",
        "train_data = DigineticaDataset(root=root, shuffle=True, is_train=True)\n",
        "test_data = DigineticaDataset(root=root, shuffle=False, is_train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEYJt6YQeRJU"
      },
      "outputs": [],
      "source": [
        "class TmallDataset(SessionGraphDataset):\n",
        "    train_url = \"https://github.com/RecoHut-Datasets/tmall/raw/v1/train.txt\"\n",
        "    test_url = \"https://github.com/RecoHut-Datasets/tmall/raw/v1/test.txt\"\n",
        "    all_train_seq_url = \"https://github.com/RecoHut-Datasets/tmall/raw/v1/all_train_seq.txt\"\n",
        "\n",
        "    def __init__(self, root, shuffle=False, n_node=40727, is_train=True):\n",
        "        self.n_node = n_node\n",
        "        self.shuffle = shuffle\n",
        "        self.is_train = is_train\n",
        "        super().__init__(root, shuffle, n_node)\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self) -> str:\n",
        "        if self.is_train:\n",
        "            return ['train.txt', 'all_train_seq.txt']\n",
        "        return ['test.txt', 'all_train_seq.txt']\n",
        "\n",
        "    def download(self):\n",
        "        download_url(self.all_train_seq_url, self.raw_dir)\n",
        "        if self.is_train:\n",
        "            download_url(self.train_url, self.raw_dir)\n",
        "        else:\n",
        "            download_url(self.test_url, self.raw_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AX8Gl-PeRJU",
        "outputId": "d6596f1f-b094-4fb5-cb07-268c39168b81"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ]
        }
      ],
      "source": [
        "root = '/content/tmall'\n",
        "\n",
        "train_data = TmallDataset(root=root, shuffle=True, is_train=True)\n",
        "test_data = TmallDataset(root=root, shuffle=False, is_train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lie9joXeRJV"
      },
      "outputs": [],
      "source": [
        "class RetailRocketDataset(SessionGraphDataset):\n",
        "    train_url = \"https://github.com/RecoHut-Datasets/retail_rocket/raw/v1/train.txt\"\n",
        "    test_url = \"https://github.com/RecoHut-Datasets/retail_rocket/raw/v1/test.txt\"\n",
        "    all_train_seq_url = \"https://github.com/RecoHut-Datasets/retail_rocket/raw/v1/all_train_seq.txt\"\n",
        "\n",
        "    def __init__(self, root, shuffle=False, n_node=40727, is_train=True):\n",
        "        self.n_node = n_node\n",
        "        self.shuffle = shuffle\n",
        "        self.is_train = is_train\n",
        "        super().__init__(root, shuffle, n_node)\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self) -> str:\n",
        "        if self.is_train:\n",
        "            return ['train.txt', 'all_train_seq.txt']\n",
        "        return ['test.txt', 'all_train_seq.txt']\n",
        "\n",
        "    def download(self):\n",
        "        download_url(self.all_train_seq_url, self.raw_dir)\n",
        "        if self.is_train:\n",
        "            download_url(self.train_url, self.raw_dir)\n",
        "        else:\n",
        "            download_url(self.test_url, self.raw_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEWeXLo0eRJW",
        "outputId": "c55a8dd5-9e4b-4df2-aad6-b25088ae90d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/RecoHut-Datasets/retail_rocket/raw/v1/all_train_seq.txt\n",
            "Downloading https://github.com/RecoHut-Datasets/retail_rocket/raw/v1/train.txt\n",
            "Using existing file all_train_seq.txt\n",
            "Downloading https://github.com/RecoHut-Datasets/retail_rocket/raw/v1/test.txt\n"
          ]
        }
      ],
      "source": [
        "root = '/content/retail_rocket'\n",
        "\n",
        "train_data = RetailRocketDataset(root=root, shuffle=True, is_train=True)\n",
        "test_data = RetailRocketDataset(root=root, shuffle=False, is_train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NFDHA_D9eRJX"
      },
      "outputs": [],
      "source": [
        "class SampleDataset(SessionGraphDataset):\n",
        "    train_url = \"https://github.com/RecoHut-Datasets/sample_session/raw/v2/train.txt\"\n",
        "    test_url = \"https://github.com/RecoHut-Datasets/sample_session/raw/v2/test.txt\"\n",
        "    all_train_seq_url = \"https://github.com/RecoHut-Datasets/sample_session/raw/v2/all_train_seq.txt\"\n",
        "\n",
        "    def __init__(self, root, shuffle=False, n_node=309, is_train=True):\n",
        "        self.n_node = n_node\n",
        "        self.shuffle = shuffle\n",
        "        self.is_train = is_train\n",
        "        super().__init__(root, shuffle, n_node)\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self) -> str:\n",
        "        if self.is_train:\n",
        "            return ['train.txt', 'all_train_seq.txt']\n",
        "        return ['test.txt', 'all_train_seq.txt']\n",
        "\n",
        "    def download(self):\n",
        "        download_url(self.all_train_seq_url, self.raw_dir)\n",
        "        if self.is_train:\n",
        "            download_url(self.train_url, self.raw_dir)\n",
        "        else:\n",
        "            download_url(self.test_url, self.raw_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TGoKMRIYeRJY",
        "outputId": "7fd7d2a7-afdc-4e35-ffac-b3cb5e8a50cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "Using existing file all_train_seq.txt\n",
            "Downloading https://github.com/RecoHut-Datasets/sample_session/raw/v2/test.txt\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ]
        }
      ],
      "source": [
        "root = '/content/sample'\n",
        "\n",
        "train_data = SampleDataset(root=root, shuffle=True, is_train=True)\n",
        "test_data = SampleDataset(root=root, shuffle=False, is_train=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "name": "datasets > bases > session_graph",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}