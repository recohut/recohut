{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T781774 | SASRec on ML-1m in TF 2.x","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPgehjea9KxZtV+9B0Zar9k"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"JdjjuprkMSy-"},"source":["!pip install tensorflow==2.5.0\n","!pip install tensorflow-gpu==2.5.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-GuKHWjANGpW","executionInfo":{"status":"ok","timestamp":1637058061296,"user_tz":-330,"elapsed":2894,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"04f14de5-d50f-40a4-c03e-2b4c3b81c793"},"source":["!wget -q --show-progress https://files.grouplens.org/datasets/movielens/ml-1m.zip\n","!unzip ml-1m.zip"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ml-1m.zip           100%[===================>]   5.64M  4.17MB/s    in 1.4s    \n","Archive:  ml-1m.zip\n","   creating: ml-1m/\n","  inflating: ml-1m/movies.dat        \n","  inflating: ml-1m/ratings.dat       \n","  inflating: ml-1m/README            \n","  inflating: ml-1m/users.dat         \n"]}]},{"cell_type":"code","metadata":{"id":"PruyIn0iAyRx"},"source":["import os\n","import pandas as pd\n","import numpy as np\n","import random\n","from time import time\n","from tqdm.notebook import tqdm\n","from collections import defaultdict\n","\n","import tensorflow as tf\n","from tensorflow.keras import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.losses import Loss\n","from tensorflow.keras.layers import Layer, Dense, LayerNormalization\n","from tensorflow.keras.layers import Dropout, Embedding, Input, Conv1D"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8LOSVVJUC6au","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637058068948,"user_tz":-330,"elapsed":5337,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"ba92aa67-0516-4dd4-981f-3db331fb553f"},"source":["!pip install -q watermark\n","%reload_ext watermark\n","%watermark -m -iv -u -t -d"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Last updated: 2021-11-16 10:21:08\n","\n","Compiler    : GCC 7.5.0\n","OS          : Linux\n","Release     : 5.4.104+\n","Machine     : x86_64\n","Processor   : x86_64\n","CPU cores   : 2\n","Architecture: 64bit\n","\n","pandas    : 1.1.5\n","IPython   : 5.5.0\n","numpy     : 1.19.5\n","tensorflow: 2.7.0\n","\n"]}]},{"cell_type":"code","metadata":{"id":"9vSaa-LDB4KO"},"source":["os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","\n","file = 'ml-1m/ratings.dat'\n","trans_score = 1\n","maxlen = 200\n","test_neg_num = 100\n","\n","embed_dim = 50\n","blocks = 2\n","num_heads = 1\n","ffn_hidden_unit = 64\n","dropout = 0.2\n","norm_training = True\n","causality = False\n","embed_reg = 0  # 1e-6\n","K = 10\n","\n","learning_rate = 0.001\n","# epochs = 50\n","epochs = 10\n","batch_size = 512"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6YXVIHnhA0_B"},"source":["def sparseFeature(feat, feat_num, embed_dim=4):\n","    \"\"\"\n","    create dictionary for sparse feature\n","    :param feat: feature name\n","    :param feat_num: the total number of sparse features that do not repeat\n","    :param embed_dim: embedding dimension\n","    :return:\n","    \"\"\"\n","    return {'feat': feat, 'feat_num': feat_num, 'embed_dim': embed_dim}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PDHSNwWoA3Lm"},"source":["def create_ml_1m_dataset(file, trans_score=2, embed_dim=8, maxlen=40, test_neg_num=100):\n","    \"\"\"\n","    :param file: A string. dataset path.\n","    :param trans_score: A scalar. Greater than it is 1, and less than it is 0.\n","    :param embed_dim: A scalar. latent factor.\n","    :param maxlen: A scalar. maxlen.\n","    :param test_neg_num: A scalar. The number of test negative samples\n","    :return: user_num, item_num, train_df, test_df\n","    \"\"\"\n","    print('==========Data Preprocess Start=============')\n","    data_df = pd.read_csv(file, sep=\"::\", engine='python',\n","                          names=['user_id', 'item_id', 'label', 'Timestamp'])\n","    # filtering\n","    data_df['item_count'] = data_df.groupby('item_id')['item_id'].transform('count')\n","    data_df = data_df[data_df.item_count >= 5]\n","    # trans score\n","    data_df = data_df[data_df.label >= trans_score]\n","    # sort\n","    data_df = data_df.sort_values(by=['user_id', 'Timestamp'])\n","    # split dataset and negative sampling\n","    print('============Negative Sampling===============')\n","    train_data, val_data, test_data = defaultdict(list), defaultdict(list), defaultdict(list)\n","    item_id_max = data_df['item_id'].max()\n","    for user_id, df in tqdm(data_df[['user_id', 'item_id']].groupby('user_id')):\n","        pos_list = df['item_id'].tolist()\n","\n","        def gen_neg():\n","            neg = pos_list[0]\n","            while neg in set(pos_list):\n","                neg = random.randint(1, item_id_max)\n","            return neg\n","\n","        neg_list = [gen_neg() for i in range(len(pos_list) + test_neg_num)]\n","        for i in range(1, len(pos_list)):\n","            hist_i = pos_list[:i]\n","            if i == len(pos_list) - 1:\n","                test_data['hist'].append(hist_i)\n","                test_data['pos_id'].append(pos_list[i])\n","                test_data['neg_id'].append(neg_list[i:])\n","            elif i == len(pos_list) - 2:\n","                val_data['hist'].append(hist_i)\n","                val_data['pos_id'].append(pos_list[i])\n","                val_data['neg_id'].append(neg_list[i])\n","            else:\n","                train_data['hist'].append(hist_i)\n","                train_data['pos_id'].append(pos_list[i])\n","                train_data['neg_id'].append(neg_list[i])\n","    # item feature columns\n","    user_num, item_num = data_df['user_id'].max() + 1, data_df['item_id'].max() + 1\n","    item_feat_col = sparseFeature('item_id', item_num, embed_dim)\n","    # shuffle\n","    random.shuffle(train_data)\n","    random.shuffle(val_data)\n","    # padding\n","    print('==================Padding===================')\n","    train = [pad_sequences(train_data['hist'], maxlen=maxlen), np.array(train_data['pos_id']),\n","               np.array(train_data['neg_id'])]\n","    val = [pad_sequences(val_data['hist'], maxlen=maxlen), np.array(val_data['pos_id']),\n","             np.array(val_data['neg_id'])]\n","    test = [pad_sequences(test_data['hist'], maxlen=maxlen), np.array(test_data['pos_id']),\n","             np.array(test_data['neg_id'])]\n","    print('============Data Preprocess End=============')\n","    return item_feat_col, train, val, test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vxpW2xTeNRc2"},"source":["def get_angles(pos, i, d_model):\n","    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n","    return pos * angle_rates\n","\n","\n","def positional_encoding(seq_inputs, embed_dim):\n","    angle_rads = get_angles(np.arange(seq_inputs.shape[-1])[:, np.newaxis],\n","                            np.arange(embed_dim)[np.newaxis, :], embed_dim)\n","    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","\n","    pos_encoding = angle_rads[np.newaxis, ...]\n","\n","    return tf.cast(pos_encoding, dtype=tf.float32)\n","\n","\n","def scaled_dot_product_attention(q, k, v, mask, causality=True):\n","    \"\"\"\n","    Attention Mechanism\n","    :param q: A 3d tensor with shape of (None, seq_len, depth), depth = d_model // num_heads\n","    :param k: A 3d tensor with shape of (None, seq_len, depth)\n","    :param v: A 3d tensor with shape of (None, seq_len, depth)\n","    :param mask:\n","    :param causality: Boolean. If True, using causality, default True\n","    :return:\n","    \"\"\"\n","    mat_qk = tf.matmul(q, k, transpose_b=True)  # (None, seq_len, seq_len)\n","    dk = tf.cast(k.shape[-1], dtype=tf.float32)\n","    # Scaled\n","    scaled_att_logits = mat_qk / tf.sqrt(dk)\n","\n","    paddings = tf.ones_like(scaled_att_logits) * (-2 ** 32 + 1)\n","    outputs = tf.where(tf.equal(mask, 0), paddings, scaled_att_logits)  # (None, seq_len, seq_len)\n","\n","    # Causality\n","    if causality:\n","        diag_vals = tf.ones_like(outputs)  # (None, seq_len, seq_len)\n","        masks = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()  # (None, seq_len, seq_len)\n","\n","        paddings = tf.ones_like(masks) * (-2 ** 32 + 1)\n","        outputs = tf.where(tf.equal(masks, 0), paddings, outputs)  # (None, seq_len, seq_len)\n","\n","    # softmax\n","    outputs = tf.nn.softmax(logits=outputs)  # (None, seq_len, seq_len)\n","    outputs = tf.matmul(outputs, v)  # (None, seq_len, depth)\n","\n","    return outputs\n","\n","\n","class MultiHeadAttention(Layer):\n","    def __init__(self, d_model, num_heads, causality=True):\n","        \"\"\"\n","        Multi Head Attention Mechanism\n","        :param d_model: A scalar. The self-attention hidden size.\n","        :param num_heads: A scalar. Number of heads. If num_heads == 1, the layer is a single self-attention layer.\n","        :param causality: Boolean. If True, using causality, default True\n","        \"\"\"\n","        super(MultiHeadAttention, self).__init__()\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.causality = causality\n","\n","        self.wq = Dense(d_model, activation=None)\n","        self.wk = Dense(d_model, activation=None)\n","        self.wv = Dense(d_model, activation=None)\n","\n","    def call(self, q, k, v, mask):\n","        q = self.wq(q)  # (None, seq_len, d_model)\n","        k = self.wk(k)  # (None, seq_len, d_model)\n","        v = self.wv(v)  # (None, seq_len, d_model)\n","\n","        # split d_model into num_heads * depth, and concatenate\n","        q = tf.reshape(tf.concat([tf.split(q, self.num_heads, axis=2)], axis=0),\n","                       (-1, q.shape[1], q.shape[2] // self.num_heads))  # (None * num_heads, seq_len, d_model // num_heads)\n","        k = tf.reshape(tf.concat([tf.split(k, self.num_heads, axis=2)], axis=0),\n","                       (-1, k.shape[1], k.shape[2] // self.num_heads))  # (None * num_heads, seq_len, d_model // num_heads)\n","        v = tf.reshape(tf.concat([tf.split(v, self.num_heads, axis=2)], axis=0),\n","                       (-1, v.shape[1], v.shape[2] // self.num_heads))  # (None * num_heads, seq_len, d_model // num_heads)\n","\n","        # attention\n","        scaled_attention = scaled_dot_product_attention(q, k, v, mask, self.causality)  # (None * num_heads, seq_len, d_model // num_heads)\n","\n","        # Reshape\n","        outputs = tf.concat(tf.split(scaled_attention, self.num_heads, axis=0), axis=2)  # (N, seq_len, d_model)\n","\n","        return outputs\n","\n","\n","class FFN(Layer):\n","    def __init__(self, hidden_unit, d_model):\n","        \"\"\"\n","        Feed Forward Network\n","        :param hidden_unit: A scalar. W1\n","        :param d_model: A scalar. W2\n","        \"\"\"\n","        super(FFN, self).__init__()\n","        self.conv1 = Conv1D(filters=hidden_unit, kernel_size=1, activation='relu', use_bias=True)\n","        self.conv2 = Conv1D(filters=d_model, kernel_size=1, activation=None, use_bias=True)\n","\n","    def call(self, inputs):\n","        x = self.conv1(inputs)\n","        output = self.conv2(x)\n","        return output\n","\n","\n","class EncoderLayer(Layer):\n","    def __init__(self, d_model, num_heads=1, ffn_hidden_unit=128, dropout=0., norm_training=True, causality=True):\n","        \"\"\"\n","        Encoder Layer\n","        :param d_model: A scalar. The self-attention hidden size.\n","        :param num_heads: A scalar. Number of heads.\n","        :param ffn_hidden_unit: A scalar. Number of hidden unit in FFN\n","        :param dropout: A scalar. Number of dropout.\n","        :param norm_training: Boolean. If True, using layer normalization, default True\n","        :param causality: Boolean. If True, using causality, default True\n","        \"\"\"\n","        super(EncoderLayer, self).__init__()\n","        self.mha = MultiHeadAttention(d_model, num_heads, causality)\n","        self.ffn = FFN(ffn_hidden_unit, d_model)\n","\n","        self.layernorm1 = LayerNormalization(epsilon=1e-6, trainable=norm_training)\n","        self.layernorm2 = LayerNormalization(epsilon=1e-6, trainable=norm_training)\n","\n","        self.dropout1 = Dropout(dropout)\n","        self.dropout2 = Dropout(dropout)\n","\n","    def call(self, inputs):\n","        x, mask = inputs\n","        # self-attention\n","        att_out = self.mha(x, x, x, mask)  # ï¼ˆNone, seq_len, d_model)\n","        att_out = self.dropout1(att_out)\n","        # residual add\n","        out1 = self.layernorm1(x + att_out)\n","        # ffn\n","        ffn_out = self.ffn(out1)\n","        ffn_out = self.dropout2(ffn_out)\n","        # residual add\n","        out2 = self.layernorm2(out1 + ffn_out)  # (None, seq_len, d_model)\n","        return out2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jvILsUdsNRZy"},"source":["class SASRec(tf.keras.Model):\n","    def __init__(self, item_fea_col, blocks=1, num_heads=1, ffn_hidden_unit=128,\n","                 dropout=0., maxlen=40, norm_training=True, causality=False, embed_reg=1e-6):\n","        \"\"\"\n","        SASRec model\n","        :param item_fea_col: A dict contains 'feat_name', 'feat_num' and 'embed_dim'.\n","        :param blocks: A scalar. The Number of blocks.\n","        :param num_heads: A scalar. Number of heads.\n","        :param ffn_hidden_unit: A scalar. Number of hidden unit in FFN\n","        :param dropout: A scalar. Number of dropout.\n","        :param maxlen: A scalar. Number of length of sequence\n","        :param norm_training: Boolean. If True, using layer normalization, default True\n","        :param causality: Boolean. If True, using causality, default True\n","        :param embed_reg: A scalar. The regularizer of embedding\n","        \"\"\"\n","        super(SASRec, self).__init__()\n","        # sequence length\n","        self.maxlen = maxlen\n","        # item feature columns\n","        self.item_fea_col = item_fea_col\n","        # embed_dim\n","        self.embed_dim = self.item_fea_col['embed_dim']\n","        # d_model must be the same as embedding_dim, because of residual connection\n","        self.d_model = self.embed_dim\n","        # item embedding\n","        self.item_embedding = Embedding(input_dim=self.item_fea_col['feat_num'],\n","                                        input_length=1,\n","                                        output_dim=self.item_fea_col['embed_dim'],\n","                                        mask_zero=True,\n","                                        embeddings_initializer='random_uniform',\n","                                        embeddings_regularizer=l2(embed_reg))\n","        self.pos_embedding = Embedding(input_dim=self.maxlen,\n","                                       input_length=1,\n","                                       output_dim=self.embed_dim,\n","                                       mask_zero=False,\n","                                       embeddings_initializer='random_uniform',\n","                                       embeddings_regularizer=l2(embed_reg))\n","        self.dropout = Dropout(dropout)\n","        # attention block\n","        self.encoder_layer = [EncoderLayer(self.d_model, num_heads, ffn_hidden_unit,\n","                                           dropout, norm_training, causality) for b in range(blocks)]\n","\n","    def call(self, inputs, training=None):\n","        # inputs\n","        seq_inputs, pos_inputs, neg_inputs = inputs  # (None, maxlen), (None, 1), (None, 1)\n","        # mask\n","        mask = tf.expand_dims(tf.cast(tf.not_equal(seq_inputs, 0), dtype=tf.float32), axis=-1)  # (None, maxlen, 1)\n","        # seq info\n","        seq_embed = self.item_embedding(seq_inputs)  # (None, maxlen, dim)\n","        # pos encoding\n","        # pos_encoding = positional_encoding(seq_inputs, self.embed_dim)\n","        pos_encoding = tf.expand_dims(self.pos_embedding(tf.range(self.maxlen)), axis=0)\n","        seq_embed += pos_encoding\n","        seq_embed = self.dropout(seq_embed)\n","        att_outputs = seq_embed  # (None, maxlen, dim)\n","        att_outputs *= mask\n","\n","        # self-attention\n","        for block in self.encoder_layer:\n","            att_outputs = block([att_outputs, mask])  # (None, seq_len, dim)\n","            att_outputs *= mask\n","\n","        # user_info = tf.reduce_mean(att_outputs, axis=1)  # (None, dim)\n","        user_info = tf.expand_dims(att_outputs[:, -1], axis=1)  # (None, 1, dim)\n","        # item info\n","        pos_info = self.item_embedding(pos_inputs)  # (None, 1, dim)\n","        neg_info = self.item_embedding(neg_inputs)  # (None, 1/100, dim)\n","        pos_logits = tf.reduce_sum(user_info * pos_info, axis=-1)  # (None, 1)\n","        neg_logits = tf.reduce_sum(user_info * neg_info, axis=-1)  # (None, 1)\n","        # loss\n","        losses = tf.reduce_mean(- tf.math.log(tf.nn.sigmoid(pos_logits)) -\n","                                tf.math.log(1 - tf.nn.sigmoid(neg_logits))) / 2\n","        self.add_loss(losses)\n","        logits = tf.concat([pos_logits, neg_logits], axis=-1)\n","        return logits\n","\n","    def summary(self):\n","        seq_inputs = Input(shape=(self.maxlen,), dtype=tf.int32)\n","        pos_inputs = Input(shape=(1,), dtype=tf.int32)\n","        neg_inputs = Input(shape=(1,), dtype=tf.int32)\n","        tf.keras.Model(inputs=[seq_inputs, pos_inputs, neg_inputs],\n","                       outputs=self.call([seq_inputs, pos_inputs, neg_inputs])).summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jNhwtb9JN6m7"},"source":["def test_model():\n","    item_fea_col = {'feat': 'item_id', 'feat_num': 100, 'embed_dim': 8}\n","    model = SASRec(item_fea_col, num_heads=8)\n","    model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nM2p7ZIgNRW2"},"source":["def evaluate_model(model, test, K):\n","    \"\"\"\n","    evaluate model\n","    :param model: model\n","    :param test: test set\n","    :param K: top K\n","    :return: hit rate, ndcg\n","    \"\"\"\n","    pred_y = - model.predict(test)\n","    rank = pred_y.argsort().argsort()[:, 0]\n","    hr, ndcg = 0.0, 0.0\n","    for r in rank:\n","        if r < K:\n","            hr += 1\n","            ndcg += 1 / np.log2(r + 2)\n","    return hr / len(rank), ndcg / len(rank)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qubt4TjBNRTY"},"source":["# ========================== Create dataset =======================\n","item_fea_col, train, val, test = create_ml_1m_dataset(file, trans_score, embed_dim, maxlen, test_neg_num)\n","\n","# ============================Build Model==========================\n","mirrored_strategy = tf.distribute.MirroredStrategy()\n","with mirrored_strategy.scope():\n","    model = SASRec(item_fea_col, blocks, num_heads, ffn_hidden_unit, dropout,\n","                    maxlen, norm_training, causality, embed_reg)\n","    model.summary()\n","    # =========================Compile============================\n","    model.compile(optimizer=Adam(learning_rate=learning_rate))\n","\n","results = []\n","for epoch in range(1, epochs + 1):\n","    # ===========================Fit==============================\n","    t1 = time()\n","    model.fit(\n","        train,\n","        validation_data=(val, None),\n","        epochs=1,\n","        batch_size=batch_size,\n","    )\n","    # ===========================Test==============================\n","    t2 = time()\n","    if epoch % 5 == 0:\n","        hit_rate, ndcg = evaluate_model(model, test, K)\n","        print('Iteration %d Fit [%.1f s], Evaluate [%.1f s]: HR = %.4f, NDCG = %.4f, '\n","                % (epoch, t2 - t1, time() - t2, hit_rate, ndcg))\n","        results.append([epoch + 1, t2 - t1, time() - t2, hit_rate, ndcg])\n","# ============================Write============================\n","pd.DataFrame(results, columns=['Iteration', 'fit_time', 'evaluate_time', 'hit_rate', 'ndcg']).\\\n","    to_csv('SASRec_log_maxlen_{}_dim_{}_blocks_{}_heads_{}_K_{}_.csv'.\n","            format(maxlen, embed_dim, blocks, num_heads, K), index=False)"],"execution_count":null,"outputs":[]}]}