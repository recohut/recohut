{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-11-mbgmn-beibei.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/P197505%20%7C%20MB-GMN%20on%20BeiBei.ipynb","timestamp":1644603071396}],"collapsed_sections":[],"authorship_tag":"ABX9TyOnRz+yrAZ52jPIuWSjRWoM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"24c87c2912974b6d8becb5b9190aa829":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4314716903e64982b95392e7e37156bd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6f654bcada5d41f78b08c66d2e692fde","IPY_MODEL_9dd6ba2d32ff4c0cb76b467e51e8060c","IPY_MODEL_7fd736fa68424784a086f63ba9a43657"]}},"4314716903e64982b95392e7e37156bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6f654bcada5d41f78b08c66d2e692fde":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0bbb945cc6d54d289cbf67827ae1b723","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_551b14c6f28644eea7c40845274e672c"}},"9dd6ba2d32ff4c0cb76b467e51e8060c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_02be78ca98ab4b39adaa14cfd532ce00","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":10,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":10,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0343c7f1998946c68782b6fa7993b824"}},"7fd736fa68424784a086f63ba9a43657":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_439b1cc5861d446b84eba9b023d4d410","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 10/10 [10:19&lt;00:00, 58.37s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1ceba6676a0f4736961546da9a54dac6"}},"0bbb945cc6d54d289cbf67827ae1b723":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"551b14c6f28644eea7c40845274e672c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"02be78ca98ab4b39adaa14cfd532ce00":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0343c7f1998946c68782b6fa7993b824":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"439b1cc5861d446b84eba9b023d4d410":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1ceba6676a0f4736961546da9a54dac6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","source":["# MB-GMN on BeiBei"],"metadata":{"id":"LOAjWAu1zj56"}},{"cell_type":"markdown","source":["## Executive summary\n","| | |\n","| --- | --- |\n","| Problem | Modern recommender systems often embed users and items into low-dimensional latent representations, based on their observed interactions. In practical recommendation scenarios, users often exhibit various intents which drive them to interact with items with multiple behavior types (e.g., click, tag-as-favorite, purchase). However, the diversity of user behaviors is ignored in most of the existing approaches, which makes them difficult to capture heterogeneous relational structures across different types of interactive behaviors. |\n","| Prblm Stmt. | We define a three-way tensor $X \\in \\mathbb{R}^{ð¼Ã—ð½Ã—ð¾}$ to reflect the multi-typed interaction (e.g., click, tag-as-favorite, purchase) between user ($ð‘¢_ð‘– \\in U$) and item ($ð‘£_ð‘— \\in V$). Here, ð¾ denotes the number of behavior types. Specifically, the individual element $ð‘¥_{i,j}^k \\in X$ is set as 1 if user $ð‘¢_ð‘–$ interacts with item $ð‘£_ð‘—$ under the ð‘˜-th behavior type, and $ð‘˜_{ð‘–,ð‘—}$ = 0 otherwise. In the multi-behavior recommendation scenario, one type of user-item interaction will be treated as target behavior (e.g., purchase). Other behaviors are referred to as context behaviors (e.g., click, tag-as-favorite, add-to-cart) for providing insightful knowledge in assisting the target behavior prediction. Based on the aforementioned definitions, we formally state our studied problem as: â€¢ Input: the observed multi-behavior interaction tensor $X \\in \\mathbb{R}^{ð¼Ã—ð½Ã—ð¾}$ between users U and items V across ð¾ behavior types. â€¢ Output: the predictive function which estimates the likelihood of user $ð‘¢_ð‘–$ adopts the item $ð‘£_ð‘—$ with the target behavior type. |\n","| Solution | Multi-Behavior recommendation framework with Graph Meta Network (MB-GMN) incorporate the multi-behavior pattern modeling into a meta-learning paradigm. Our developed MB-GMN empowers the user-item interaction learning with the capability of uncovering type-dependent behavior representations, which automatically distills the behavior heterogeneity and interaction diversity for recommendations. MB-GMN is composed of two key components: i) multi-behavior pattern encoding, a meta-knowledge learner that captures the personalized multi-behavior characteristics; ii) cross-type behavior dependency modeling, a transfer learning paradigm which learns a well-customized prediction network by transferring knowledge across different behavior types. |\n","| Dataset | BeiBei. |\n","| Preprocessing | Leave-one-out evaluation is leveraged for training and test set partition. We generate the test data set by including the last interactive item and consider the rest of data for training. For efficient and fair model evaluation, we pair each positive item instance with 99 randomly sampled non-interactive items for each user. We regard usersâ€™ purchases as the target behaviors. |\n","| Metrics | NDCG, HR |\n","| Models | MB-GMN |\n","| Cluster | Python 3.6+, Tensorflow 1.x |\n","| Tags | `MetaLearning`, `GNN` |\n","| Credits | akaxlh |"],"metadata":{"id":"bpPKhl0fzj2G"}},{"cell_type":"markdown","source":["## Process flow\n","\n","![](https://github.com/RecoHut-Stanzas/S346877/raw/main/images/process_flow.svg)"],"metadata":{"id":"B3NwTUYRzjyz"}},{"cell_type":"markdown","source":["## Model architecture\n","\n","![](https://github.com/RecoHut-Stanzas/S346877/raw/main/images/mbgmn_model.png)"],"metadata":{"id":"UiUls4wo5IBp"}},{"cell_type":"markdown","source":["*The model architecture of MB-GMN. (a) Multi-behavior pattern modeling with meta-knowledge learner for behavior heterogeneity; (b) Meta graph neural network which preserves the behavior semantics with high-order connectivity; (c) Metaknowledge transfer networks that customize the parameter of prediction layer to capture cross-type behavior dependency.*"],"metadata":{"id":"i35fbAIe5P2d"}},{"cell_type":"markdown","source":["## Setup"],"metadata":{"id":"uZ5MRgBc0v49"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A3mT2WvpnDfP","executionInfo":{"status":"ok","timestamp":1639298230167,"user_tz":-330,"elapsed":541,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"2819ceb9-ae56-40fe-e732-f7879df70c5e"},"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow 1.x selected.\n"]}],"source":["%tensorflow_version 1.x"]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.core.protobuf import config_pb2\n","import pickle\n","import argparse\n","import random\n","import gc\n","import datetime\n","from tensorflow.contrib.layers import xavier_initializer\n","from scipy.sparse import csr_matrix\n","import scipy.sparse as sp\n","from tqdm.notebook import tqdm"],"metadata":{"id":"_sPDAiS8nNN_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"],"metadata":{"id":"kIIytIU8s3Jq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.makedirs('./History', exist_ok=True)"],"metadata":{"id":"vSKHNr0Atb8S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def parse_args():\n","\tparser = argparse.ArgumentParser(description='Model Params')\n","\tparser.add_argument('--lr', default=1e-3, type=float, help='learning rate')\n","\tparser.add_argument('--batch', default=256, type=int, help='batch size')\n","\tparser.add_argument('--reg', default=1e-2, type=float, help='weight decay regularizer')\n","\tparser.add_argument('--epoch', default=10, type=int, help='number of epochs')\n","\tparser.add_argument('--decay', default=0.96, type=float, help='weight decay rate')\n","\tparser.add_argument('--save_path', default='tem', help='file name to save model and training record')\n","\tparser.add_argument('--latdim', default=32, type=int, help='embedding size')\n","\tparser.add_argument('--rank', default=4, type=int, help='embedding size')\n","\tparser.add_argument('--memosize', default=2, type=int, help='memory size')\n","\tparser.add_argument('--sampNum', default=40, type=int, help='batch size for sampling')\n","\tparser.add_argument('--att_head', default=2, type=int, help='number of attention heads')\n","\tparser.add_argument('--gnn_layer', default=2, type=int, help='number of gnn layers')\n","\tparser.add_argument('--trnNum', default=10000, type=int, help='number of training instances per epoch')\n","\tparser.add_argument('--load_model', default=None, help='model name to load')\n","\tparser.add_argument('--shoot', default=10, type=int, help='K of top k')\n","\tparser.add_argument('--data', default='beibei', type=str, help='name of dataset')\n","\tparser.add_argument('--target', default='buy', type=str, help='target behavior to predict on')\n","\tparser.add_argument('--deep_layer', default=0, type=int, help='number of deep layers to make the final prediction')\n","\tparser.add_argument('--mult', default=100, type=float, help='multiplier for the result')\n","\tparser.add_argument('--keepRate', default=0.7, type=float, help='rate for dropout')\n","\tparser.add_argument('--slot', default=5, type=float, help='length of time slots')\n","\tparser.add_argument('--graphSampleN', default=15000, type=int, help='use 25000 for training and 200000 for testing, empirically')\n","\tparser.add_argument('--divSize', default=10000, type=int, help='div size for smallTestEpoch')\n","\tparser.add_argument('--tstEpoch', default=3, type=int, help='number of epoch to test while training')\n","\tparser.add_argument('--subUsrSize', default=10, type=int, help='number of item for each sub-user')\n","\tparser.add_argument('--subUsrDcy', default=0.9, type=float, help='decay factor for sub-users over time')\n","\treturn parser.parse_args([])\n"," \n","args = parse_args()\n","\n","args.user = 21716\n","args.item = 7977\n","args.decay_step = args.trnNum//args.batch"],"metadata":{"id":"92B7jEwLtA0j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Utils"],"metadata":{"id":"wVCw2Kc31DdY"}},{"cell_type":"code","source":["logmsg = ''\n","timemark = dict()\n","saveDefault = False\n","\n","def log(msg, save=None, oneline=False):\n","\tglobal logmsg\n","\tglobal saveDefault\n","\ttime = datetime.datetime.now()\n","\ttem = '%s: %s' % (time, msg)\n","\tif save != None:\n","\t\tif save:\n","\t\t\tlogmsg += tem + '\\n'\n","\telif saveDefault:\n","\t\tlogmsg += tem + '\\n'\n","\tif oneline:\n","\t\tprint(tem, end='\\r')\n","\telse:\n","\t\tprint(tem)\n","\n","def marktime(marker):\n","\tglobal timemark\n","\ttimemark[marker] = datetime.datetime.now()\n","\n","def SpentTime(marker):\n","\tglobal timemark\n","\tif marker not in timemark:\n","\t\tmsg = 'LOGGER ERROR, marker', marker, ' not found'\n","\t\ttem = '%s: %s' % (time, msg)\n","\t\tprint(tem)\n","\t\treturn False\n","\treturn datetime.datetime.now() - timemark[marker]\n","\n","def SpentTooLong(marker, day=0, hour=0, minute=0, second=0):\n","\tglobal timemark\n","\tif marker not in timemark:\n","\t\tmsg = 'LOGGER ERROR, marker', marker, ' not found'\n","\t\ttem = '%s: %s' % (time, msg)\n","\t\tprint(tem)\n","\t\treturn False\n","\treturn datetime.datetime.now() - timemark[marker] >= datetime.timedelta(days=day, hours=hour, minutes=minute, seconds=second)"],"metadata":{"id":"Z80jSTrUtr8g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def RandomShuffle(infile, outfile, deleteSchema=False):\n","\twith open(infile, 'r', encoding='utf-8') as fs:\n","\t\tarr = fs.readlines()\n","\tif not arr[-1].endswith('\\n'):\n","\t\tarr[-1] += '\\n'\n","\tif deleteSchema:\n","\t\tarr = arr[1:]\n","\trandom.shuffle(arr)\n","\twith open(outfile, 'w', encoding='utf-8') as fs:\n","\t\tfor line in arr:\n","\t\t\tfs.write(line)\n","\tdel arr\n","\n","def WriteToBuff(buff, line, out):\n","\tBUFF_SIZE = 1000000\n","\tbuff.append(line)\n","\tif len(buff) == BUFF_SIZE:\n","\t\tWriteToDisk(buff, out)\n","\n","def WriteToDisk(buff, out):\n","\twith open(out, 'a', encoding='utf-8') as fs:\n","\t\tfor line in buff:\n","\t\t\tfs.write(line)\n","\tbuff.clear()\n","\n","\n","def SubDataSet(infile, outfile1, outfile2, rate):\n","\tout1 = list()\n","\tout2 = list()\n","\twith open(infile, 'r', encoding='utf-8') as fs:\n","\t\tfor line in fs:\n","\t\t\tif random.random() < rate:\n","\t\t\t\tWriteToBuff(out1, line, outfile1)\n","\t\t\telse:\n","\t\t\t\tWriteToBuff(out2, line, outfile2)\n","\tWriteToDisk(out1, outfile1)\n","\tWriteToDisk(out2, outfile2)\n","\n","def CombineFiles(files, out):\n","\tbuff = list()\n","\tfor file in files:\n","\t\twith open(file, 'r') as fs:\n","\t\t\tfor line in fs:\n","\t\t\t\tWriteToBuff(buff, line, out)\n","\tWriteToDisk(buff, out)"],"metadata":{"id":"0BvkMCQytcGV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## NN Layers"],"metadata":{"id":"X5d4EeIU1ARg"}},{"cell_type":"code","source":["paramId = 0\n","biasDefault = False\n","params = {}\n","regParams = {}\n","ita = 0.2\n","leaky = 0.1\n","\n","def getParamId():\n","\tglobal paramId\n","\tparamId += 1\n","\treturn paramId\n","\n","def setIta(ITA):\n","\tita = ITA\n","\n","def setBiasDefault(val):\n","\tglobal biasDefault\n","\tbiasDefault = val\n","\n","def getParam(name):\n","\treturn params[name]\n","\n","def addReg(name, param):\n","\tglobal regParams\n","\tif name not in regParams:\n","\t\tregParams[name] = param\n","\telse:\n","\t\tprint('ERROR: Parameter already exists')\n","\n","def addParam(name, param):\n","\tglobal params\n","\tif name not in params:\n","\t\tparams[name] = param\n","\n","def defineRandomNameParam(shape, dtype=tf.float32, reg=False, initializer='xavier', trainable=True):\n","\tname = 'defaultParamName%d'%getParamId()\n","\treturn defineParam(name, shape, dtype, reg, initializer, trainable)\n","\n","def defineParam(name, shape, dtype=tf.float32, reg=False, initializer='xavier', trainable=True):\n","\tglobal params\n","\tglobal regParams\n","\tassert name not in params, 'name %s already exists' % name\n","\tif initializer == 'xavier':\n","\t\tret = tf.get_variable(name=name, dtype=dtype, shape=shape,\n","\t\t\tinitializer=xavier_initializer(dtype=tf.float32),\n","\t\t\ttrainable=trainable)\n","\telif initializer == 'trunc_normal':\n","\t\tret = tf.get_variable(name=name, initializer=tf.random.truncated_normal(shape=[int(shape[0]), shape[1]], mean=0.0, stddev=0.03, dtype=dtype))\n","\telif initializer == 'zeros':\n","\t\tret = tf.get_variable(name=name, dtype=dtype,\n","\t\t\tinitializer=tf.zeros(shape=shape, dtype=tf.float32),\n","\t\t\ttrainable=trainable)\n","\telif initializer == 'ones':\n","\t\tret = tf.get_variable(name=name, dtype=dtype, initializer=tf.ones(shape=shape, dtype=tf.float32), trainable=trainable)\n","\telif not isinstance(initializer, str):\n","\t\tret = tf.get_variable(name=name, dtype=dtype,\n","\t\t\tinitializer=initializer, trainable=trainable)\n","\telse:\n","\t\tprint('ERROR: Unrecognized initializer')\n","\t\texit()\n","\tparams[name] = ret\n","\tif reg:\n","\t\tregParams[name] = ret\n","\treturn ret\n","\n","def getOrDefineParam(name, shape, dtype=tf.float32, reg=False, initializer='xavier', trainable=True, reuse=False):\n","\tglobal params\n","\tglobal regParams\n","\tif name in params:\n","\t\tassert reuse, 'Reusing Param %s Not Specified' % name\n","\t\tif reg and name not in regParams:\n","\t\t\tregParams[name] = params[name]\n","\t\treturn params[name]\n","\treturn defineParam(name, shape, dtype, reg, initializer, trainable)\n","\n","def BN(inp, name=None):\n","\tglobal ita\n","\tdim = inp.get_shape()[1]\n","\tname = 'defaultParamName%d'%getParamId()\n","\tscale = tf.Variable(tf.ones([dim]))\n","\tshift = tf.Variable(tf.zeros([dim]))\n","\tfcMean, fcVar = tf.nn.moments(inp, axes=[0])\n","\tema = tf.train.ExponentialMovingAverage(decay=0.5)\n","\temaApplyOp = ema.apply([fcMean, fcVar])\n","\twith tf.control_dependencies([emaApplyOp]):\n","\t\tmean = tf.identity(fcMean)\n","\t\tvar = tf.identity(fcVar)\n","\tret = tf.nn.batch_normalization(inp, mean, var, shift,\n","\t\tscale, 1e-8)\n","\treturn ret\n","\n","def FC(inp, outDim, name=None, useBias=False, activation=None, reg=False, useBN=False, dropout=None, initializer='xavier', reuse=False, biasReg=False, biasInitializer='zeros'):\n","\tglobal params\n","\tglobal regParams\n","\tglobal leaky\n","\tinDim = inp.get_shape()[1]\n","\ttemName = name if name!=None else 'defaultParamName%d'%getParamId()\n","\tW = getOrDefineParam(temName, [inDim, outDim], reg=reg, initializer=initializer, reuse=reuse)\n","\tif dropout != None:\n","\t\tret = tf.nn.dropout(inp, rate=dropout) @ W\n","\telse:\n","\t\tret = inp @ W\n","\tif useBias:\n","\t\tret = Bias(ret, name=name, reuse=reuse, reg=biasReg, initializer=biasInitializer)\n","\tif useBN:\n","\t\tret = BN(ret)\n","\tif activation != None:\n","\t\tret = Activate(ret, activation)\n","\treturn ret\n","\n","def Bias(data, name=None, reg=False, reuse=False, initializer='zeros'):\n","\tinDim = data.get_shape()[-1]\n","\ttemName = name if name!=None else 'defaultParamName%d'%getParamId()\n","\ttemBiasName = temName + 'Bias'\n","\tbias = getOrDefineParam(temBiasName, inDim, reg=False, initializer=initializer, reuse=reuse)\n","\tif reg:\n","\t\tregParams[temBiasName] = bias\n","\treturn data + bias\n","\n","def ActivateHelp(data, method):\n","\tif method == 'relu':\n","\t\tret = tf.nn.relu(data)\n","\telif method == 'sigmoid':\n","\t\tret = tf.nn.sigmoid(data)\n","\telif method == 'tanh':\n","\t\tret = tf.nn.tanh(data)\n","\telif method == 'softmax':\n","\t\tret = tf.nn.softmax(data, axis=-1)\n","\telif method == 'leakyRelu':\n","\t\tret = tf.maximum(leaky*data, data)\n","\telif method == 'twoWayLeakyRelu6':\n","\t\ttemMask = tf.to_float(tf.greater(data, 6.0))\n","\t\tret = temMask * (6 + leaky * (data - 6)) + (1 - temMask) * tf.maximum(leaky * data, data)\n","\telif method == '-1relu':\n","\t\tret = tf.maximum(-1.0, data)\n","\telif method == 'relu6':\n","\t\tret = tf.maximum(0.0, tf.minimum(6.0, data))\n","\telif method == 'relu3':\n","\t\tret = tf.maximum(0.0, tf.minimum(3.0, data))\n","\telse:\n","\t\traise Exception('Error Activation Function')\n","\treturn ret\n","\n","def Activate(data, method, useBN=False):\n","\tglobal leaky\n","\tif useBN:\n","\t\tret = BN(data)\n","\telse:\n","\t\tret = data\n","\tret = ActivateHelp(ret, method)\n","\treturn ret\n","\n","def Regularize(names=None, method='L2'):\n","\tret = 0\n","\tif method == 'L1':\n","\t\tif names != None:\n","\t\t\tfor name in names:\n","\t\t\t\tret += tf.reduce_sum(tf.abs(getParam(name)))\n","\t\telse:\n","\t\t\tfor name in regParams:\n","\t\t\t\tret += tf.reduce_sum(tf.abs(regParams[name]))\n","\telif method == 'L2':\n","\t\tif names != None:\n","\t\t\tfor name in names:\n","\t\t\t\tret += tf.reduce_sum(tf.square(getParam(name)))\n","\t\telse:\n","\t\t\tfor name in regParams:\n","\t\t\t\tret += tf.reduce_sum(tf.square(regParams[name]))\n","\treturn ret\n","\n","def Dropout(data, rate):\n","\tif rate == None:\n","\t\treturn data\n","\telse:\n","\t\treturn tf.nn.dropout(data, rate=rate)\n","\n","def selfAttention(localReps, number, inpDim, numHeads):\n","\tQ = defineRandomNameParam([inpDim, inpDim], reg=True)\n","\tK = defineRandomNameParam([inpDim, inpDim], reg=True)\n","\tV = defineRandomNameParam([inpDim, inpDim], reg=True)\n","\trspReps = tf.reshape(tf.stack(localReps, axis=1), [-1, inpDim])\n","\tq = tf.reshape(rspReps @ Q, [-1, number, 1, numHeads, inpDim//numHeads])\n","\tk = tf.reshape(rspReps @ K, [-1, 1, number, numHeads, inpDim//numHeads])\n","\tv = tf.reshape(rspReps @ V, [-1, 1, number, numHeads, inpDim//numHeads])\n","\tatt = tf.nn.softmax(tf.reduce_sum(q * k, axis=-1, keepdims=True) / tf.sqrt(inpDim/numHeads), axis=2)\n","\tattval = tf.reshape(tf.reduce_sum(att * v, axis=2), [-1, number, inpDim])\n","\trets = [None] * number\n","\tparamId = 'dfltP%d' % getParamId()\n","\tfor i in range(number):\n","\t\ttem1 = tf.reshape(tf.slice(attval, [0, i, 0], [-1, 1, -1]), [-1, inpDim])\n","\t\t# tem2 = FC(tem1, inpDim, useBias=True, name=paramId+'_1', reg=True, activation='relu', reuse=True) + localReps[i]\n","\t\trets[i] = tem1 + localReps[i]\n","\treturn rets\n","\n","def lightSelfAttention(localReps, number, inpDim, numHeads):\n","\tQ = defineRandomNameParam([inpDim, inpDim], reg=True)\n","\trspReps = tf.reshape(tf.stack(localReps, axis=1), [-1, inpDim])\n","\ttem = rspReps @ Q\n","\tq = tf.reshape(tem, [-1, number, 1, numHeads, inpDim//numHeads])\n","\tk = tf.reshape(tem, [-1, 1, number, numHeads, inpDim//numHeads])\n","\tv = tf.reshape(rspReps, [-1, 1, number, numHeads, inpDim//numHeads])\n","\t# att = tf.nn.softmax(tf.reduce_sum(q * k, axis=-1, keepdims=True) * tf.sqrt(inpDim/numHeads), axis=2)\n","\tatt = tf.nn.softmax(tf.reduce_sum(q * k, axis=-1, keepdims=True) / tf.sqrt(inpDim/numHeads), axis=2)\n","\tattval = tf.reshape(tf.reduce_sum(att * v, axis=2), [-1, number, inpDim])\n","\trets = [None] * number\n","\tparamId = 'dfltP%d' % getParamId()\n","\tfor i in range(number):\n","\t\ttem1 = tf.reshape(tf.slice(attval, [0, i, 0], [-1, 1, -1]), [-1, inpDim])\n","\t\t# tem2 = FC(tem1, inpDim, useBias=True, name=paramId+'_1', reg=True, activation='relu', reuse=True) + localReps[i]\n","\t\trets[i] = tem1 + localReps[i]\n","\treturn rets#, tf.squeeze(att)"],"metadata":{"id":"C5z3c78ctcD0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset"],"metadata":{"id":"ZBlNyUpR08Qr"}},{"cell_type":"code","source":["!git clone --branch v1 https://github.com/RecoHut-Datasets/beibei.git"],"metadata":{"id":"LiC15-XSnFCC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def transpose(mat):\n","\tcoomat = sp.coo_matrix(mat)\n","\treturn csr_matrix(coomat.transpose())\n","\n","def negSamp(temLabel, sampSize, nodeNum):\n","\tnegset = [None] * sampSize\n","\tcur = 0\n","\twhile cur < sampSize:\n","\t\trdmItm = np.random.choice(nodeNum)\n","\t\tif temLabel[rdmItm] == 0:\n","\t\t\tnegset[cur] = rdmItm\n","\t\t\tcur += 1\n","\treturn negset\n","\n","def transToLsts(mat, mask=False, norm=False):\n","\tshape = [mat.shape[0], mat.shape[1]]\n","\tcoomat = sp.coo_matrix(mat)\n","\tindices = np.array(list(map(list, zip(coomat.row, coomat.col))), dtype=np.int32)\n","\tdata = coomat.data.astype(np.float32)\n","\n","\tif norm:\n","\t\trowD = np.squeeze(np.array(1 / (np.sqrt(np.sum(mat, axis=1) + 1e-8) + 1e-8)))\n","\t\tcolD = np.squeeze(np.array(1 / (np.sqrt(np.sum(mat, axis=0) + 1e-8) + 1e-8)))\n","\t\tfor i in range(len(data)):\n","\t\t\trow = indices[i, 0]\n","\t\t\tcol = indices[i, 1]\n","\t\t\tdata[i] = data[i] * rowD[row] * colD[col]\n","\n","\t# half mask\n","\tif mask:\n","\t\tspMask = (np.random.uniform(size=data.shape) > 0.5) * 1.0\n","\t\tdata = data * spMask\n","\n","\tif indices.shape[0] == 0:\n","\t\tindices = np.array([[0, 0]], dtype=np.int32)\n","\t\tdata = np.array([0.0], np.float32)\n","\treturn indices, data, shape\n","\n","class DataHandler:\n","    def __init__(self):\n","        predir = './beibei/'\n","        behs = ['pv', 'cart', 'buy']\n","        self.predir = predir\n","        self.behs = behs\n","        self.trnfile = predir + 'trn_'\n","        self.tstfile = predir + 'tst_'\n","\n","    def LoadData(self):\n","        trnMats = list()\n","        for i in range(len(self.behs)):\n","            beh = self.behs[i]\n","            path = self.trnfile + beh\n","            with open(path, 'rb') as fs:\n","                mat = (pickle.load(fs) != 0).astype(np.float32)\n","            trnMats.append(mat)\n","        # test set\n","        path = self.tstfile + 'int'\n","        with open(path, 'rb') as fs:\n","            tstInt = np.array(pickle.load(fs))\n","        tstStat = (tstInt != None)\n","        tstUsrs = np.reshape(np.argwhere(tstStat != False), [-1])\n","        self.trnMats = trnMats\n","        self.tstInt = tstInt\n","        self.tstUsrs = tstUsrs\n","        args.user, args.item = self.trnMats[0].shape\n","        args.behNum = len(self.behs)\n","        self.prepareGlobalData()\n","\n","    def prepareGlobalData(self):\n","        adj = 0\n","        for i in range(args.behNum):\n","            adj = adj + self.trnMats[i]\n","        adj = (adj != 0).astype(np.float32)\n","        self.labelP = np.squeeze(np.array(np.sum(adj, axis=0)))\n","        tpadj = transpose(adj)\n","        adjNorm = np.reshape(np.array(np.sum(adj, axis=1)), [-1])\n","        tpadjNorm = np.reshape(np.array(np.sum(tpadj, axis=1)), [-1])\n","        for i in range(adj.shape[0]):\n","            for j in range(adj.indptr[i], adj.indptr[i+1]):\n","                adj.data[j] /= adjNorm[i]\n","        for i in range(tpadj.shape[0]):\n","            for j in range(tpadj.indptr[i], tpadj.indptr[i+1]):\n","                tpadj.data[j] /= tpadjNorm[i]\n","        self.adj = adj\n","        self.tpadj = tpadj\n","\n","    def sampleLargeGraph(self, pckUsrs, pckItms=None, sampDepth=2, sampNum=args.graphSampleN, preSamp=False):\n","        adj = self.adj\n","        tpadj = self.tpadj\n","        def makeMask(nodes, size):\n","            mask = np.ones(size)\n","            if not nodes is None:\n","                mask[nodes] = 0.0\n","            return mask\n","\n","        def updateBdgt(adj, nodes):\n","            if nodes is None:\n","                return 0\n","            tembat = 1000\n","            ret = 0\n","            for i in range(int(np.ceil(len(nodes) / tembat))):\n","                st = tembat * i\n","                ed = min((i+1) * tembat, len(nodes))\n","                temNodes = nodes[st: ed]\n","                ret += np.sum(adj[temNodes], axis=0)\n","            return ret\n","\n","        def sample(budget, mask, sampNum):\n","            score = (mask * np.reshape(np.array(budget), [-1])) ** 2\n","            norm = np.sum(score)\n","            if norm == 0:\n","                return np.random.choice(len(score), 1), sampNum - 1\n","            score = list(score / norm)\n","            arrScore = np.array(score)\n","            posNum = np.sum(np.array(score)!=0)\n","            if posNum < sampNum:\n","                pckNodes1 = np.squeeze(np.argwhere(arrScore!=0))\n","                # pckNodes2 = np.random.choice(np.squeeze(np.argwhere(arrScore==0.0)), min(len(score) - posNum, sampNum - posNum), replace=False)\n","                # pckNodes = np.concatenate([pckNodes1, pckNodes2], axis=0)\n","                pckNodes = pckNodes1\n","            else:\n","                pckNodes = np.random.choice(len(score), sampNum, p=score, replace=False)\n","            return pckNodes, max(sampNum - posNum, 0)\n","\n","        def constructData(usrs, itms):\n","            adjs = self.trnMats\n","            pckAdjs = []\n","            pckTpAdjs = []\n","            for i in range(len(adjs)):\n","                pckU = adjs[i][usrs]\n","                tpPckI = transpose(pckU)[itms]\n","                pckTpAdjs.append(tpPckI)\n","                pckAdjs.append(transpose(tpPckI))\n","            return pckAdjs, pckTpAdjs, usrs, itms\n","\n","        usrMask = makeMask(pckUsrs, adj.shape[0])\n","        itmMask = makeMask(pckItms, adj.shape[1])\n","        itmBdgt = updateBdgt(adj, pckUsrs)\n","        if pckItms is None:\n","            pckItms, _ = sample(itmBdgt, itmMask, len(pckUsrs))\n","            itmMask = itmMask * makeMask(pckItms, adj.shape[1])\n","        usrBdgt = updateBdgt(tpadj, pckItms)\n","        uSampRes = 0\n","        iSampRes = 0\n","        for i in range(sampDepth + 1):\n","            uSamp = uSampRes + (sampNum if i < sampDepth else 0)\n","            iSamp = iSampRes + (sampNum if i < sampDepth else 0)\n","            newUsrs, uSampRes = sample(usrBdgt, usrMask, uSamp)\n","            usrMask = usrMask * makeMask(newUsrs, adj.shape[0])\n","            newItms, iSampRes = sample(itmBdgt, itmMask, iSamp)\n","            itmMask = itmMask * makeMask(newItms, adj.shape[1])\n","            if i == sampDepth or i == sampDepth and uSampRes == 0 and iSampRes == 0:\n","                break\n","            usrBdgt += updateBdgt(tpadj, newItms)\n","            itmBdgt += updateBdgt(adj, newUsrs)\n","        usrs = np.reshape(np.argwhere(usrMask==0), [-1])\n","        itms = np.reshape(np.argwhere(itmMask==0), [-1])\n","        return constructData(usrs, itms)"],"metadata":{"id":"WJf_ck4htcBW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model"],"metadata":{"id":"cxWiPevg2o4x"}},{"cell_type":"code","source":["class Recommender:\n","\tdef __init__(self, sess, handler):\n","\t\tself.sess = sess\n","\t\tself.handler = handler\n","\n","\t\tprint('USER', args.user, 'ITEM', args.item)\n","\t\tself.metrics = dict()\n","\t\tmets = ['Loss', 'preLoss', 'HR', 'NDCG']\n","\t\tfor met in mets:\n","\t\t\tself.metrics['Train' + met] = list()\n","\t\t\tself.metrics['Test' + met] = list()\n","\n","\tdef makePrint(self, name, ep, reses, save):\n","\t\tret = 'Epoch %d/%d, %s: ' % (ep, args.epoch, name)\n","\t\tfor metric in reses:\n","\t\t\tval = reses[metric]\n","\t\t\tret += '%s = %.4f, ' % (metric, val)\n","\t\t\ttem = name + metric\n","\t\t\tif save and tem in self.metrics:\n","\t\t\t\tself.metrics[tem].append(val)\n","\t\tret = ret[:-2] + '  '\n","\t\treturn ret\n","\n","\tdef run(self):\n","\t\tself.prepareModel()\n","\t\tlog('Model Prepared')\n","\t\tif args.load_model != None:\n","\t\t\tself.loadModel()\n","\t\t\tstloc = len(self.metrics['TrainLoss']) * args.tstEpoch - (args.tstEpoch - 1)\n","\t\telse:\n","\t\t\tstloc = 0\n","\t\t\tinit = tf.global_variables_initializer()\n","\t\t\tself.sess.run(init)\n","\t\t\tlog('Variables Initiated')\n","\t\tfor ep in tqdm(range(stloc, args.epoch)):\n","\t\t\ttest = (ep % args.tstEpoch == 0)\n","\t\t\treses = self.trainEpoch()\n","\t\t\tlog(self.makePrint('Train', ep, reses, test))\n","\t\t\tif test:\n","\t\t\t\treses = self.testEpoch()\n","\t\t\t\tlog(self.makePrint('Test', ep, reses, test))\n","\t\t\tif ep % args.tstEpoch == 0:\n","\t\t\t\tself.saveHistory()\n","\t\t\tprint()\n","\t\treses = self.testEpoch()\n","\t\tlog(self.makePrint('Test', args.epoch, reses, True))\n","\t\tself.saveHistory()\n","\n","\tdef messagePropagate(self, lats, adj, lats2):\n","\t\treturn Activate(tf.sparse.sparse_dense_matmul(adj, lats), self.actFunc)\n","\n","\tdef metaForSpecialize(self, uEmbed, iEmbed, behEmbed, adjs, tpAdjs):\n","\t\tlatdim = args.latdim // 2\n","\t\trank = args.rank\n","\t\tassert len(adjs) == len(tpAdjs)\n","\t\tuNeighbor = iNeighbor = 0\n","\t\tfor i in range(len(adjs)):\n","\t\t\tuNeighbor += tf.sparse.sparse_dense_matmul(adjs[i], iEmbed)\n","\t\t\tiNeighbor += tf.sparse.sparse_dense_matmul(tpAdjs[i], uEmbed)\n","\t\tubehEmbed = tf.expand_dims(behEmbed, axis=0) * tf.ones_like(uEmbed)\n","\t\tibehEmbed = tf.expand_dims(behEmbed, axis=0) * tf.ones_like(iEmbed)\n","\t\tuMetaLat = FC(tf.concat([ubehEmbed, uEmbed, uNeighbor], axis=-1), latdim, useBias=True, activation=self.actFunc, reg=True, name='specMeta_FC1', reuse=True)\n","\t\tiMetaLat = FC(tf.concat([ibehEmbed, iEmbed, iNeighbor], axis=-1), latdim, useBias=True, activation=self.actFunc, reg=True, name='specMeta_FC1', reuse=True)\n","\t\tuW1 = tf.reshape(FC(uMetaLat, rank * latdim, useBias=True, reg=True, biasInitializer='xavier', biasReg=True, name='specMeta_FC2', reuse=True), [-1, latdim, rank])\n","\t\tuW2 = tf.reshape(FC(uMetaLat, rank * latdim, useBias=True, reg=True, biasInitializer='xavier', biasReg=True, name='specMeta_FC3', reuse=True), [-1, rank, latdim])\n","\t\tiW1 = tf.reshape(FC(iMetaLat, rank * latdim, useBias=True, reg=True, biasInitializer='xavier', biasReg=True, name='specMeta_FC4', reuse=True), [-1, latdim, rank])\n","\t\tiW2 = tf.reshape(FC(iMetaLat, rank * latdim, useBias=True, reg=True, biasInitializer='xavier', biasReg=True, name='specMeta_FC5', reuse=True), [-1, rank, latdim])\n","\n","\t\tparams = {'uW1': uW1, 'uW2': uW2, 'iW1': iW1, 'iW2': iW2}\n","\t\treturn params\n","\n","\tdef specialize(self, uEmbed, iEmbed, params):\n","\t\tretUEmbed = tf.reduce_sum(tf.expand_dims(uEmbed, axis=-1) * params['uW1'], axis=1)\n","\t\tretUEmbed = tf.reduce_sum(tf.expand_dims(retUEmbed, axis=-1) * params['uW2'], axis=1)\n","\t\tretUEmbed = tf.concat([retUEmbed, uEmbed], axis=-1)\n","\t\tretIEmbed = tf.reduce_sum(tf.expand_dims(iEmbed, axis=-1) * params['iW1'], axis=1)\n","\t\tretIEmbed = tf.reduce_sum(tf.expand_dims(retIEmbed, axis=-1) * params['iW2'], axis=1)\n","\t\tretIEmbed = tf.concat([retIEmbed, iEmbed], axis=-1)\n","\t\treturn retUEmbed, retIEmbed\n","\n","\tdef defineModel(self):\n","\t\tuEmbed0 = defineParam('uEmbed0', [args.user, args.latdim//2], reg=True)\n","\t\tiEmbed0 = defineParam('iEmbed0', [args.item, args.latdim//2], reg=True)\n","\t\tbehEmbeds = defineParam('behEmbeds', [args.behNum + 1, args.latdim//2])\n","\t\tself.ulat = [0] * (args.behNum + 1)\n","\t\tself.ilat = [0] * (args.behNum + 1)\n","\t\tfor beh in range(args.behNum):\n","\t\t\tparams = self.metaForSpecialize(uEmbed0, iEmbed0, behEmbeds[beh], [self.adjs[beh]], [self.tpAdjs[beh]])\n","\t\t\tbehUEmbed0, behIEmbed0 = self.specialize(uEmbed0, iEmbed0, params)\n","\t\t\t# behUEmbed0 = uEmbed0\n","\t\t\t# behIEmbed0 = iEmbed0\n","\t\t\tulats = [behUEmbed0]\n","\t\t\tilats = [behIEmbed0]\n","\t\t\tfor i in range(args.gnn_layer):\n","\t\t\t\tulat = self.messagePropagate(ilats[-1], self.adjs[beh], ulats[-1])\n","\t\t\t\tilat = self.messagePropagate(ulats[-1], self.tpAdjs[beh], ilats[-1])\n","\t\t\t\tulats.append(ulat + ulats[-1])\n","\t\t\t\tilats.append(ilat + ilats[-1])\n","\t\t\tself.ulat[beh] = tf.add_n(ulats)\n","\t\t\tself.ilat[beh] = tf.add_n(ilats)\n","\n","\t\tparams = self.metaForSpecialize(uEmbed0, iEmbed0, behEmbeds[-1], self.adjs, self.tpAdjs)\n","\t\tbehUEmbed0, behIEmbed0 = self.specialize(uEmbed0, iEmbed0, params)\n","\t\tulats = [behUEmbed0]\n","\t\tilats = [behIEmbed0]\n","\t\tfor i in range(args.gnn_layer):\n","\t\t\tubehLats = []\n","\t\t\tibehLats = []\n","\t\t\tfor beh in range(args.behNum):\n","\t\t\t\tulat = self.messagePropagate(ilats[-1], self.adjs[beh], ulats[-1])\n","\t\t\t\tilat = self.messagePropagate(ulats[-1], self.tpAdjs[beh], ilats[-1])\n","\t\t\t\tubehLats.append(ulat)\n","\t\t\t\tibehLats.append(ilat)\n","\t\t\tulat = tf.add_n(lightSelfAttention(ubehLats, args.behNum, args.latdim, args.att_head))\n","\t\t\tilat = tf.add_n(lightSelfAttention(ibehLats, args.behNum, args.latdim, args.att_head))\n","\t\t\tulats.append(ulat)\n","\t\t\tilats.append(ilat)\n","\t\tself.ulat[-1] = tf.add_n(ulats)\n","\t\tself.ilat[-1] = tf.add_n(ilats)\n","\n","\tdef metaForPredict(self, src_ulat, src_ilat, tgt_ulat, tgt_ilat):\n","\t\tlatdim = args.latdim\n","\t\tsrc_ui = FC(tf.concat([src_ulat * src_ilat, src_ulat, src_ilat], axis=-1), latdim, reg=True, useBias=True, activation=self.actFunc, name='predMeta_FC1', reuse=True)\n","\t\ttgt_ui = FC(tf.concat([tgt_ulat * tgt_ilat, tgt_ulat, tgt_ilat], axis=-1), latdim, reg=True, useBias=True, activation=self.actFunc, name='predMeta_FC1', reuse=True)\n","\t\tmetalat = FC(tf.concat([src_ui * tgt_ui, src_ui, tgt_ui], axis=-1), latdim * 3, reg=True, useBias=True, activation=self.actFunc, name='predMeta_FC2', reuse=True)\n","\t\tw1 = tf.reshape(FC(metalat, latdim * 3 * latdim, reg=True, useBias=True, name='predMeta_FC3', reuse=True, biasReg=True, biasInitializer='xavier'), [-1, latdim * 3, latdim])\n","\t\tb1 = tf.reshape(FC(metalat, latdim, reg=True, useBias=True, name='predMeta_FC4', reuse=True), [-1, 1, latdim])\n","\t\tw2 = tf.reshape(FC(metalat, latdim, reg=True, useBias=True, name='predMeta_FC5', reuse=True, biasReg=True,biasInitializer='xavier'), [-1, latdim, 1])\n","\n","\t\tparams = {\n","\t\t\t'w1': w1,\n","\t\t\t'b1': b1,\n","\t\t\t'w2': w2\n","\t\t}\n","\t\treturn params\n","\n","\tdef _predict(self, ulat, ilat, params):\n","\t\tpredEmbed = tf.expand_dims(tf.concat([ulat * ilat, ulat, ilat], axis=-1), axis=1)\n","\t\tpredEmbed = Activate(predEmbed @ params['w1'] + params['b1'], self.actFunc)\n","\t\tpreds = tf.squeeze(predEmbed @ params['w2'])\n","\t\treturn preds\n","\n","\tdef predict(self, src, tgt):\n","\t\tuids = self.uids[tgt]\n","\t\tiids = self.iids[tgt]\n","\n","\t\tsrc_ulat = tf.nn.embedding_lookup(self.ulat[src], uids)\n","\t\tsrc_ilat = tf.nn.embedding_lookup(self.ilat[src], iids)\n","\t\ttgt_ulat = tf.nn.embedding_lookup(self.ulat[tgt], uids)\n","\t\ttgt_ilat = tf.nn.embedding_lookup(self.ilat[tgt], iids)\n","\n","\t\tpredParams = self.metaForPredict(src_ulat, src_ilat, tgt_ulat, tgt_ilat)\n","\t\treturn self._predict(src_ulat, src_ilat, predParams) * args.mult\n","\n","\tdef prepareModel(self):\n","\t\tself.actFunc = 'leakyRelu'\n","\t\tself.adjs = []\n","\t\tself.tpAdjs = []\n","\t\tself.uids, self.iids = [], []\n","\t\tfor i in range(args.behNum):\n","\t\t\tadj = self.handler.trnMats[i]\n","\t\t\tidx, data, shape = transToLsts(adj, norm=True)\n","\t\t\tself.adjs.append(tf.sparse.SparseTensor(idx, data, shape))\n","\t\t\tidx, data, shape = transToLsts(transpose(adj), norm=True)\n","\t\t\tself.tpAdjs.append(tf.sparse.SparseTensor(idx, data, shape))\n","\t\t\tself.uids.append(tf.placeholder(name='uids'+str(i), dtype=tf.int32, shape=[None]))\n","\t\t\tself.iids.append(tf.placeholder(name='iids'+str(i), dtype=tf.int32, shape=[None]))\n","\t\t\n","\t\tself.defineModel()\n","\t\tself.preLoss = 0\n","\t\tfor src in range(args.behNum + 1):\n","\t\t\tfor tgt in range(args.behNum):\n","\t\t\t\tpreds = self.predict(src, tgt)\n","\t\t\t\tsampNum = tf.shape(self.uids[tgt])[0] // 2\n","\t\t\t\tposPred = tf.slice(preds, [0], [sampNum])\n","\t\t\t\tnegPred = tf.slice(preds, [sampNum], [-1])\n","\t\t\t\tself.preLoss += tf.reduce_mean(tf.maximum(0.0, 1.0 - (posPred - negPred)))\n","\t\t\t\tif src == args.behNum and tgt == args.behNum - 1:\n","\t\t\t\t\tself.targetPreds = preds\n","\t\tself.regLoss = args.reg * Regularize()\n","\t\tself.loss = self.preLoss + self.regLoss\n","\n","\t\tglobalStep = tf.Variable(0, trainable=False)\n","\t\tlearningRate = tf.train.exponential_decay(args.lr, globalStep, args.decay_step, args.decay, staircase=True)\n","\t\tself.optimizer = tf.train.AdamOptimizer(learningRate).minimize(self.loss, global_step=globalStep)\n","\n","\tdef sampleTrainBatch(self, batIds, labelMat):\n","\t\ttemLabel = labelMat[batIds].toarray()\n","\t\tbatch = len(batIds)\n","\t\ttemlen = batch * 2 * args.sampNum\n","\t\tuLocs = [None] * temlen\n","\t\tiLocs = [None] * temlen\n","\t\tcur = 0\n","\t\tfor i in range(batch):\n","\t\t\tposset = np.reshape(np.argwhere(temLabel[i]!=0), [-1])\n","\t\t\tsampNum = min(args.sampNum, len(posset))\n","\t\t\tif sampNum == 0:\n","\t\t\t\tposlocs = [np.random.choice(args.item)]\n","\t\t\t\tneglocs = [poslocs[0]]\n","\t\t\telse:\n","\t\t\t\tposlocs = np.random.choice(posset, sampNum)\n","\t\t\t\tneglocs = negSamp(temLabel[i], sampNum, args.item)\n","\t\t\tfor j in range(sampNum):\n","\t\t\t\tposloc = poslocs[j]\n","\t\t\t\tnegloc = neglocs[j]\n","\t\t\t\tuLocs[cur] = uLocs[cur+temlen//2] = batIds[i]\n","\t\t\t\tiLocs[cur] = posloc\n","\t\t\t\tiLocs[cur+temlen//2] = negloc\n","\t\t\t\tcur += 1\n","\t\tuLocs = uLocs[:cur] + uLocs[temlen//2: temlen//2 + cur]\n","\t\tiLocs = iLocs[:cur] + iLocs[temlen//2: temlen//2 + cur]\n","\t\treturn uLocs, iLocs\n","\n","\tdef trainEpoch(self):\n","\t\tnum = args.user\n","\t\tsfIds = np.random.permutation(num)[:args.trnNum]\n","\t\tepochLoss, epochPreLoss = [0] * 2\n","\t\tnum = len(sfIds)\n","\t\tsteps = int(np.ceil(num / args.batch))\n","\n","\t\tfor i in range(steps):\n","\t\t\tst = i * args.batch\n","\t\t\ted = min((i+1) * args.batch, num)\n","\t\t\tbatIds = sfIds[st: ed]\n","\n","\t\t\ttarget = [self.optimizer, self.preLoss, self.regLoss, self.loss]\n","\t\t\tfeed_dict = {}\n","\t\t\tfor beh in range(args.behNum):\n","\t\t\t\tuLocs, iLocs = self.sampleTrainBatch(batIds, self.handler.trnMats[beh])\n","\t\t\t\tfeed_dict[self.uids[beh]] = uLocs\n","\t\t\t\tfeed_dict[self.iids[beh]] = iLocs\n","\n","\t\t\tres = self.sess.run(target, feed_dict=feed_dict, options=config_pb2.RunOptions(report_tensor_allocations_upon_oom=True))\n","\n","\t\t\tpreLoss, regLoss, loss = res[1:]\n","\n","\t\t\tepochLoss += loss\n","\t\t\tepochPreLoss += preLoss\n","\t\t\tlog('Step %d/%d: loss = %.2f, regLoss = %.2f         ' % (i, steps, loss, regLoss), save=False, oneline=True)\n","\t\tret = dict()\n","\t\tret['Loss'] = epochLoss / steps\n","\t\tret['preLoss'] = epochPreLoss / steps\n","\t\treturn ret\n","\n","\tdef sampleTestBatch(self, batIds, labelMat):\n","\t\tbatch = len(batIds)\n","\t\ttemTst = self.handler.tstInt[batIds]\n","\t\ttemLabel = labelMat[batIds].toarray()\n","\t\ttemlen = batch * 100\n","\t\tuLocs = [None] * temlen\n","\t\tiLocs = [None] * temlen\n","\t\ttstLocs = [None] * batch\n","\t\tcur = 0\n","\t\tfor i in range(batch):\n","\t\t\tposloc = temTst[i]\n","\t\t\tnegset = np.reshape(np.argwhere(temLabel[i]==0), [-1])\n","\t\t\trdnNegSet = np.random.permutation(negset)[:99]\n","\t\t\tlocset = np.concatenate((rdnNegSet, np.array([posloc])))\n","\t\t\ttstLocs[i] = locset\n","\t\t\tfor j in range(100):\n","\t\t\t\tuLocs[cur] = batIds[i]\n","\t\t\t\tiLocs[cur] = locset[j]\n","\t\t\t\tcur += 1\n","\t\treturn uLocs, iLocs, temTst, tstLocs\n","\n","\tdef testEpoch(self):\n","\t\tepochHit, epochNdcg = [0] * 2\n","\t\tids = self.handler.tstUsrs\n","\t\tnum = len(ids)\n","\t\ttstBat = args.batch\n","\t\tsteps = int(np.ceil(num / tstBat))\n","\t\tfor i in range(steps):\n","\t\t\tst = i * tstBat\n","\t\t\ted = min((i+1) * tstBat, num)\n","\t\t\tbatIds = ids[st: ed]\n","\t\t\tfeed_dict = {}\n","\t\t\tuLocs, iLocs, temTst, tstLocs = self.sampleTestBatch(batIds, self.handler.trnMats[-1])\n","\t\t\tfeed_dict[self.uids[-1]] = uLocs\n","\t\t\tfeed_dict[self.iids[-1]] = iLocs\n","\t\t\tpreds = self.sess.run(self.targetPreds, feed_dict=feed_dict, options=config_pb2.RunOptions(report_tensor_allocations_upon_oom=True))\n","\t\t\thit, ndcg = self.calcRes(np.reshape(preds, [ed-st, 100]), temTst, tstLocs)\n","\t\t\tepochHit += hit\n","\t\t\tepochNdcg += ndcg\n","\t\t\tlog('Steps %d/%d: hit = %d, ndcg = %d          ' % (i, steps, hit, ndcg), save=False, oneline=True)\n","\t\tret = dict()\n","\t\tret['HR'] = epochHit / num\n","\t\tret['NDCG'] = epochNdcg / num\n","\t\treturn ret\n","\n","\tdef calcRes(self, preds, temTst, tstLocs):\n","\t\thit = 0\n","\t\tndcg = 0\n","\t\tfor j in range(preds.shape[0]):\n","\t\t\tpredvals = list(zip(preds[j], tstLocs[j]))\n","\t\t\tpredvals.sort(key=lambda x: x[0], reverse=True)\n","\t\t\tshoot = list(map(lambda x: x[1], predvals[:args.shoot]))\n","\t\t\tif temTst[j] in shoot:\n","\t\t\t\thit += 1\n","\t\t\t\tndcg += np.reciprocal(np.log2(shoot.index(temTst[j])+2))\n","\t\treturn hit, ndcg\n","\t\n","\tdef saveHistory(self):\n","\t\tif args.epoch == 0:\n","\t\t\treturn\n","\t\twith open('History/' + args.save_path + '.his', 'wb') as fs:\n","\t\t\tpickle.dump(self.metrics, fs)\n","\n","\t\tsaver = tf.train.Saver()\n","\t\tsaver.save(self.sess, 'Models/' + args.save_path)\n","\t\tlog('Model Saved: %s' % args.save_path)\n","\n","\tdef loadModel(self):\n","\t\tsaver = tf.train.Saver()\n","\t\tsaver.restore(sess, 'Models/' + args.load_model)\n","\t\twith open('History/' + args.load_model + '.his', 'rb') as fs:\n","\t\t\tself.metrics = pickle.load(fs)\n","\t\tlog('Model Loaded')\t"],"metadata":{"id":"TNDAiXW0tb-l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training & Evaluation"],"metadata":{"id":"VV1cEgWJ2q9L"}},{"cell_type":"code","source":["if __name__ == '__main__':\n","\tsaveDefault = True\n","\tconfig = tf.ConfigProto()\n","\tconfig.gpu_options.allow_growth = True\n","\n","\tlog('Start')\n","\thandler = DataHandler()\n","\thandler.LoadData()\n","\tlog('Load Data')\n","\n","\twith tf.Session(config=config) as sess:\n","\t\trecom = Recommender(sess, handler)\n","\t\trecom.run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":729,"referenced_widgets":["24c87c2912974b6d8becb5b9190aa829","4314716903e64982b95392e7e37156bd","6f654bcada5d41f78b08c66d2e692fde","9dd6ba2d32ff4c0cb76b467e51e8060c","7fd736fa68424784a086f63ba9a43657","0bbb945cc6d54d289cbf67827ae1b723","551b14c6f28644eea7c40845274e672c","02be78ca98ab4b39adaa14cfd532ce00","0343c7f1998946c68782b6fa7993b824","439b1cc5861d446b84eba9b023d4d410","1ceba6676a0f4736961546da9a54dac6"]},"id":"j9AC1tZouQWn","executionInfo":{"status":"ok","timestamp":1639298905239,"user_tz":-330,"elapsed":669639,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"051d89cd-13b8-45fe-ac02-ba61e5b5ccf8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2021-12-12 08:37:23.971121: Start\n","2021-12-12 08:37:27.161611: Load Data\n","USER 21716 ITEM 7977\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","2021-12-12 08:38:00.972851: Model Prepared\n","2021-12-12 08:38:01.834669: Variables Initiated\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"24c87c2912974b6d8becb5b9190aa829","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["2021-12-12 08:39:42.215211: Epoch 0/10, Train: Loss = 12.1183, preLoss = 6.3383  \n","2021-12-12 08:39:53.562941: Epoch 0/10, Test: HR = 0.5946, NDCG = 0.3332  \n","2021-12-12 08:39:55.301442: Model Saved: tem\n","\n","2021-12-12 08:40:47.575082: Epoch 1/10, Train: Loss = 9.0478, preLoss = 5.1342  \n","\n","2021-12-12 08:41:40.084938: Epoch 2/10, Train: Loss = 7.5280, preLoss = 4.5253  \n","\n","2021-12-12 08:42:32.611271: Epoch 3/10, Train: Loss = 6.5597, preLoss = 4.1519  \n","2021-12-12 08:42:42.480947: Epoch 3/10, Test: HR = 0.6345, NDCG = 0.3680  \n","2021-12-12 08:42:44.058778: Model Saved: tem\n","\n","2021-12-12 08:43:36.462615: Epoch 4/10, Train: Loss = 5.9555, preLoss = 3.9571  \n","\n","2021-12-12 08:44:29.043362: Epoch 5/10, Train: Loss = 5.5330, preLoss = 3.8189  \n","\n","2021-12-12 08:45:21.376325: Epoch 6/10, Train: Loss = 5.2156, preLoss = 3.7027  \n","2021-12-12 08:45:31.129196: Epoch 6/10, Test: HR = 0.6427, NDCG = 0.3792  \n","2021-12-12 08:45:32.804393: Model Saved: tem\n","\n","2021-12-12 08:46:25.139950: Epoch 7/10, Train: Loss = 4.9785, preLoss = 3.6121  \n","\n","2021-12-12 08:47:17.625253: Epoch 8/10, Train: Loss = 4.7693, preLoss = 3.5099  \n","\n","2021-12-12 08:48:10.046328: Epoch 9/10, Train: Loss = 4.6129, preLoss = 3.4353  \n","2021-12-12 08:48:19.891913: Epoch 9/10, Test: HR = 0.6497, NDCG = 0.3784  \n","2021-12-12 08:48:21.573201: Model Saved: tem\n","\n","2021-12-12 08:48:31.548992: Epoch 10/10, Test: HR = 0.6583, NDCG = 0.3822  \n","2021-12-12 08:48:33.469254: Model Saved: tem\n"]}]},{"cell_type":"markdown","source":["## References\n","\n","1. [https://github.com/RecoHut-Stanzas/S346877](https://github.com/RecoHut-Stanzas/S346877)\n","2. [https://arxiv.org/abs/2110.03969v1](https://arxiv.org/abs/2110.03969v1)\n","3. [https://github.com/akaxlh/MB-GMN](https://github.com/akaxlh/MB-GMN)"],"metadata":{"id":"cRuCzMKk5wAP"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"Z9_DARRQ1eUt"}},{"cell_type":"code","source":["!apt-get -qq install tree\n","!rm -r sample_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xNBE4KFf1jN9","executionInfo":{"status":"ok","timestamp":1639299049247,"user_tz":-330,"elapsed":6161,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"38813f0a-27d4-4fc0-fd65-822e0dc1678e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Selecting previously unselected package tree.\n","(Reading database ... 155222 files and directories currently installed.)\n","Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n","Unpacking tree (1.7.0-5) ...\n","Setting up tree (1.7.0-5) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"]}]},{"cell_type":"code","source":["!tree -h --du ."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_4QWNrMv1eUx","executionInfo":{"status":"ok","timestamp":1639299049978,"user_tz":-330,"elapsed":745,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"a4b90a2c-705d-43ff-abe2-10ff4c94ca1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[".\n","â”œâ”€â”€ [ 26M]  beibei\n","â”‚Â Â  â”œâ”€â”€ [ 233]  README.md\n","â”‚Â Â  â”œâ”€â”€ [2.2M]  trn_buy\n","â”‚Â Â  â”œâ”€â”€ [5.0M]  trn_cart\n","â”‚Â Â  â”œâ”€â”€ [ 18M]  trn_pv\n","â”‚Â Â  â””â”€â”€ [275K]  tst_int\n","â”œâ”€â”€ [4.6K]  History\n","â”‚Â Â  â””â”€â”€ [ 587]  tem.his\n","â””â”€â”€ [139M]  Models\n","    â”œâ”€â”€ [  63]  checkpoint\n","    â”œâ”€â”€ [9.2M]  tem.data-00000-of-00001\n","    â”œâ”€â”€ [2.6K]  tem.index\n","    â””â”€â”€ [129M]  tem.meta\n","\n"," 165M used in 3 directories, 10 files\n"]}]},{"cell_type":"code","source":["!pip install -q watermark\n","%reload_ext watermark\n","%watermark -a \"Sparsh A.\" -m -iv -u -t -d"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GKmE1saI1eUx","executionInfo":{"status":"ok","timestamp":1639299060337,"user_tz":-330,"elapsed":3910,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"c27afee4-57c1-40ef-f54c-d86fa7d30b6a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Author: Sparsh A.\n","\n","Last updated: 2021-12-12 08:51:08\n","\n","Compiler    : GCC 7.5.0\n","OS          : Linux\n","Release     : 5.4.104+\n","Machine     : x86_64\n","Processor   : x86_64\n","CPU cores   : 2\n","Architecture: 64bit\n","\n","tensorflow: 1.15.2\n","IPython   : 5.5.0\n","argparse  : 1.1\n","scipy     : 1.4.1\n","numpy     : 1.19.5\n","\n"]}]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"qvid872_1eUy"}},{"cell_type":"markdown","source":["**END**"],"metadata":{"id":"O0inFkFz1eUz"}}]}