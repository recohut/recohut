{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIyonrTKWvJK"
      },
      "outputs": [],
      "source": [
        "# default_exp models.layers.attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIHuvjtfWvJe"
      },
      "source": [
        "# Attention Layers\n",
        "> Implementation of Attention modules including Multihead attention etc.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQAdi7OrWvJq"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "from nbdev.showdoc import *\n",
        "from fastcore.nb_imports import *\n",
        "from fastcore.test import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_eIFYy_WvJv"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from recohut.utils.distances import wasserstein_distance_matmul"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TokenEmbedding"
      ],
      "metadata": {
        "id": "5q4rFiTuW7-7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vb7cz8BWvJy"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class TokenEmbedding(nn.Embedding):\n",
        "    def __init__(self, vocab_size, embed_size=512):\n",
        "        super().__init__(vocab_size, embed_size, padding_idx=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PositionalEmbedding"
      ],
      "metadata": {
        "id": "OTxHFn4hW9LO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnh9-xIpWvJ5"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, max_len, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.pe = nn.Embedding(max_len+1, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        pose = (x > 0) * (x > 0).sum(dim=-1).unsqueeze(1).repeat(1, x.size(-1))\n",
        "        pose += torch.arange(start=-(x.size(1)-1), end=1, step=1, device=x.device)\n",
        "        pose = pose * (x > 0)\n",
        "\n",
        "        return self.pe(pose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6pnBHyUWvJ8"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class GELU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP_j6wj3WvKA"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.activation = GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.activation(self.w_1(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfAVopztWvKD"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(features))\n",
        "        self.bias = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.weight * (x - mean) / (std + self.eps) + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcQc2nQDWvKG"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"layer norm and dropout (dropout and then layer norm)\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super().__init__()\n",
        "        self.layer_norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        # return x + self.dropout(sublayer(self.norm(x)))  # original implementation\n",
        "        return self.layer_norm(x + self.dropout(sublayer(x)))  # BERT4Rec implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention"
      ],
      "metadata": {
        "id": "vpp6ivErW_yC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8bLw3EhWvKL"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class Attention(nn.Module):\n",
        "    def forward(self, query, key, value, mask=None, dropout=None, sas=False):\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "            / math.sqrt(query.size(-1))\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        if sas:\n",
        "            direction_mask = torch.ones_like(scores)\n",
        "            direction_mask = torch.tril(direction_mask)\n",
        "            scores = scores.masked_fill(direction_mask == 0, -1e9)\n",
        "\n",
        "        p_attn = F.softmax(scores, dim=-1)\n",
        "\n",
        "        if dropout is not None:\n",
        "            p_attn = dropout(p_attn)\n",
        "\n",
        "        return torch.matmul(p_attn, value), p_attn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ScaledDotProductAttention"
      ],
      "metadata": {
        "id": "xD2Xw6CBXA4y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSb3oltcWvKN"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    \"\"\" Scaled Dot-Product Attention \n",
        "    Ref: https://zhuanlan.zhihu.com/p/47812375\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout_rate=0.):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.dropout = None\n",
        "        if dropout_rate > 0:\n",
        "            self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.softmax = nn.Softmax(dim=2)\n",
        "\n",
        "    def forward(self, W_q, W_k, W_v, scale=None, mask=None):\n",
        "        attention = torch.bmm(W_q, W_k.transpose(1, 2))\n",
        "        if scale:\n",
        "            attention = attention / scale\n",
        "        if mask:\n",
        "            attention = attention.masked_fill_(mask, -np.inf)\n",
        "        attention = self.softmax(attention)\n",
        "        if self.dropout is not None:\n",
        "            attention = self.dropout(attention)\n",
        "        output = torch.bmm(attention, W_v)\n",
        "        return output, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultiHeadAttention"
      ],
      "metadata": {
        "id": "TMng4XeRXCPN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7arMcQaFWvKP"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Multi-head attention module \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, attention_dim=None, num_heads=1, dropout_rate=0., \n",
        "                 use_residual=True, use_scale=False, layer_norm=False, align_to=\"input\"):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        if attention_dim is None:\n",
        "            attention_dim = input_dim // num_heads\n",
        "        self.attention_dim = attention_dim\n",
        "        self.output_dim = num_heads * attention_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.use_residual = use_residual\n",
        "        self.align_to = align_to\n",
        "        self.scale = attention_dim ** 0.5 if use_scale else None\n",
        "        self.W_q = nn.Linear(input_dim, self.output_dim, bias=False)\n",
        "        self.W_k = nn.Linear(input_dim, self.output_dim, bias=False)\n",
        "        self.W_v = nn.Linear(input_dim, self.output_dim, bias=False)\n",
        "        if input_dim != self.output_dim:\n",
        "            if align_to == \"output\":\n",
        "                self.W_res = nn.Linear(input_dim, self.output_dim, bias=False)\n",
        "            elif align_to == \"input\":\n",
        "                self.W_res = nn.Linear(self.output_dim, input_dim, bias=False)\n",
        "        else:\n",
        "            self.W_res = None\n",
        "        self.dot_product_attention = ScaledDotProductAttention(dropout_rate)\n",
        "        self.layer_norm = nn.LayerNorm(self.output_dim) if layer_norm else None\n",
        "        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        residual = query\n",
        "        \n",
        "        # linear projection\n",
        "        query = self.W_q(query)\n",
        "        key = self.W_k(key)\n",
        "        value = self.W_v(value)\n",
        "        \n",
        "        # split by heads\n",
        "        batch_size = query.size(0)\n",
        "        query = query.view(batch_size * self.num_heads, -1, self.attention_dim)\n",
        "        key = key.view(batch_size * self.num_heads, -1, self.attention_dim)\n",
        "        value = value.view(batch_size * self.num_heads, -1, self.attention_dim)\n",
        "        if mask:\n",
        "            mask = mask.repeat(self.num_heads, 1, 1)\n",
        "        # scaled dot product attention\n",
        "        output, attention = self.dot_product_attention(query, key, value, self.scale, mask)\n",
        "        # concat heads\n",
        "        output = output.view(batch_size, -1, self.output_dim)\n",
        "        # final linear projection\n",
        "        if self.W_res is not None:\n",
        "            if self.align_to == \"output\": # AutoInt style\n",
        "                residual = self.W_res(residual)\n",
        "            elif self.align_to == \"input\": # Transformer stype\n",
        "                output = self.W_res(output)\n",
        "        if self.dropout is not None:\n",
        "            output = self.dropout(output)\n",
        "        if self.use_residual:\n",
        "            output = output + residual\n",
        "        if self.layer_norm is not None:\n",
        "            output = self.layer_norm(output)\n",
        "        output = output.relu()\n",
        "        return output, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultiHeadedAttention_v2"
      ],
      "metadata": {
        "id": "GJt9N695XDeo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nr4_GgcWvKR"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class MultiHeadedAttention_v2(nn.Module):\n",
        "    def __init__(self, h, d_model, head_size=None, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % h == 0\n",
        "\n",
        "        self.h = h\n",
        "        self.d_k = d_model // h\n",
        "        if head_size is not None:\n",
        "            self.head_size = head_size\n",
        "        else:\n",
        "            self.head_size = d_model // h\n",
        "\n",
        "        self.linear_layers = nn.ModuleList(\n",
        "            [nn.Linear(d_model, self.h * self.head_size) for _ in range(3)])\n",
        "        self.attention = Attention()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.output_linear = nn.Linear(self.h * self.head_size, d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # 1) do all the linear projections in batch from d_model => h x d_k\n",
        "        query, key, value = [l(x).view(batch_size, -1, self.h, self.head_size).transpose(1, 2)\n",
        "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
        "        \n",
        "        # 2) apply attention on all the projected vectors in batch.\n",
        "        x, attn = self.attention(\n",
        "            query, key, value, mask=mask, dropout=self.dropout)\n",
        "\n",
        "        # 3) \"concat\" using a view and apply a final linear.\n",
        "        x = x.transpose(1, 2).contiguous().view(\n",
        "            batch_size, -1, self.h * self.head_size)\n",
        "        return self.output_linear(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultiHeadSelfAttention"
      ],
      "metadata": {
        "id": "9BuHS0nEXEqP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzftgHZPWvKT"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class MultiHeadSelfAttention(MultiHeadAttention):\n",
        "    def forward(self, X):\n",
        "        output, attention = super(MultiHeadSelfAttention, self).forward(X, X, X)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TransformerBlock"
      ],
      "metadata": {
        "id": "YLicyGLRXF0V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVc0qCd2WvKU"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, hidden, attn_heads, head_size, feed_forward_hidden, dropout, attn_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadedAttention(\n",
        "            h=attn_heads, d_model=hidden, head_size=head_size, dropout=attn_dropout)\n",
        "        self.feed_forward = PositionwiseFeedForward(\n",
        "            d_model=hidden, d_ff=feed_forward_hidden)\n",
        "        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
        "        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.input_sublayer(\n",
        "            x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n",
        "        x = self.output_sublayer(x, self.feed_forward)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SqueezeExcitationLayer"
      ],
      "metadata": {
        "id": "4adyuai2XHLl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvNpM4RiWvKV"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class SqueezeExcitationLayer(nn.Module):\n",
        "    def __init__(self, num_fields, reduction_ratio=3):\n",
        "        super(SqueezeExcitationLayer, self).__init__()\n",
        "        reduced_size = max(1, int(num_fields / reduction_ratio))\n",
        "        self.excitation = nn.Sequential(nn.Linear(num_fields, reduced_size, bias=False),\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Linear(reduced_size, num_fields, bias=False),\n",
        "                                        nn.ReLU())\n",
        "\n",
        "    def forward(self, feature_emb):\n",
        "        Z = torch.mean(feature_emb, dim=-1, out=None)\n",
        "        A = self.excitation(Z)\n",
        "        V = feature_emb * A.unsqueeze(-1)\n",
        "        return V"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SASMultiHeadedAttention"
      ],
      "metadata": {
        "id": "t2JgwNEDXINz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKwY_RbeWvKW"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class SASMultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, head_size=None, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % h == 0\n",
        "\n",
        "        self.h = h\n",
        "        self.d_k = d_model // h\n",
        "        if head_size is not None:\n",
        "            self.head_size = head_size\n",
        "        else:\n",
        "            self.head_size = d_model // h\n",
        "\n",
        "        self.linear_layers = nn.ModuleList(\n",
        "            [nn.Linear(d_model, self.h * self.head_size) for _ in range(3)])\n",
        "        self.attention = Attention()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.layer_norm = LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # 1) do all the linear projections in batch from d_model => h x d_k\n",
        "        query_, key_, value_ = [l(x).view(batch_size, -1, self.h, self.head_size).transpose(1, 2)\n",
        "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
        "        \n",
        "        # 2) apply attention on all the projected vectors in batch.\n",
        "        x, attn = self.attention(\n",
        "            query_, key_, value_, mask=mask, dropout=self.dropout, sas=True)\n",
        "\n",
        "        # 3) \"concat\" using a view and apply a final linear.\n",
        "        x = x.transpose(1, 2).contiguous().view(\n",
        "            batch_size, -1, self.h * self.head_size)\n",
        "        \n",
        "        return self.layer_norm(x + query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uD9GLFu4WvKZ"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class SASPositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
        "        self.layer_norm = LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_ = self.dropout(self.activation(self.conv1(x.permute(0, 2, 1))))\n",
        "        return self.layer_norm(self.dropout(self.conv2(x_)).permute(0, 2, 1) + x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SASTransformerBlock"
      ],
      "metadata": {
        "id": "g8V2W9pWXKXX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMtVFgB6WvKb"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class SASTransformerBlock(nn.Module):\n",
        "    def __init__(self, hidden, attn_heads, head_size, feed_forward_hidden, dropout, attn_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layer_norm = LayerNorm(hidden)\n",
        "        self.attention = SASMultiHeadedAttention(\n",
        "            h=attn_heads, d_model=hidden, head_size=head_size, dropout=attn_dropout)\n",
        "        self.feed_forward = SASPositionwiseFeedForward(\n",
        "            d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.attention(self.layer_norm(x), x, x, mask)\n",
        "        x = self.feed_forward(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SelfAttention"
      ],
      "metadata": {
        "id": "u8pfW9O1Wlzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    References:\n",
        "        1. https://github.com/RecoHut-Stanzas/STOSA/blob/ee14e2eabcc60922eb52cc7d3231df4954d9ff16/modules.py#L127\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 hidden_size,\n",
        "                 num_attention_heads,\n",
        "                 attention_probs_dropout_prob,\n",
        "                 hidden_dropout_prob):\n",
        "        super().__init__()\n",
        "        if hidden_size % num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.attention_head_size = int(hidden_size / num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(hidden_size, self.all_head_size)\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(attention_probs_dropout_prob)\n",
        "\n",
        "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
        "        self.layernorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
        "        self.out_dropout = nn.Dropout(hidden_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, input_tensor, attention_mask):\n",
        "        mixed_query_layer = self.query(input_tensor)\n",
        "        mixed_key_layer = self.key(input_tensor)\n",
        "        mixed_value_layer = self.value(input_tensor)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.attn_dropout(attention_probs)\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        hidden_states = self.dense(context_layer)\n",
        "        hidden_states = self.out_dropout(hidden_states)\n",
        "        hidden_states = self.layernorm(hidden_states + input_tensor)\n",
        "\n",
        "        return hidden_states, attention_probs"
      ],
      "metadata": {
        "id": "RjEv5UTzJZQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 4\n",
        "num_attention_heads = 2\n",
        "hidden_dropout_prob = 0.2\n",
        "attention_probs_dropout_prob = 0.2\n",
        "\n",
        "layer = SelfAttention(hidden_size, num_attention_heads, hidden_dropout_prob,\n",
        "                      attention_probs_dropout_prob)\n",
        "\n",
        "input_tensor = torch.rand((2,4,4))\n",
        "attention_mask = torch.rand((4,4))\n",
        "\n",
        "hidden_states = torch.round(layer.forward(input_tensor, attention_mask)[0].detach()*1e4)/1e4\n",
        "\n",
        "test_eq(hidden_states.shape.numel(), 32)\n",
        "test_eq(list(hidden_states.shape), [2, 4, 4])\n",
        "\n",
        "attention_probs = torch.round(layer.forward(input_tensor, attention_mask)[1].detach()*1e4)/1e4\n",
        "\n",
        "test_eq(attention_probs.shape.numel(), 64)\n",
        "test_eq(list(attention_probs.shape), [2, 2, 4, 4])"
      ],
      "metadata": {
        "id": "jTL5TnCAAalh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DistSelfAttention"
      ],
      "metadata": {
        "id": "k-30pa6OWnpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class DistSelfAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 hidden_size,\n",
        "                 num_attention_heads,\n",
        "                 hidden_dropout_prob,\n",
        "                 attention_probs_dropout_prob,\n",
        "                 distance_metric = 'wasserstein'):\n",
        "        super().__init__()\n",
        "        if hidden_size % num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.attention_head_size = int(hidden_size / num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.mean_query = nn.Linear(hidden_size, self.all_head_size)\n",
        "        self.cov_query = nn.Linear(hidden_size, self.all_head_size)\n",
        "        self.mean_key = nn.Linear(hidden_size, self.all_head_size)\n",
        "        self.cov_key = nn.Linear(hidden_size, self.all_head_size)\n",
        "        self.mean_value = nn.Linear(hidden_size, self.all_head_size)\n",
        "        self.cov_value = nn.Linear(hidden_size, self.all_head_size)\n",
        "\n",
        "        self.activation = nn.ELU()\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(attention_probs_dropout_prob)\n",
        "        self.mean_dense = nn.Linear(hidden_size, hidden_size)\n",
        "        self.cov_dense = nn.Linear(hidden_size, hidden_size)\n",
        "        self.out_dropout = nn.Dropout(hidden_dropout_prob)\n",
        "\n",
        "        self.distance_metric = distance_metric\n",
        "        self.layernorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
        "        \n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, input_mean_tensor, input_cov_tensor, attention_mask):\n",
        "        mixed_mean_query_layer = self.mean_query(input_mean_tensor)\n",
        "        mixed_mean_key_layer = self.mean_key(input_mean_tensor)\n",
        "        mixed_mean_value_layer = self.mean_value(input_mean_tensor)\n",
        "\n",
        "        mean_query_layer = self.transpose_for_scores(mixed_mean_query_layer)\n",
        "        mean_key_layer = self.transpose_for_scores(mixed_mean_key_layer)\n",
        "        mean_value_layer = self.transpose_for_scores(mixed_mean_value_layer)\n",
        "\n",
        "        mixed_cov_query_layer = self.activation(self.cov_query(input_cov_tensor)) + 1\n",
        "        mixed_cov_key_layer = self.activation(self.cov_key(input_cov_tensor)) + 1\n",
        "        mixed_cov_value_layer = self.activation(self.cov_value(input_cov_tensor)) + 1\n",
        "\n",
        "        cov_query_layer = self.transpose_for_scores(mixed_cov_query_layer)\n",
        "        cov_key_layer = self.transpose_for_scores(mixed_cov_key_layer)\n",
        "        cov_value_layer = self.transpose_for_scores(mixed_cov_value_layer)\n",
        "\n",
        "        if self.distance_metric == 'wasserstein':\n",
        "            attention_scores = -wasserstein_distance_matmul(mean_query_layer, cov_query_layer, mean_key_layer, cov_key_layer)\n",
        "        else:\n",
        "            attention_scores = -kl_distance_matmul(mean_query_layer, cov_query_layer, mean_key_layer, cov_key_layer)\n",
        "\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        attention_scores = attention_scores + attention_mask\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        attention_probs = self.attn_dropout(attention_probs)\n",
        "        mean_context_layer = torch.matmul(attention_probs, mean_value_layer)\n",
        "        cov_context_layer = torch.matmul(attention_probs ** 2, cov_value_layer)\n",
        "        mean_context_layer = mean_context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        cov_context_layer = cov_context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = mean_context_layer.size()[:-2] + (self.all_head_size,)\n",
        "\n",
        "        mean_context_layer = mean_context_layer.view(*new_context_layer_shape)\n",
        "        cov_context_layer = cov_context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "        mean_hidden_states = self.mean_dense(mean_context_layer)\n",
        "        mean_hidden_states = self.out_dropout(mean_hidden_states)\n",
        "        mean_hidden_states = self.layernorm(mean_hidden_states + input_mean_tensor)\n",
        "\n",
        "        cov_hidden_states = self.cov_dense(cov_context_layer)\n",
        "        cov_hidden_states = self.out_dropout(cov_hidden_states)\n",
        "        cov_hidden_states = self.layernorm(cov_hidden_states + input_cov_tensor)\n",
        "\n",
        "        return mean_hidden_states, cov_hidden_states, attention_probs"
      ],
      "metadata": {
        "id": "oesfym_GPw3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 4\n",
        "num_attention_heads = 2\n",
        "hidden_dropout_prob = 0.2\n",
        "attention_probs_dropout_prob = 0.2\n",
        "distance = 'wasserstein'\n",
        "\n",
        "layer = DistSelfAttention(hidden_size, num_attention_heads, hidden_dropout_prob,\n",
        "                          attention_probs_dropout_prob, distance)\n",
        "\n",
        "input_tensor = torch.rand((2,4,4))\n",
        "attention_mask = torch.rand((4,4))\n",
        "\n",
        "output = layer.forward(input_tensor, input_tensor, attention_mask)\n",
        "\n",
        "mean_hidden_states = torch.round(output[0].detach()*1e4)/1e4\n",
        "\n",
        "test_eq(mean_hidden_states.shape.numel(), 32)\n",
        "test_eq(list(mean_hidden_states.shape), [2, 4, 4])\n",
        "\n",
        "cov_hidden_states = torch.round(output[1].detach()*1e4)/1e4\n",
        "\n",
        "test_eq(cov_hidden_states.shape.numel(), 32)\n",
        "test_eq(list(cov_hidden_states.shape), [2, 4, 4])\n",
        "\n",
        "attention_probs = torch.round(output[2].detach()*1e4)/1e4\n",
        "\n",
        "test_eq(attention_probs.shape.numel(), 64)\n",
        "test_eq(list(attention_probs.shape), [2, 2, 4, 4])"
      ],
      "metadata": {
        "id": "W0r0us2WPfAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DistMeanSelfAttention"
      ],
      "metadata": {
        "id": "CDVeUjZeWpXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class DistMeanSelfAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 hidden_size,\n",
        "                 num_attention_heads,\n",
        "                 attention_probs_dropout_prob,\n",
        "                 hidden_dropout_prob):\n",
        "        super().__init__()\n",
        "        if hidden_size % num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.attention_head_size = int(hidden_size / num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.mean_query = nn.Linear(hidden_size, self.all_head_size)\n",
        "        self.mean_key = nn.Linear(hidden_size, self.all_head_size)\n",
        "        self.mean_value = nn.Linear(hidden_size, self.all_head_size)\n",
        "        self.cov_key = nn.Linear(hidden_size, self.all_head_size)\n",
        "        self.cov_query = nn.Linear(hidden_size, self.all_head_size)\n",
        "        self.cov_value = nn.Linear(hidden_size, self.all_head_size)\n",
        "\n",
        "        self.activation = nn.ELU()\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(attention_probs_dropout_prob)\n",
        "        self.mean_dense = nn.Linear(hidden_size, hidden_size)\n",
        "        self.cov_dense = nn.Linear(hidden_size, hidden_size)\n",
        "        self.out_dropout = nn.Dropout(hidden_dropout_prob)\n",
        "\n",
        "        self.layernorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, input_mean_tensor, input_cov_tensor, attention_mask):\n",
        "        mixed_mean_query_layer = self.mean_query(input_mean_tensor)\n",
        "        mixed_mean_key_layer = self.mean_key(input_mean_tensor)\n",
        "        mixed_mean_value_layer = self.mean_value(input_mean_tensor)\n",
        "\n",
        "        mean_query_layer = self.transpose_for_scores(mixed_mean_query_layer)\n",
        "        mean_key_layer = self.transpose_for_scores(mixed_mean_key_layer)\n",
        "        mean_value_layer = self.transpose_for_scores(mixed_mean_value_layer)\n",
        "\n",
        "        mixed_cov_query_layer = self.activation(self.cov_query(input_cov_tensor)) + 1\n",
        "        mixed_cov_key_layer = self.activation(self.cov_key(input_cov_tensor)) + 1\n",
        "        mixed_cov_value_layer = self.activation(self.cov_value(input_cov_tensor)) + 1\n",
        "\n",
        "        cov_query_layer = self.transpose_for_scores(mixed_cov_query_layer)\n",
        "        cov_key_layer = self.transpose_for_scores(mixed_cov_key_layer)\n",
        "        cov_value_layer = self.transpose_for_scores(mixed_cov_value_layer)\n",
        "\n",
        "        mean_attention_scores = torch.matmul(mean_query_layer, mean_key_layer.transpose(-1, -2))\n",
        "        cov_attention_scores = torch.matmul(cov_query_layer, cov_key_layer.transpose(-1, -2))\n",
        "\n",
        "        mean_attention_scores = mean_attention_scores / math.sqrt(self.attention_head_size)\n",
        "        mean_attention_scores = mean_attention_scores + attention_mask\n",
        "        mean_attention_probs = nn.Softmax(dim=-1)(mean_attention_scores)\n",
        "\n",
        "        cov_attention_scores = cov_attention_scores / math.sqrt(self.attention_head_size)\n",
        "        cov_attention_scores = cov_attention_scores + attention_mask\n",
        "        cov_attention_probs = nn.Softmax(dim=-1)(cov_attention_scores)\n",
        "\n",
        "        mean_attention_probs = self.attn_dropout(mean_attention_probs)\n",
        "        cov_attention_probs = self.attn_dropout(cov_attention_probs)\n",
        "        mean_context_layer = torch.matmul(mean_attention_probs, mean_value_layer)\n",
        "        cov_context_layer = torch.matmul(cov_attention_probs, cov_value_layer)\n",
        "        mean_context_layer = mean_context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        cov_context_layer = cov_context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = mean_context_layer.size()[:-2] + (self.all_head_size,)\n",
        "\n",
        "        mean_context_layer = mean_context_layer.view(*new_context_layer_shape)\n",
        "        cov_context_layer = cov_context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "        mean_hidden_states = self.mean_dense(mean_context_layer)\n",
        "        mean_hidden_states = self.out_dropout(mean_hidden_states)\n",
        "        mean_hidden_states = self.layernorm(mean_hidden_states + input_mean_tensor)\n",
        "\n",
        "        cov_hidden_states = self.cov_dense(cov_context_layer)\n",
        "        cov_hidden_states = self.out_dropout(cov_hidden_states)\n",
        "        cov_hidden_states = self.layernorm(cov_hidden_states + input_cov_tensor)\n",
        "\n",
        "        return mean_hidden_states, cov_hidden_states, mean_attention_probs"
      ],
      "metadata": {
        "id": "xKedDKtG_fXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 4\n",
        "num_attention_heads = 2\n",
        "hidden_dropout_prob = 0.2\n",
        "attention_probs_dropout_prob = 0.2\n",
        "\n",
        "layer = DistMeanSelfAttention(hidden_size, num_attention_heads, hidden_dropout_prob,\n",
        "                          attention_probs_dropout_prob)\n",
        "\n",
        "input_tensor = torch.rand((2,4,4))\n",
        "attention_mask = torch.rand((4,4))\n",
        "\n",
        "output = layer.forward(input_tensor, input_tensor, attention_mask)\n",
        "\n",
        "mean_hidden_states = torch.round(output[0].detach()*1e4)/1e4\n",
        "\n",
        "test_eq(mean_hidden_states.shape.numel(), 32)\n",
        "test_eq(list(mean_hidden_states.shape), [2, 4, 4])\n",
        "\n",
        "cov_hidden_states = torch.round(output[1].detach()*1e4)/1e4\n",
        "\n",
        "test_eq(cov_hidden_states.shape.numel(), 32)\n",
        "test_eq(list(cov_hidden_states.shape), [2, 4, 4])\n",
        "\n",
        "attention_probs = torch.round(output[2].detach()*1e4)/1e4\n",
        "\n",
        "test_eq(attention_probs.shape.numel(), 64)\n",
        "test_eq(list(attention_probs.shape), [2, 2, 4, 4])"
      ],
      "metadata": {
        "id": "R5TRyifPWaC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WxA1xFhWvKd"
      },
      "source": [
        "> References\n",
        "1. https://github.com/sparsh-ai/stanza/blob/S714864/model/attention.py\n",
        "2. https://github.com/xue-pai/FuxiCTR/blob/main/fuxictr/pytorch/layers/attention.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXwRDjpKI65c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6fa4420-26ba-4534-a1b8-3d0b8c409f89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author: Sparsh A.\n",
            "\n",
            "Last updated: 2022-01-22 15:19:18\n",
            "\n",
            "recohut: 0.0.11\n",
            "\n",
            "Compiler    : GCC 7.5.0\n",
            "OS          : Linux\n",
            "Release     : 5.4.144+\n",
            "Machine     : x86_64\n",
            "Processor   : x86_64\n",
            "CPU cores   : 2\n",
            "Architecture: 64bit\n",
            "\n",
            "IPython: 5.5.0\n",
            "torch  : 1.10.0+cu111\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#hide\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Sparsh A.\" -m -iv -u -t -d -p recohut"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "name": "models.layers.attention.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}