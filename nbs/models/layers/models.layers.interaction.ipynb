{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "models > layers > interaction",
      "provenance": [],
      "collapsed_sections": [
        "1KypvcFZI64_"
      ],
      "mount_file_id": "1FEZmnoLGIsTsGiK2gi1TsIHLAaWCXF_a",
      "authorship_tag": "ABX9TyMK6xSed9OZ1Mf7H87k4VRR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fycOO2OxKHEF"
      },
      "outputs": [],
      "source": [
        "# default_exp models.layers.interaction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpmQ2HGl-ecy"
      },
      "source": [
        "# Interaction Layers\n",
        "> Implementation of NN interaction layers in Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOcmGC-Y-ec7"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "from nbdev.showdoc import *\n",
        "from fastcore.nb_imports import *\n",
        "from fastcore.test import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLrLSvzp-ec8"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "import torch\n",
        "from torch import nn\n",
        "from itertools import combinations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exporti\n",
        "def get_activation(activation):\n",
        "    if isinstance(activation, str):\n",
        "        if activation.lower() == \"relu\":\n",
        "            return nn.ReLU()\n",
        "        elif activation.lower() == \"sigmoid\":\n",
        "            return nn.Sigmoid()\n",
        "        elif activation.lower() == \"tanh\":\n",
        "            return nn.Tanh()\n",
        "        else:\n",
        "            return getattr(nn, activation)()\n",
        "    else:\n",
        "        return activation"
      ],
      "metadata": {
        "id": "csDuSc-TCI_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class InnerProductLayer(nn.Module):\n",
        "    \"\"\" output: product_sum_pooling (bs x 1), \n",
        "                Bi_interaction_pooling (bs * dim), \n",
        "                inner_product (bs x f2/2), \n",
        "                elementwise_product (bs x f2/2 x emb_dim)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_fields=None, output=\"product_sum_pooling\"):\n",
        "        super(InnerProductLayer, self).__init__()\n",
        "        self._output_type = output\n",
        "        if output not in [\"product_sum_pooling\", \"Bi_interaction_pooling\", \"inner_product\", \"elementwise_product\"]:\n",
        "            raise ValueError(\"InnerProductLayer output={} is not supported.\".format(output))\n",
        "        if num_fields is None:\n",
        "            if output in [\"inner_product\", \"elementwise_product\"]:\n",
        "                raise ValueError(\"num_fields is required when InnerProductLayer output={}.\".format(output))\n",
        "        else:\n",
        "            p, q = zip(*list(combinations(range(num_fields), 2)))\n",
        "            self.field_p = nn.Parameter(torch.LongTensor(p), requires_grad=False)\n",
        "            self.field_q = nn.Parameter(torch.LongTensor(q), requires_grad=False)\n",
        "            self.interaction_units = int(num_fields * (num_fields - 1) / 2)\n",
        "            self.upper_triange_mask = nn.Parameter(torch.triu(torch.ones(num_fields, num_fields), 1).type(torch.ByteTensor),\n",
        "                                                   requires_grad=False)\n",
        "\n",
        "    def forward(self, feature_emb):\n",
        "        if self._output_type in [\"product_sum_pooling\", \"Bi_interaction_pooling\"]: \n",
        "            sum_of_square = torch.sum(feature_emb, dim=1) ** 2  # sum then square\n",
        "            square_of_sum = torch.sum(feature_emb ** 2, dim=1) # square then sum\n",
        "            bi_interaction = (sum_of_square - square_of_sum) * 0.5\n",
        "            if self._output_type == \"Bi_interaction_pooling\":\n",
        "                return bi_interaction\n",
        "            else:\n",
        "                return bi_interaction.sum(dim=-1, keepdim=True)\n",
        "        elif self._output_type == \"elementwise_product\":\n",
        "            emb1 =  torch.index_select(feature_emb, 1, self.field_p)\n",
        "            emb2 = torch.index_select(feature_emb, 1, self.field_q)\n",
        "            return emb1 * emb2\n",
        "        elif self._output_type == \"inner_product\":\n",
        "            inner_product_matrix = torch.bmm(feature_emb, feature_emb.transpose(1, 2))\n",
        "            flat_upper_triange = torch.masked_select(inner_product_matrix, self.upper_triange_mask)\n",
        "            return flat_upper_triange.view(-1, self.interaction_units)"
      ],
      "metadata": {
        "id": "2L3OyRif-ec8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class BilinearInteractionLayer(nn.Module):\n",
        "    def __init__(self, num_fields, embedding_dim, bilinear_type=\"field_interaction\"):\n",
        "        super(BilinearInteractionLayer, self).__init__()\n",
        "        self.bilinear_type = bilinear_type\n",
        "        if self.bilinear_type == \"field_all\":\n",
        "            self.bilinear_layer = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "        elif self.bilinear_type == \"field_each\":\n",
        "            self.bilinear_layer = nn.ModuleList([nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "                                                 for i in range(num_fields)])\n",
        "        elif self.bilinear_type == \"field_interaction\":\n",
        "            self.bilinear_layer = nn.ModuleList([nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "                                                 for i, j in combinations(range(num_fields), 2)])\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "    def forward(self, feature_emb):\n",
        "        feature_emb_list = torch.split(feature_emb, 1, dim=1)\n",
        "        if self.bilinear_type == \"field_all\":\n",
        "            bilinear_list = [self.bilinear_layer(v_i) * v_j\n",
        "                             for v_i, v_j in combinations(feature_emb_list, 2)]\n",
        "        elif self.bilinear_type == \"field_each\":\n",
        "            bilinear_list = [self.bilinear_layer[i](feature_emb_list[i]) * feature_emb_list[j]\n",
        "                             for i, j in combinations(range(len(feature_emb_list)), 2)]\n",
        "        elif self.bilinear_type == \"field_interaction\":\n",
        "            bilinear_list = [self.bilinear_layer[i](v[0]) * v[1]\n",
        "                             for i, v in enumerate(combinations(feature_emb_list, 2))]\n",
        "        return torch.cat(bilinear_list, dim=1)"
      ],
      "metadata": {
        "id": "GU7ouRlxCzM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class HolographicInteractionLayer(nn.Module):\n",
        "    def __init__(self, num_fields, interaction_type=\"circular_convolution\"):\n",
        "        super(HolographicInteractionLayer, self).__init__()\n",
        "        self.interaction_type = interaction_type\n",
        "        if self.interaction_type == \"circular_correlation\":\n",
        "            self.conj_sign =  nn.Parameter(torch.tensor([1., -1.]), requires_grad=False)\n",
        "        p, q = zip(*list(combinations(range(num_fields), 2)))\n",
        "        self.field_p = nn.Parameter(torch.LongTensor(p), requires_grad=False)\n",
        "        self.field_q = nn.Parameter(torch.LongTensor(q), requires_grad=False)\n",
        "\n",
        "    def forward(self, feature_emb):\n",
        "        emb1 =  torch.index_select(feature_emb, 1, self.field_p)\n",
        "        emb2 = torch.index_select(feature_emb, 1, self.field_q)\n",
        "        if self.interaction_type == \"hadamard_product\":\n",
        "            interact_tensor = emb1 * emb2\n",
        "        elif self.interaction_type == \"circular_convolution\":\n",
        "            fft1 = torch.rfft(emb1, 1, onesided=False)\n",
        "            fft2 = torch.rfft(emb2, 1, onesided=False)\n",
        "            fft_product = torch.stack([fft1[..., 0] * fft2[..., 0] - fft1[..., 1] * fft2[..., 1], \n",
        "                                       fft1[..., 0] * fft2[..., 1] + fft1[..., 1] * fft2[..., 0]], \n",
        "                                       dim=-1)\n",
        "            interact_tensor = torch.irfft(fft_product, 1, onesided=False)\n",
        "        elif self.interaction_type == \"circular_correlation\":\n",
        "            fft1_emb = torch.rfft(emb1, 1, onesided=False) \n",
        "            fft1 = fft1_emb * self.conj_sign.expand_as(fft1_emb)\n",
        "            fft2 = torch.rfft(emb2, 1, onesided=False)\n",
        "            fft_product = torch.stack([fft1[..., 0] * fft2[..., 0] - fft1[..., 1] * fft2[..., 1], \n",
        "                                       fft1[..., 0] * fft2[..., 1] + fft1[..., 1] * fft2[..., 0]], \n",
        "                                       dim=-1)\n",
        "            interact_tensor = torch.irfft(fft_product, 1, onesided=False)\n",
        "        else:\n",
        "            raise ValueError(\"interaction_type={} not supported.\".format(self.interaction_type))\n",
        "        return interact_tensor"
      ],
      "metadata": {
        "id": "2_g_sBngC1UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class CrossInteractionLayer(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(CrossInteractionLayer, self).__init__()\n",
        "        self.weight = nn.Linear(input_dim, 1, bias=False)\n",
        "        self.bias = nn.Parameter(torch.zeros(input_dim))\n",
        "\n",
        "    def forward(self, X_0, X_i):\n",
        "        interaction_out = self.weight(X_i) * X_0 + self.bias\n",
        "        return interaction_out"
      ],
      "metadata": {
        "id": "uV6yDNkHC36A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class CrossNet(nn.Module):\n",
        "    def __init__(self, input_dim, num_layers):\n",
        "        super(CrossNet, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.cross_net = nn.ModuleList(CrossInteractionLayer(input_dim)\n",
        "                                       for _ in range(self.num_layers))\n",
        "\n",
        "    def forward(self, X_0):\n",
        "        X_i = X_0 # b x dim\n",
        "        for i in range(self.num_layers):\n",
        "            X_i = X_i + self.cross_net[i](X_0, X_i)\n",
        "        return X_i"
      ],
      "metadata": {
        "id": "sHkE3IkMC6eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class CompressedInteractionNet(nn.Module):\n",
        "    def __init__(self, num_fields, cin_layer_units, output_dim=1):\n",
        "        super(CompressedInteractionNet, self).__init__()\n",
        "        self.cin_layer_units = cin_layer_units\n",
        "        self.fc = nn.Linear(sum(cin_layer_units), output_dim)\n",
        "        self.cin_layer = nn.ModuleDict()\n",
        "        for i, unit in enumerate(self.cin_layer_units):\n",
        "            in_channels = num_fields * self.cin_layer_units[i - 1] if i > 0 else num_fields ** 2\n",
        "            out_channels = unit\n",
        "            self.cin_layer[\"layer_\" + str(i + 1)] = nn.Conv1d(in_channels,\n",
        "                                                              out_channels,  # how many filters\n",
        "                                                              kernel_size=1) # kernel output shape\n",
        "\n",
        "    def forward(self, feature_emb):\n",
        "        pooling_outputs = []\n",
        "        X_0 = feature_emb\n",
        "        batch_size = X_0.shape[0]\n",
        "        embedding_dim = X_0.shape[-1]\n",
        "        X_i = X_0\n",
        "        for i in range(len(self.cin_layer_units)):\n",
        "            hadamard_tensor = torch.einsum(\"bhd,bmd->bhmd\", X_0, X_i)\n",
        "            hadamard_tensor = hadamard_tensor.view(batch_size, -1, embedding_dim)\n",
        "            X_i = self.cin_layer[\"layer_\" + str(i + 1)](hadamard_tensor) \\\n",
        "                      .view(batch_size, -1, embedding_dim)\n",
        "            pooling_outputs.append(X_i.sum(dim=-1))\n",
        "        concate_vec = torch.cat(pooling_outputs, dim=-1)\n",
        "        output = self.fc(concate_vec)\n",
        "        return output"
      ],
      "metadata": {
        "id": "CPbItJL1C8ED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class InteractionMachine(nn.Module):\n",
        "    def __init__(self, embedding_dim, order=2, batch_norm=False):\n",
        "        super(InteractionMachine, self).__init__()\n",
        "        assert order < 6, \"order={} is not supported.\".format(order)\n",
        "        self.order = order\n",
        "        self.bn = nn.BatchNorm1d(embedding_dim * order) if batch_norm else None\n",
        "        self.fc = nn.Linear(order * embedding_dim, 1)\n",
        "        \n",
        "    def second_order(self, p1, p2):\n",
        "        return (p1.pow(2) - p2) / 2\n",
        "\n",
        "    def third_order(self, p1, p2, p3):\n",
        "        return (p1.pow(3) - 3 * p1 * p2 + 2 * p3) / 6\n",
        "\n",
        "    def fourth_order(self, p1, p2, p3, p4):\n",
        "        return (p1.pow(4) - 6 * p1.pow(2) * p2 + 3 * p2.pow(2)\n",
        "                + 8 * p1 * p3 - 6 * p4) / 24\n",
        "\n",
        "    def fifth_order(self, p1, p2, p3, p4, p5):\n",
        "        return (p1.pow(5) - 10 * p1.pow(3) * p2 + 20 * p1.pow(2) * p3 - 30 * p1 * p4\n",
        "                - 20 * p2 * p3 + 15 * p1 * p2.pow(2) + 24 * p5) / 120\n",
        "\n",
        "    def forward(self, X):\n",
        "        out = []\n",
        "        Q = X\n",
        "        if self.order >= 1:\n",
        "            p1 = Q.sum(dim=1)\n",
        "            out.append(p1)\n",
        "            if self.order >= 2:\n",
        "                Q = Q * X\n",
        "                p2 = Q.sum(dim=1)\n",
        "                out.append(self.second_order(p1, p2))\n",
        "                if self.order >= 3:\n",
        "                    Q = Q * X\n",
        "                    p3 = Q.sum(dim=1)\n",
        "                    out.append(self.third_order(p1, p2, p3))\n",
        "                    if self.order >= 4:\n",
        "                        Q = Q * X\n",
        "                        p4 = Q.sum(dim=1)\n",
        "                        out.append(self.fourth_order(p1, p2, p3, p4))\n",
        "                        if self.order == 5:\n",
        "                            Q = Q * X\n",
        "                            p5 = Q.sum(dim=1)\n",
        "                            out.append(self.fifth_order(p1, p2, p3, p4, p5))\n",
        "        out = torch.cat(out, dim=-1)\n",
        "        if self.bn is not None:\n",
        "            out = self.bn(out)\n",
        "        y = self.fc(out)\n",
        "        return y"
      ],
      "metadata": {
        "id": "wbLNkJcRC-Pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **References**\n",
        "> - https://github.com/xue-pai/FuxiCTR/blob/main/fuxictr/pytorch/layers"
      ],
      "metadata": {
        "id": "RIS5Oenn-ec9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19ca55bb-6e8c-4417-a4c5-c4e11a1edd0e",
        "id": "jLYDS7ap-ec9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author: Sparsh A.\n",
            "\n",
            "Last updated: 2022-01-11 12:07:54\n",
            "\n",
            "Compiler    : GCC 7.5.0\n",
            "OS          : Linux\n",
            "Release     : 5.4.144+\n",
            "Machine     : x86_64\n",
            "Processor   : x86_64\n",
            "CPU cores   : 2\n",
            "Architecture: 64bit\n",
            "\n",
            "torch     : 1.10.0+cu111\n",
            "numpy     : 1.19.5\n",
            "PIL       : 7.1.2\n",
            "IPython   : 5.5.0\n",
            "matplotlib: 3.2.2\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#hide\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Sparsh A.\" -m -iv -u -t -d"
      ]
    }
  ]
}