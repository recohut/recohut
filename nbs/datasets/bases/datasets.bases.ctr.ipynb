{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "datasets > bases > ctr",
      "provenance": [],
      "collapsed_sections": [
        "1KypvcFZI64_"
      ],
      "mount_file_id": "1FEZmnoLGIsTsGiK2gi1TsIHLAaWCXF_a",
      "authorship_tag": "ABX9TyPO1yV8V+OlWxevB37HbSSr"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fycOO2OxKHEF"
      },
      "outputs": [],
      "source": [
        "# default_exp datasets.bases.ctr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KypvcFZI64_"
      },
      "source": [
        "# CTR Dataset\n",
        "> Implementation of base modules for ctr dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGwuVx5oI65E"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "from nbdev.showdoc import *\n",
        "from fastcore.nb_imports import *\n",
        "from fastcore.test import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "from typing import Any, Iterable, List, Optional, Tuple, Union, Callable\n",
        "\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "import pickle\n",
        "import sklearn.preprocessing as sklearn_preprocess\n",
        "from collections import Counter, OrderedDict, defaultdict\n",
        "import io\n",
        "import os\n",
        "import logging\n",
        "import json\n",
        "from datetime import datetime, date\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from pytorch_lightning import LightningDataModule\n",
        "\n",
        "from recohut.datasets.bases.common import Dataset as BaseDataset\n",
        "from recohut.utils.common_utils import download_url"
      ],
      "metadata": {
        "id": "fJptZHOChOrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#exporti\n",
        "class Tokenizer(object):\n",
        "    def __init__(self, topk_words=None, na_value=None, min_freq=1, splitter=None, \n",
        "                 lower=False, oov_token=0, max_len=0, padding_type=\"pre\"):\n",
        "        self._topk_words = topk_words\n",
        "        self._na_value = na_value\n",
        "        self._min_freq = min_freq\n",
        "        self._lower = lower\n",
        "        self._splitter = splitter\n",
        "        self.oov_token = oov_token # use 0 for __OOV__\n",
        "        self.word_counts = Counter()\n",
        "        self.vocab = dict()\n",
        "        self.vocab_size = 0 # include oov and padding\n",
        "        self.max_len = max_len\n",
        "        self.padding_type = padding_type\n",
        "\n",
        "    def fit_on_texts(self, texts, use_padding=True):\n",
        "        tokens = list(texts)\n",
        "        if self._splitter is not None: # for sequence\n",
        "            text_splits = [text.split(self._splitter) for text in texts if not pd.isnull(text)]\n",
        "            if self.max_len == 0:\n",
        "                self.max_len = max(len(x) for x in text_splits)\n",
        "            tokens = list(itertools.chain(*text_splits))\n",
        "        if self._lower:\n",
        "            tokens = [tk.lower() for tk in tokens]\n",
        "        if self._na_value is not None:\n",
        "            tokens = [tk for tk in tokens if tk != self._na_value]\n",
        "        self.word_counts = Counter(tokens)\n",
        "        words = [token for token, count in self.word_counts.most_common() if count >= self._min_freq]\n",
        "        self.word_counts.clear() # empty the dict to save memory\n",
        "        if self._topk_words:\n",
        "            words = words[0:self._topk_words]\n",
        "        self.vocab = dict((token, idx) for idx, token in enumerate(words, 1 + self.oov_token))\n",
        "        self.vocab[\"__OOV__\"] = self.oov_token\n",
        "        if use_padding:\n",
        "            self.vocab[\"__PAD__\"] = len(words) + self.oov_token + 1 # use the last index for __PAD__\n",
        "        self.vocab_size = len(self.vocab) + self.oov_token\n",
        "\n",
        "    def encode_category(self, categories):\n",
        "        category_indices = [self.vocab.get(x, self.oov_token) for x in categories]\n",
        "        return np.array(category_indices)\n",
        "\n",
        "    def encode_sequence(self, texts):\n",
        "        sequence_list = []\n",
        "        for text in texts:\n",
        "            if pd.isnull(text) or text == '':\n",
        "                sequence_list.append([])\n",
        "            else:\n",
        "                sequence_list.append([self.vocab.get(x, self.oov_token) for x in text.split(self._splitter)])\n",
        "        sequence_list = self.padding(sequence_list, maxlen=self.max_len, value=self.vocab_size - 1,\n",
        "                                padding=self.padding_type, truncating=self.padding_type)\n",
        "        return np.array(sequence_list)\n",
        "    \n",
        "    def load_pretrained_embedding(self, feature_name, pretrain_path, embedding_dim, output_path):\n",
        "        with h5py.File(pretrain_path, 'r') as hf:\n",
        "            keys = hf[\"key\"][:]\n",
        "            pretrained_vocab = dict(zip(keys, range(len(keys))))\n",
        "            pretrained_emb = hf[\"value\"][:]\n",
        "        embedding_matrix = np.random.normal(loc=0, scale=1.e-4, size=(self.vocab_size, embedding_dim))\n",
        "        for word, idx in self.vocab.items():\n",
        "            if word in pretrained_vocab:\n",
        "                embedding_matrix[idx] = pretrained_emb[pretrained_vocab[word]]\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "        with h5py.File(output_path, 'a') as hf:\n",
        "            hf.create_dataset(feature_name, data=embedding_matrix)\n",
        "\n",
        "    def set_vocab(self, vocab):\n",
        "        self.vocab = vocab\n",
        "        self.vocab_size = len(self.vocab) + self.oov_token\n",
        "\n",
        "    @staticmethod\n",
        "    def padding(sequences, maxlen=None, dtype='int32',\n",
        "                padding='pre', truncating='pre', value=0.):\n",
        "        \"\"\" Pads sequences (list of list) to the ndarray of same length \"\"\"\n",
        "        assert padding in [\"pre\", \"post\"], \"Invalid padding={}.\".format(padding)\n",
        "        assert truncating in [\"pre\", \"post\"], \"Invalid truncating={}.\".format(truncating)\n",
        "        \n",
        "        if maxlen is None:\n",
        "            maxlen = max(len(x) for x in sequences)\n",
        "        arr = np.full((len(sequences), maxlen), value, dtype=dtype)\n",
        "        for idx, x in enumerate(sequences):\n",
        "            if len(x) == 0:\n",
        "                continue  # empty list\n",
        "            if truncating == 'pre':\n",
        "                trunc = x[-maxlen:]\n",
        "            else:\n",
        "                trunc = x[:maxlen]\n",
        "            trunc = np.asarray(trunc, dtype=dtype)\n",
        "\n",
        "            if padding == 'pre':\n",
        "                arr[idx, -len(trunc):] = trunc\n",
        "            else:\n",
        "                arr[idx, :len(trunc)] = trunc\n",
        "        return arr"
      ],
      "metadata": {
        "id": "Ea_20yl9U1dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#exporti\n",
        "class Normalizer(object):\n",
        "    def __init__(self, normalizer):\n",
        "        if not callable(normalizer):\n",
        "            self.callable = False\n",
        "            if normalizer in ['StandardScaler', 'MinMaxScaler']:\n",
        "                self.normalizer = getattr(sklearn_preprocess, normalizer)()\n",
        "            else:\n",
        "                raise NotImplementedError('normalizer={}'.format(normalizer))\n",
        "        else:\n",
        "            # normalizer is a method\n",
        "            self.normalizer = normalizer\n",
        "            self.callable = True\n",
        "\n",
        "    def fit(self, X):\n",
        "        if not self.callable:\n",
        "            null_index = np.isnan(X)\n",
        "            self.normalizer.fit(X[~null_index].reshape(-1, 1))\n",
        "\n",
        "    def normalize(self, X):\n",
        "        if self.callable:\n",
        "            return self.normalizer(X)\n",
        "        else:\n",
        "            return self.normalizer.transform(X.reshape(-1, 1)).flatten()"
      ],
      "metadata": {
        "id": "xXhv2rEkcbb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#exporti\n",
        "class FeatureMap(object):\n",
        "    def __init__(self, dataset_id='ctr'):\n",
        "        self.dataset_id = dataset_id\n",
        "        self.num_fields = 0\n",
        "        self.num_features = 0\n",
        "        self.feature_len = 0\n",
        "        self.feature_specs = OrderedDict()\n",
        "        \n",
        "    def set_feature_index(self):\n",
        "        logging.info(\"Set feature index...\")\n",
        "        idx = 0\n",
        "        for feature, feature_spec in self.feature_specs.items():\n",
        "            if feature_spec[\"type\"] != \"sequence\":\n",
        "                self.feature_specs[feature][\"index\"] = idx\n",
        "                idx += 1\n",
        "            else:\n",
        "                seq_indexes = [i + idx for i in range(feature_spec[\"max_len\"])]\n",
        "                self.feature_specs[feature][\"index\"] = seq_indexes\n",
        "                idx += feature_spec[\"max_len\"]\n",
        "        self.feature_len = idx\n",
        "\n",
        "    def get_feature_index(self, feature_type=None):\n",
        "        feature_indexes = []\n",
        "        if feature_type is not None:\n",
        "            if not isinstance(feature_type, list):\n",
        "                feature_type = [feature_type]\n",
        "            feature_indexes = [feature_spec[\"index\"] for feature, feature_spec in self.feature_specs.items()\n",
        "                               if feature_spec[\"type\"] in feature_type]\n",
        "        return feature_indexes\n",
        "\n",
        "    def load(self, json_file):\n",
        "        logging.info(\"Load feature_map from json: \" + json_file)\n",
        "        with io.open(json_file, \"r\", encoding=\"utf-8\") as fd:\n",
        "            feature_map = json.load(fd, object_pairs_hook=OrderedDict)\n",
        "        if feature_map[\"dataset_id\"] != self.dataset_id:\n",
        "            raise RuntimeError(\"dataset_id={} does not match to feature_map!\".format(self.dataset_id))\n",
        "        self.num_fields = feature_map[\"num_fields\"]\n",
        "        self.num_features = feature_map.get(\"num_features\", None)\n",
        "        self.feature_len = feature_map.get(\"feature_len\", None)\n",
        "        self.feature_specs = OrderedDict(feature_map[\"feature_specs\"])\n",
        "\n",
        "    def save(self, json_file):\n",
        "        logging.info(\"Save feature_map to json: \" + json_file)\n",
        "        if not os.path.exists(os.path.dirname(json_file)):\n",
        "            os.makedirs(os.path.dirname(json_file))\n",
        "        feature_map = OrderedDict()\n",
        "        feature_map[\"dataset_id\"] = self.dataset_id\n",
        "        feature_map[\"num_fields\"] = self.num_fields\n",
        "        feature_map[\"num_features\"] = self.num_features\n",
        "        feature_map[\"feature_len\"] = self.feature_len\n",
        "        feature_map[\"feature_specs\"] = self.feature_specs\n",
        "        with open(json_file, \"w\") as fd:\n",
        "            json.dump(feature_map, fd, indent=4)"
      ],
      "metadata": {
        "id": "8W9veKeAUwBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class CTRDataset(torch.utils.data.Dataset, BaseDataset):\n",
        "    feature_cols = list()\n",
        "    label_col = dict()\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_dir,\n",
        "                 data_type=None,\n",
        "                 *args,\n",
        "                 **kwargs):\n",
        "        super().__init__(data_dir)\n",
        "        self.data_type = data_type\n",
        "        self.pickle_file = os.path.join(self.processed_dir, \"feature_encoder.pkl\")\n",
        "        self.json_file = os.path.join(self.processed_dir, \"feature_map.json\")\n",
        "        self.feature_cols = self._complete_feature_cols(self.feature_cols)\n",
        "        self.feature_map = FeatureMap()\n",
        "        self.encoders = dict()\n",
        "\n",
        "        if self.data_type == 'train':\n",
        "            self.darray =  self.load_data(self.raw_paths[0])\n",
        "            self.num_samples = len(self.darray)\n",
        "        elif self.data_type == 'valid':\n",
        "            self.darray = self.load_data(self.raw_paths[1])\n",
        "            self.validation_samples = len(self.darray)\n",
        "        elif self.data_type == 'test':\n",
        "            self.darray = self.load_data(self.raw_paths[2])\n",
        "            self.test_samples = len(self.darray)\n",
        "        elif self.data_type is None:\n",
        "            self._process()\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['feature_encoder.pkl',\n",
        "                'feature_map.json',\n",
        "                'train_sample.h5',\n",
        "                'valid_sample.h5',\n",
        "                'test_sample.h5']\n",
        "\n",
        "    def download(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def process(self):\n",
        "        self.fit(self.raw_paths[0])\n",
        "\n",
        "    @staticmethod\n",
        "    def _complete_feature_cols(feature_cols):\n",
        "        full_feature_cols = []\n",
        "        for col in feature_cols:\n",
        "            name_or_namelist = col[\"name\"]\n",
        "            if isinstance(name_or_namelist, list):\n",
        "                for _name in name_or_namelist:\n",
        "                    _col = col.copy()\n",
        "                    _col[\"name\"] = _name\n",
        "                    full_feature_cols.append(_col)\n",
        "            else:\n",
        "                full_feature_cols.append(col)\n",
        "        return full_feature_cols\n",
        "\n",
        "    def read_csv(self, data_path):\n",
        "        all_cols = self.feature_cols + [self.label_col]\n",
        "        dtype_dict = dict((x[\"name\"], eval(x[\"dtype\"]) if isinstance(x[\"dtype\"], str) else x[\"dtype\"]) \n",
        "                          for x in all_cols)\n",
        "        ddf = pd.read_csv(data_path, dtype=dtype_dict, memory_map=True) \n",
        "        return ddf\n",
        "\n",
        "    def _preprocess(self, ddf):\n",
        "        all_cols = [self.label_col] + self.feature_cols[::-1]\n",
        "        for col in all_cols:\n",
        "            name = col[\"name\"]\n",
        "            if name in ddf.columns and ddf[name].isnull().values.any():\n",
        "                ddf[name] = self._fill_na(col, ddf[name])\n",
        "            if \"preprocess\" in col and col[\"preprocess\"] != \"\":\n",
        "                preprocess_fn = getattr(self, col[\"preprocess\"])\n",
        "                ddf[name] = preprocess_fn(ddf, name)\n",
        "        active_cols = [self.label_col[\"name\"]] + [col[\"name\"] for col in self.feature_cols if col[\"active\"]]\n",
        "        ddf = ddf.loc[:, active_cols]\n",
        "        return ddf\n",
        "\n",
        "    def _fill_na(self, col, series):\n",
        "        na_value = col.get(\"na_value\")\n",
        "        if na_value is not None:\n",
        "            return series.fillna(na_value)\n",
        "        elif col[\"dtype\"] == \"str\":\n",
        "            return series.fillna(\"\")\n",
        "        else:\n",
        "            raise RuntimeError(\"Feature column={} requires to assign na_value!\".format(col[\"name\"]))\n",
        "\n",
        "    def fit(self, train_data, min_categr_count=1, num_buckets=10):           \n",
        "        ddf = self.read_csv(train_data)\n",
        "        ddf = self._preprocess(ddf)\n",
        "        self.feature_map.num_fields = 0\n",
        "        for col in self.feature_cols:\n",
        "            if col[\"active\"]:\n",
        "                name = col[\"name\"]\n",
        "                self.fit_feature_col(col, ddf, \n",
        "                                     min_categr_count=min_categr_count,\n",
        "                                     num_buckets=num_buckets)\n",
        "                self.feature_map.num_fields += 1\n",
        "        self.feature_map.set_feature_index()\n",
        "        self.save_pickle(self.pickle_file)\n",
        "        self.feature_map.save(self.json_file)\n",
        "        \n",
        "    def fit_feature_col(self, feature_column, ddf, min_categr_count=1, num_buckets=10):\n",
        "        name = feature_column[\"name\"]\n",
        "        feature_type = feature_column[\"type\"]\n",
        "        feature_source = feature_column.get(\"source\", \"\")\n",
        "        self.feature_map.feature_specs[name] = {\"source\": feature_source,\n",
        "                                                \"type\": feature_type}\n",
        "        if \"min_categr_count\" in feature_column:\n",
        "            min_categr_count = feature_column[\"min_categr_count\"]\n",
        "        self.feature_map.feature_specs[name][\"min_categr_count\"] = min_categr_count\n",
        "        if \"embedding_dim\" in feature_column:\n",
        "            self.feature_map.feature_specs[name][\"embedding_dim\"] = feature_column[\"embedding_dim\"]\n",
        "        feature_values = ddf[name].values\n",
        "        if feature_type == \"numeric\":\n",
        "            normalizer_name = feature_column.get(\"normalizer\", None)\n",
        "            if normalizer_name is not None:\n",
        "                normalizer = Normalizer(normalizer_name)\n",
        "                normalizer.fit(feature_values)\n",
        "                self.encoders[name + \"_normalizer\"] = normalizer\n",
        "            self.feature_map.num_features += 1\n",
        "        elif feature_type == \"categorical\":\n",
        "            encoder = feature_column.get(\"encoder\", \"\")\n",
        "            if encoder != \"\":\n",
        "                self.feature_map.feature_specs[name][\"encoder\"] = encoder\n",
        "            if encoder == \"\":\n",
        "                tokenizer = Tokenizer(min_freq=min_categr_count, \n",
        "                                      na_value=feature_column.get(\"na_value\", \"\"))\n",
        "                if \"share_embedding\" in feature_column:\n",
        "                    self.feature_map.feature_specs[name][\"share_embedding\"] = feature_column[\"share_embedding\"]\n",
        "                    tokenizer.set_vocab(self.encoders[\"{}_tokenizer\".format(feature_column[\"share_embedding\"])].vocab)\n",
        "                else:\n",
        "                    if self.is_share_embedding_with_sequence(name):\n",
        "                        tokenizer.fit_on_texts(feature_values, use_padding=True)\n",
        "                        self.feature_map.feature_specs[name][\"padding_idx\"] = tokenizer.vocab_size - 1\n",
        "                    else:\n",
        "                        tokenizer.fit_on_texts(feature_values, use_padding=False)\n",
        "                self.encoders[name + \"_tokenizer\"] = tokenizer\n",
        "                self.feature_map.num_features += tokenizer.vocab_size\n",
        "                self.feature_map.feature_specs[name][\"vocab_size\"] = tokenizer.vocab_size\n",
        "                if \"pretrained_emb\" in feature_column:\n",
        "                    self.feature_map.feature_specs[name][\"pretrained_emb\"] = \"pretrained_embedding.h5\"\n",
        "                    self.feature_map.feature_specs[name][\"freeze_emb\"] = feature_column.get(\"freeze_emb\", True)\n",
        "                    tokenizer.load_pretrained_embedding(name,\n",
        "                                                        feature_column[\"pretrained_emb\"], \n",
        "                                                        feature_column[\"embedding_dim\"],\n",
        "                                                        os.path.join(self.processed_dir, \"pretrained_embedding.h5\"))\n",
        "            elif encoder == \"numeric_bucket\":\n",
        "                num_buckets = feature_column.get(\"num_buckets\", num_buckets)\n",
        "                qtf = sklearn_preprocess.QuantileTransformer(n_quantiles=num_buckets + 1)\n",
        "                qtf.fit(feature_values)\n",
        "                boundaries = qtf.quantiles_[1:-1]\n",
        "                self.feature_map.feature_specs[name][\"vocab_size\"] = num_buckets\n",
        "                self.feature_map.num_features += num_buckets\n",
        "                self.encoders[name + \"_boundaries\"] = boundaries\n",
        "            elif encoder == \"hash_bucket\":\n",
        "                num_buckets = feature_column.get(\"num_buckets\", num_buckets)\n",
        "                uniques = Counter(feature_values)\n",
        "                num_buckets = min(num_buckets, len(uniques))\n",
        "                self.feature_map.feature_specs[name][\"vocab_size\"] = num_buckets\n",
        "                self.feature_map.num_features += num_buckets\n",
        "                self.encoders[name + \"_num_buckets\"] = num_buckets\n",
        "        elif feature_type == \"sequence\":\n",
        "            encoder = feature_column.get(\"encoder\", \"MaskedAveragePooling\")\n",
        "            splitter = feature_column.get(\"splitter\", \" \")\n",
        "            na_value = feature_column.get(\"na_value\", \"\")\n",
        "            max_len = feature_column.get(\"max_len\", 0)\n",
        "            padding = feature_column.get(\"padding\", \"post\")\n",
        "            tokenizer = Tokenizer(min_freq=min_categr_count, splitter=splitter, \n",
        "                                  na_value=na_value, max_len=max_len, padding=padding)\n",
        "            if \"share_embedding\" in feature_column:\n",
        "                self.feature_map.feature_specs[name][\"share_embedding\"] = feature_column[\"share_embedding\"]\n",
        "                tokenizer.set_vocab(self.encoders[\"{}_tokenizer\".format(feature_column[\"share_embedding\"])].vocab)\n",
        "            else:\n",
        "                tokenizer.fit_on_texts(feature_values, use_padding=True)\n",
        "            self.encoders[name + \"_tokenizer\"] = tokenizer\n",
        "            self.feature_map.num_features += tokenizer.vocab_size\n",
        "            self.feature_map.feature_specs[name].update({\"encoder\": encoder,\n",
        "                                                         \"padding_idx\": tokenizer.vocab_size - 1,\n",
        "                                                         \"vocab_size\": tokenizer.vocab_size,\n",
        "                                                         \"max_len\": tokenizer.max_len})\n",
        "            if \"pretrained_emb\" in feature_column:\n",
        "                self.feature_map.feature_specs[name][\"pretrained_emb\"] = \"pretrained_embedding.h5\"\n",
        "                self.feature_map.feature_specs[name][\"freeze_emb\"] = feature_column.get(\"freeze_emb\", True)\n",
        "                tokenizer.load_pretrained_embedding(name,\n",
        "                                                    feature_column[\"pretrained_emb\"], \n",
        "                                                    feature_column[\"embedding_dim\"],\n",
        "                                                    os.path.join(self.processed_dir, \"pretrained_embedding.h5\"))\n",
        "        else:\n",
        "            raise NotImplementedError(\"feature_col={}\".format(feature_column))\n",
        "\n",
        "    def transform(self, ddf):\n",
        "        ddf = self._preprocess(ddf)\n",
        "        data_arrays = []\n",
        "        for feature, feature_spec in self.feature_map.feature_specs.items():\n",
        "            feature_type = feature_spec[\"type\"]\n",
        "            if feature_type == \"numeric\":\n",
        "                numeric_array = ddf.loc[:, feature].fillna(0).apply(lambda x: float(x)).values\n",
        "                normalizer = self.encoders.get(feature + \"_normalizer\")\n",
        "                if normalizer:\n",
        "                     numeric_array = normalizer.normalize(numeric_array)\n",
        "                data_arrays.append(numeric_array) \n",
        "            elif feature_type == \"categorical\":\n",
        "                encoder = feature_spec.get(\"encoder\", \"\")\n",
        "                if encoder == \"\":\n",
        "                    data_arrays.append(self.encoders.get(feature + \"_tokenizer\") \\\n",
        "                                                    .encode_category(ddf.loc[:, feature].values))\n",
        "                elif encoder == \"numeric_bucket\":\n",
        "                    raise NotImplementedError\n",
        "                elif encoder == \"hash_bucket\":\n",
        "                    raise NotImplementedError\n",
        "            elif feature_type == \"sequence\":\n",
        "                data_arrays.append(self.encoders.get(feature + \"_tokenizer\") \\\n",
        "                                                .encode_sequence(ddf.loc[:, feature].values))\n",
        "        label_name = self.label_col[\"name\"]\n",
        "        if ddf[label_name].dtype != np.float64:\n",
        "            ddf.loc[:, label_name] = ddf.loc[:, label_name].apply(lambda x: float(x))\n",
        "        data_arrays.append(ddf.loc[:, label_name].values) # add the label column at last\n",
        "        data_arrays = [item.reshape(-1, 1) if item.ndim == 1 else item for item in data_arrays]\n",
        "        data_array = np.hstack(data_arrays)\n",
        "        return data_array\n",
        "\n",
        "    def is_share_embedding_with_sequence(self, feature):\n",
        "        for col in self.feature_cols:\n",
        "            if col.get(\"share_embedding\", None) == feature and col[\"type\"] == \"sequence\":\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def load_pickle(self, pickle_file=None):\n",
        "        return pickle.load(open(pickle_file, \"rb\"))\n",
        "\n",
        "    def save_pickle(self, pickle_file):\n",
        "        if not os.path.exists(os.path.dirname(pickle_file)):\n",
        "            os.makedirs(os.path.dirname(pickle_file))\n",
        "        pickle.dump(self.encoders, open(pickle_file, \"wb\"))\n",
        "        \n",
        "    def load_json(self, json_file):\n",
        "        self.feature_map.load(json_file)\n",
        "\n",
        "    def load_data(self, data_path, use_hdf5=True, data_format='csv'):\n",
        "        self.load_json(self.json_file)\n",
        "        self.encoders = self.load_pickle(self.pickle_file)\n",
        "        if data_format == 'h5':\n",
        "            data_array = self.load_hdf5(data_path)\n",
        "            return data_array\n",
        "        elif data_format == 'csv':\n",
        "            hdf5_file = os.path.join(self.processed_dir, \n",
        "                                     os.path.splitext(os.path.basename(data_path))[0] + '.h5')\n",
        "            if use_hdf5 and os.path.exists(hdf5_file):\n",
        "                try:\n",
        "                    data_array = self.load_hdf5(hdf5_file)\n",
        "                    return data_array\n",
        "                except:\n",
        "                    print('Loading h5 file failed, reloading from {}'.format(data_path))\n",
        "            ddf = self.read_csv(data_path)\n",
        "            data_array = self.transform(ddf)\n",
        "            if use_hdf5:\n",
        "                self.save_hdf5(data_array, hdf5_file)\n",
        "        return data_array\n",
        "\n",
        "    def save_hdf5(self, data_array, data_path, key=\"data\"):\n",
        "        if not os.path.exists(os.path.dirname(data_path)):\n",
        "            os.makedirs(os.path.dirname(data_path))\n",
        "        with h5py.File(data_path, 'w') as hf:\n",
        "            hf.create_dataset(key, data=data_array)\n",
        "\n",
        "    def load_hdf5(self, data_path, key=\"data\"):\n",
        "        with h5py.File(data_path, 'r') as hf:\n",
        "            data_array = hf[key][:]\n",
        "        return data_array\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        X = self.darray[index, 0:-1]\n",
        "        y = self.darray[index, -1]\n",
        "        return X, y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.darray.shape[0]"
      ],
      "metadata": {
        "id": "Cx0lWbjVhZ65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class CTRDataModule(LightningDataModule):\n",
        "\n",
        "    dataset_cls: str = \"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_dir: Optional[str] = None,\n",
        "                 num_workers: int = 0,\n",
        "                 normalize: bool = False,\n",
        "                 batch_size: int = 32,\n",
        "                 shuffle: bool = True,\n",
        "                 pin_memory: bool = True,\n",
        "                 drop_last: bool = False,\n",
        "                 *args, \n",
        "                 **kwargs) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir: Where to save/load the data\n",
        "            num_workers: How many workers to use for loading data\n",
        "            normalize: If true applies rating normalize\n",
        "            batch_size: How many samples per batch to load\n",
        "            shuffle: If true shuffles the train data every epoch\n",
        "            pin_memory: If true, the data loader will copy Tensors into CUDA pinned memory before\n",
        "                        returning them\n",
        "            drop_last: If true drops the last incomplete batch\n",
        "        \"\"\"\n",
        "        super().__init__(data_dir)\n",
        "\n",
        "        self.data_dir = data_dir if data_dir is not None else os.getcwd()\n",
        "        self.num_workers = num_workers\n",
        "        self.normalize = normalize\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.pin_memory = pin_memory\n",
        "        self.drop_last = drop_last\n",
        "        self.kwargs = kwargs\n",
        "\n",
        "    def prepare_data(self, *args: Any, **kwargs: Any) -> None:\n",
        "        \"\"\"Saves files to data_dir.\"\"\"\n",
        "        self.dataset = self.dataset_cls(self.data_dir, **self.kwargs)\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None) -> None:\n",
        "        \"\"\"Creates train, val, and test dataset.\"\"\"\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            self.dataset_train = self.dataset_cls(self.data_dir, data_type='train', **self.kwargs)\n",
        "            self.dataset_val = self.dataset_cls(self.data_dir, data_type='valid', **self.kwargs)\n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.dataset_test = self.dataset_cls(self.data_dir, data_type='test', **self.kwargs)\n",
        "\n",
        "    def train_dataloader(self, *args: Any, **kwargs: Any) -> DataLoader:\n",
        "        \"\"\"The train dataloader.\"\"\"\n",
        "        return self._data_loader(self.dataset_train, shuffle=self.shuffle)\n",
        "\n",
        "    def val_dataloader(self, *args: Any, **kwargs: Any) -> Union[DataLoader, List[DataLoader]]:\n",
        "        \"\"\"The val dataloader.\"\"\"\n",
        "        return self._data_loader(self.dataset_val)\n",
        "\n",
        "    def test_dataloader(self, *args: Any, **kwargs: Any) -> Union[DataLoader, List[DataLoader]]:\n",
        "        \"\"\"The test dataloader.\"\"\"\n",
        "        return self._data_loader(self.dataset_test)\n",
        "\n",
        "    def _data_loader(self, dataset: Dataset, shuffle: bool = False) -> DataLoader:\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=shuffle,\n",
        "            num_workers=self.num_workers,\n",
        "            drop_last=self.drop_last,\n",
        "            pin_memory=self.pin_memory,\n",
        "        )"
      ],
      "metadata": {
        "id": "FPp-PB-ow_Tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example"
      ],
      "metadata": {
        "id": "C29MEq4tTr3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TaobaoDataset(CTRDataset):\n",
        "\n",
        "    feature_cols = [{'name': [\"userid\",\"adgroup_id\",\"pid\",\"cate_id\",\"campaign_id\",\"customer\",\"brand\",\"cms_segid\",\n",
        "                                \"cms_group_id\",\"final_gender_code\",\"age_level\",\"pvalue_level\",\"shopping_level\",\"occupation\"],\n",
        "                        'active': True, 'dtype': 'str', 'type': 'categorical'}]\n",
        "                        \n",
        "    label_col = {'name': 'clk', 'dtype': float}\n",
        "\n",
        "    train_url = \"https://github.com/RecoHut-Datasets/sample_ctr/raw/v1/train_sample.csv\"\n",
        "    valid_url = \"https://github.com/RecoHut-Datasets/sample_ctr/raw/v1/valid_sample.csv\"\n",
        "    test_url = \"https://github.com/RecoHut-Datasets/sample_ctr/raw/v1/test_sample.csv\"\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return ['train_sample.csv',\n",
        "                'valid_sample.csv',\n",
        "                'test_sample.csv']\n",
        "\n",
        "    def download(self):\n",
        "        download_url(self.train_url, self.raw_dir)\n",
        "        download_url(self.valid_url, self.raw_dir)\n",
        "        download_url(self.test_url, self.raw_dir)\n",
        "\n",
        "    def convert_hour(self, df, col_name):\n",
        "        return df['time_stamp'].apply(lambda ts: ts[11:13])\n",
        "\n",
        "    def convert_weekday(self, df, col_name):\n",
        "        def _convert_weekday(timestamp):\n",
        "            dt = date(int(timestamp[0:4]), int(timestamp[5:7]), int(timestamp[8:10]))\n",
        "            return dt.strftime('%w')\n",
        "        return df['time_stamp'].apply(_convert_weekday)\n",
        "\n",
        "    def convert_weekend(self, df, col_name):\n",
        "        def _convert_weekend(timestamp):\n",
        "            dt = date(int(timestamp[0:4]), int(timestamp[5:7]), int(timestamp[8:10]))\n",
        "            return '1' if dt.strftime('%w') in ['6', '0'] else '0'\n",
        "        return df['time_stamp'].apply(_convert_weekend)"
      ],
      "metadata": {
        "id": "6YK5FLS6h_qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TaobaoDataModule(CTRDataModule):\n",
        "    dataset_cls = TaobaoDataset"
      ],
      "metadata": {
        "id": "o98gB9N_4dL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {'model_id': 'DCN_demo',\n",
        "          'data_dir': '/content/data',\n",
        "          'model_root': './checkpoints/',\n",
        "          'dnn_hidden_units': [64, 64],\n",
        "          'dnn_activations': \"relu\",\n",
        "          'crossing_layers': 3,\n",
        "          'learning_rate': 1e-3,\n",
        "          'net_dropout': 0,\n",
        "          'batch_norm': False,\n",
        "          'optimizer': 'adamw',\n",
        "          'task': 'binary_classification',\n",
        "          'loss': 'binary_crossentropy',\n",
        "          'metrics': ['logloss', 'AUC'],\n",
        "          'embedding_dim': 10,\n",
        "          'batch_size': 64,\n",
        "          'epochs': 3,\n",
        "          'shuffle': True,\n",
        "          'seed': 2019,\n",
        "          'use_hdf5': True,\n",
        "          'workers': 1,\n",
        "          'verbose': 0}"
      ],
      "metadata": {
        "id": "egWI7tWgVMrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = TaobaoDataModule(**params)\n",
        "ds.prepare_data()\n",
        "ds.setup()\n",
        "\n",
        "for batch in ds.train_dataloader():\n",
        "    print(batch)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wm5J-aNOV3uC",
        "outputId": "5ebc3ce0-30dd-447d-c695-780e3b7406b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/datamodule.py:74: LightningDeprecationWarning: DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
            "  \"DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\"\n",
            "Downloading https://github.com/RecoHut-Datasets/sample_ctr/raw/v1/train_sample.csv\n",
            "Downloading https://github.com/RecoHut-Datasets/sample_ctr/raw/v1/valid_sample.csv\n",
            "Downloading https://github.com/RecoHut-Datasets/sample_ctr/raw/v1/test_sample.csv\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 9., 30.,  1., 24., 30., 30., 24.,  2.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
            "        [ 7., 22.,  2., 12., 22., 22.,  0.,  4.,  3.,  1.,  4.,  1.,  1.,  1.],\n",
            "        [20., 96.,  1., 18., 94., 93., 63.,  9.,  9.,  2.,  2.,  2.,  1.,  1.],\n",
            "        [15., 45.,  2., 15., 44., 44.,  0.,  6.,  7.,  1.,  5.,  1.,  1.,  1.],\n",
            "        [11., 32.,  2., 25., 32., 32., 26.,  2.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
            "        [ 2.,  8.,  1., 14.,  9., 10., 14.,  1.,  2.,  1.,  2.,  0.,  1.,  1.],\n",
            "        [ 5., 43.,  1.,  1., 42., 42., 35.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 1.,  2.,  1.,  8.,  4.,  5.,  9.,  1.,  1.,  1.,  1.,  0.,  1.,  1.],\n",
            "        [18., 78.,  1., 43., 76., 75.,  0.,  1.,  1.,  1.,  1.,  0.,  1.,  1.],\n",
            "        [ 3., 92.,  1.,  1., 90., 89.,  0.,  1.,  4.,  1.,  3.,  0.,  1.,  1.],\n",
            "        [15., 59.,  2., 10., 57., 57., 44.,  6.,  7.,  1.,  5.,  1.,  1.,  1.],\n",
            "        [ 5., 52.,  1.,  1., 51., 51.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [17., 90.,  1., 19., 88., 87., 60.,  7.,  3.,  1.,  4.,  2.,  2.,  1.],\n",
            "        [ 8., 21.,  2.,  1., 21., 21., 18.,  2.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
            "        [11., 58.,  2.,  1., 56., 56., 43.,  2.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
            "        [14., 49.,  1., 35., 48., 48., 38.,  1.,  3.,  1.,  4.,  0.,  3.,  1.],\n",
            "        [19., 71.,  1., 39., 69., 68., 52.,  8.,  1.,  1.,  1.,  2.,  1.,  2.],\n",
            "        [ 3., 97.,  1.,  5., 95., 94., 64.,  1.,  4.,  1.,  3.,  0.,  1.,  1.],\n",
            "        [ 4., 80.,  2., 44., 78., 77.,  4.,  3.,  2.,  1.,  2.,  1.,  1.,  1.],\n",
            "        [ 4., 99.,  2., 10., 97., 96.,  8.,  3.,  2.,  1.,  2.,  1.,  1.,  1.],\n",
            "        [ 8.,  9.,  2.,  1., 10., 11.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
            "        [21., 11.,  2.,  3., 12., 13.,  0.,  1.,  7.,  1.,  5.,  0.,  1.,  1.],\n",
            "        [24., 68.,  1., 37., 66., 65., 50.,  1.,  4.,  1.,  3.,  0.,  2.,  1.],\n",
            "        [ 6., 51.,  1.,  2., 50., 50., 40.,  1.,  6.,  2.,  1.,  0.,  1.,  1.],\n",
            "        [10., 31.,  2.,  3., 31., 31., 25.,  1.,  1.,  1.,  1.,  0.,  1.,  1.],\n",
            "        [17.,  1.,  1.,  4.,  3.,  4.,  5.,  7.,  3.,  1.,  4.,  2.,  2.,  1.],\n",
            "        [ 4., 89.,  2., 11., 87., 86.,  8.,  3.,  2.,  1.,  2.,  1.,  1.,  1.],\n",
            "        [13., 60.,  1.,  2., 58., 58., 45.,  1.,  5.,  2.,  3.,  0.,  3.,  1.],\n",
            "        [12., 46.,  1.,  5., 45., 45.,  0.,  5.,  8.,  2.,  5.,  1.,  1.,  1.],\n",
            "        [12., 56.,  1.,  5., 54., 54.,  0.,  5.,  8.,  2.,  5.,  1.,  1.,  1.],\n",
            "        [ 8., 13.,  2.,  1., 14.,  2.,  3.,  2.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
            "        [ 9., 54.,  1.,  3., 52., 52.,  4.,  2.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
            "        [12., 38.,  1., 28., 37., 37., 30.,  5.,  8.,  2.,  5.,  1.,  1.,  1.],\n",
            "        [ 2., 17.,  1.,  2., 17., 17., 16.,  1.,  2.,  1.,  2.,  0.,  1.,  1.],\n",
            "        [16., 79.,  1.,  4., 77., 76.,  0.,  1.,  5.,  2.,  3.,  0.,  2.,  1.],\n",
            "        [18., 70.,  1., 38., 68., 67., 51.,  1.,  1.,  1.,  1.,  0.,  1.,  1.],\n",
            "        [21.,  4.,  2., 20.,  5.,  6., 10.,  1.,  7.,  1.,  5.,  0.,  1.,  1.],\n",
            "        [ 3., 88.,  1.,  6., 86., 85.,  0.,  1.,  4.,  1.,  3.,  0.,  1.,  1.],\n",
            "        [ 5., 53.,  1.,  7.,  2.,  3.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 6., 57.,  1.,  2., 55., 55., 42.,  1.,  6.,  2.,  1.,  0.,  1.,  1.],\n",
            "        [ 4., 86.,  2., 47., 84., 83.,  0.,  3.,  2.,  1.,  2.,  1.,  1.,  1.],\n",
            "        [ 6., 48.,  1., 34., 47., 47., 37.,  1.,  6.,  2.,  1.,  0.,  1.,  1.],\n",
            "        [ 5., 61.,  1.,  1., 59., 59.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 3., 63.,  1.,  1., 61.,  2.,  3.,  1.,  4.,  1.,  3.,  0.,  1.,  1.],\n",
            "        [22., 93.,  2., 16., 91., 90., 61.,  4.,  3.,  1.,  4.,  1.,  1.,  1.],\n",
            "        [ 3., 64.,  1.,  1., 62., 61., 47.,  1.,  4.,  1.,  3.,  0.,  1.,  1.],\n",
            "        [14., 40.,  1., 30., 39., 39., 32.,  1.,  3.,  1.,  4.,  0.,  3.,  1.],\n",
            "        [10., 34.,  2., 27., 34., 34.,  2.,  1.,  1.,  1.,  1.,  0.,  1.,  1.],\n",
            "        [ 2., 24.,  1.,  3., 24., 24., 20.,  1.,  2.,  1.,  2.,  0.,  1.,  1.],\n",
            "        [16.,  1.,  1.,  4.,  3.,  4.,  5.,  1.,  5.,  2.,  3.,  0.,  2.,  1.],\n",
            "        [ 4., 67.,  2., 17., 65., 64.,  0.,  3.,  2.,  1.,  2.,  1.,  1.,  1.],\n",
            "        [20., 75.,  1., 42., 73., 72.,  7.,  9.,  9.,  2.,  2.,  2.,  1.,  1.],\n",
            "        [ 3., 84.,  1., 45., 82., 81., 57.,  1.,  4.,  1.,  3.,  0.,  1.,  1.],\n",
            "        [ 3., 81.,  1.,  6., 79., 78.,  0.,  1.,  4.,  1.,  3.,  0.,  1.,  1.],\n",
            "        [10., 42.,  2., 31., 41., 41., 34.,  1.,  1.,  1.,  1.,  0.,  1.,  1.],\n",
            "        [ 3., 77.,  1.,  1., 75., 74., 55.,  1.,  4.,  1.,  3.,  0.,  1.,  1.],\n",
            "        [ 2., 15.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  1.,  2.,  0.,  1.,  1.],\n",
            "        [ 2., 28.,  1.,  6., 28., 28.,  0.,  1.,  2.,  1.,  2.,  0.,  1.,  1.],\n",
            "        [ 5., 36.,  1.,  7., 35., 35., 28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [ 4., 65.,  2., 11., 63., 62., 48.,  3.,  2.,  1.,  2.,  1.,  1.,  1.],\n",
            "        [ 2., 25.,  1.,  1., 25., 25., 21.,  1.,  2.,  1.,  2.,  0.,  1.,  1.],\n",
            "        [ 4., 87.,  2., 11., 85., 84., 59.,  3.,  2.,  1.,  2.,  1.,  1.,  1.],\n",
            "        [15., 55.,  2., 10., 53., 53., 41.,  6.,  7.,  1.,  5.,  1.,  1.,  1.],\n",
            "        [17., 98.,  1., 19., 96., 95., 65.,  7.,  3.,  1.,  4.,  2.,  2.,  1.]],\n",
            "       dtype=torch.float64), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=torch.float64)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **References**\n",
        "> - FuxiCTR Project, https://github.com/xue-pai/FuxiCTR."
      ],
      "metadata": {
        "id": "VmBYJuNFiO3g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXwRDjpKI65c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da20be22-af77-4f48-97c1-b4e5b06aa1f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author: Sparsh A.\n",
            "\n",
            "Last updated: 2022-01-11 13:21:07\n",
            "\n",
            "recohut          : 0.0.11\n",
            "pytorch_lightning: 1.5.8\n",
            "\n",
            "Compiler    : GCC 7.5.0\n",
            "OS          : Linux\n",
            "Release     : 5.4.144+\n",
            "Machine     : x86_64\n",
            "Processor   : x86_64\n",
            "CPU cores   : 2\n",
            "Architecture: 64bit\n",
            "\n",
            "json   : 2.0.9\n",
            "logging: 0.5.1.2\n",
            "h5py   : 3.1.0\n",
            "IPython: 5.5.0\n",
            "numpy  : 1.19.5\n",
            "six    : 1.15.0\n",
            "sklearn: 0.0\n",
            "torch  : 1.10.0+cu111\n",
            "pandas : 1.1.5\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#hide\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Sparsh A.\" -m -iv -u -t -d -p recohut,pytorch_lightning"
      ]
    }
  ]
}