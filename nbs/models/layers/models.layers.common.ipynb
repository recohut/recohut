{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJDyUwKLlczI"
      },
      "outputs": [],
      "source": [
        "# default_exp models.layers.common"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6KNmybPlczM"
      },
      "source": [
        "# Common Layers\n",
        "> Common layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kbduclf_lczO"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "from nbdev.showdoc import *\n",
        "from fastcore.nb_imports import *\n",
        "from fastcore.test import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Msw56ZwzlczP"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr3YpfVUlczQ"
      },
      "outputs": [],
      "source": [
        "#exporti\n",
        "def get_activation(activation):\n",
        "    if isinstance(activation, str):\n",
        "        if activation.lower() == \"relu\":\n",
        "            return nn.ReLU()\n",
        "        elif activation.lower() == \"sigmoid\":\n",
        "            return nn.Sigmoid()\n",
        "        elif activation.lower() == \"tanh\":\n",
        "            return nn.Tanh()\n",
        "        else:\n",
        "            return getattr(nn, activation)()\n",
        "    else:\n",
        "        return activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8US77aQlczR"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class FeaturesLinear(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    References:\n",
        "        1. https://github.com/rixwew/pytorch-fm/blob/master/torchfm/layer.py\n",
        "    \"\"\"\n",
        "    def __init__(self, field_dims, output_dim=1):\n",
        "        super().__init__()\n",
        "        self.fc = torch.nn.Embedding(sum(field_dims), output_dim)\n",
        "        self.bias = torch.nn.Parameter(torch.zeros((output_dim,)))\n",
        "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
        "        \"\"\"\n",
        "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
        "        return torch.sum(self.fc(x), dim=1) + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjMEqVSClczS"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class FeaturesEmbedding(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    References:\n",
        "        1. https://github.com/rixwew/pytorch-fm/blob/master/torchfm/layer.py\n",
        "    \"\"\"\n",
        "    def __init__(self, field_dims, embed_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(sum(field_dims), embed_dim)\n",
        "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)\n",
        "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
        "        \"\"\"\n",
        "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
        "        return self.embedding(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBVehnpJlczT"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class MultiLayerPerceptron(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    References:\n",
        "        1. https://github.com/rixwew/pytorch-fm/blob/master/torchfm/layer.py\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, embed_dims, dropout, output_layer=True):\n",
        "        super().__init__()\n",
        "        layers = list()\n",
        "        for embed_dim in embed_dims:\n",
        "            layers.append(torch.nn.Linear(input_dim, embed_dim))\n",
        "            layers.append(torch.nn.BatchNorm1d(embed_dim))\n",
        "            layers.append(torch.nn.ReLU())\n",
        "            layers.append(torch.nn.Dropout(p=dropout))\n",
        "            input_dim = embed_dim\n",
        "        if output_layer:\n",
        "            layers.append(torch.nn.Linear(input_dim, 1))\n",
        "        self.mlp = torch.nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Float tensor of size ``(batch_size, embed_dim)``\n",
        "        \"\"\"\n",
        "        return self.mlp(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXsavRuYlczV"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class MLP_Layer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 output_dim=None, \n",
        "                 hidden_units=[], \n",
        "                 hidden_activations=\"ReLU\",\n",
        "                 output_activation=None, \n",
        "                 dropout_rates=[], \n",
        "                 batch_norm=False, \n",
        "                 use_bias=True):\n",
        "        super(MLP_Layer, self).__init__()\n",
        "        dense_layers = []\n",
        "        if not isinstance(dropout_rates, list):\n",
        "            dropout_rates = [dropout_rates] * len(hidden_units)\n",
        "        if not isinstance(hidden_activations, list):\n",
        "            hidden_activations = [hidden_activations] * len(hidden_units)\n",
        "        hidden_activations = [get_activation(x) for x in hidden_activations]\n",
        "        hidden_units = [input_dim] + hidden_units\n",
        "        for idx in range(len(hidden_units) - 1):\n",
        "            dense_layers.append(nn.Linear(hidden_units[idx], hidden_units[idx + 1], bias=use_bias))\n",
        "            if batch_norm:\n",
        "                dense_layers.append(nn.BatchNorm1d(hidden_units[idx + 1]))\n",
        "            if hidden_activations[idx]:\n",
        "                dense_layers.append(hidden_activations[idx])\n",
        "            if dropout_rates[idx] > 0:\n",
        "                dense_layers.append(nn.Dropout(p=dropout_rates[idx]))\n",
        "        if output_dim is not None:\n",
        "            dense_layers.append(nn.Linear(hidden_units[-1], output_dim, bias=use_bias))\n",
        "        if output_activation is not None:\n",
        "            dense_layers.append(get_activation(output_activation))\n",
        "        self.dnn = nn.Sequential(*dense_layers) # * used to unpack list\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        return self.dnn(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUqUkKJ7lczW"
      },
      "source": [
        "Sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u-44_eNlczX"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class MaskedAveragePooling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MaskedAveragePooling, self).__init__()\n",
        "\n",
        "    def forward(self, embedding_matrix):\n",
        "        sum_pooling_matrix = torch.sum(embedding_matrix, dim=1)\n",
        "        non_padding_length = (embedding_matrix != 0).sum(dim=1)\n",
        "        embedding_vec = sum_pooling_matrix / (non_padding_length.float() + 1e-16)\n",
        "        return embedding_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aKjoxNGlczY"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class MaskedSumPooling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MaskedSumPooling, self).__init__()\n",
        "\n",
        "    def forward(self, embedding_matrix):\n",
        "        # mask by zeros\n",
        "        return torch.sum(embedding_matrix, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ILT0BqDlczZ"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class KMaxPooling(nn.Module):\n",
        "    def __init__(self, k, dim):\n",
        "        super(KMaxPooling, self).__init__()\n",
        "        self.k = k\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, X):\n",
        "        index = X.topk(self.k, dim=self.dim)[1].sort(dim=self.dim)[0]\n",
        "        output = X.gather(self.dim, index)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shallow"
      ],
      "metadata": {
        "id": "_hJSebf8liFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from itertools import combinations\n",
        "\n",
        "from recohut.models.layers.embedding import EmbeddingLayer\n",
        "from recohut.models.layers.interaction import InnerProductLayer"
      ],
      "metadata": {
        "id": "R2NrPiEHln1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class LR_Layer(nn.Module):\n",
        "    def __init__(self, feature_map, output_activation=None, use_bias=True):\n",
        "        super(LR_Layer, self).__init__()\n",
        "        self.bias = nn.Parameter(torch.zeros(1), requires_grad=True) if use_bias else None\n",
        "        self.output_activation = output_activation\n",
        "        # A trick for quick one-hot encoding in LR\n",
        "        self.embedding_layer = EmbeddingLayer(feature_map, 1, use_pretrain=False)\n",
        "\n",
        "    def forward(self, X):\n",
        "        embed_weights = self.embedding_layer(X)\n",
        "        output = embed_weights.sum(dim=1)\n",
        "        if self.bias is not None:\n",
        "            output += self.bias\n",
        "        if self.output_activation is not None:\n",
        "            output = self.output_activation(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "yFWQgLMIljrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class FM_Layer(nn.Module):\n",
        "    def __init__(self, feature_map, output_activation=None, use_bias=True):\n",
        "        super(FM_Layer, self).__init__()\n",
        "        self.inner_product_layer = InnerProductLayer(feature_map.num_fields, output=\"product_sum_pooling\")\n",
        "        self.lr_layer = LR_Layer(feature_map, output_activation=None, use_bias=use_bias)\n",
        "        self.output_activation = output_activation\n",
        "\n",
        "    def forward(self, X, feature_emb):\n",
        "        lr_out = self.lr_layer(X)\n",
        "        dot_sum = self.inner_product_layer(feature_emb)\n",
        "        output = dot_sum + lr_out\n",
        "        if self.output_activation is not None:\n",
        "            output = self.output_activation(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "M3CpzKwMlliO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkBdpVZrlczZ"
      },
      "source": [
        "> **References**\n",
        "> - https://github.com/xue-pai/FuxiCTR/blob/main/fuxictr/pytorch/layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ3W_XVFlcza",
        "outputId": "68997a9e-e19f-49bb-d6ee-74a7bfd3977a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Author: Sparsh A.\n",
            "\n",
            "Last updated: 2022-01-11 12:02:41\n",
            "\n",
            "Compiler    : GCC 7.5.0\n",
            "OS          : Linux\n",
            "Release     : 5.4.144+\n",
            "Machine     : x86_64\n",
            "Processor   : x86_64\n",
            "CPU cores   : 2\n",
            "Architecture: 64bit\n",
            "\n",
            "torch     : 1.10.0+cu111\n",
            "numpy     : 1.19.5\n",
            "PIL       : 7.1.2\n",
            "IPython   : 5.5.0\n",
            "matplotlib: 3.2.2\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#hide\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Sparsh A.\" -m -iv -u -t -d"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "name": "models.layers.common.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}