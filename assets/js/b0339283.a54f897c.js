"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[6470],{3905:function(e,t,a){a.d(t,{Zo:function(){return p},kt:function(){return h}});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function s(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?s(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):s(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},s=Object.keys(e);for(n=0;n<s.length;n++)a=s[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(n=0;n<s.length;n++)a=s[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var m=n.createContext({}),l=function(e){var t=n.useContext(m),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},p=function(e){var t=l(e.components);return n.createElement(m.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,s=e.originalType,m=e.parentName,p=o(e,["components","mdxType","originalType","parentName"]),d=l(a),h=r,u=d["".concat(m,".").concat(h)]||d[h]||c[h]||s;return a?n.createElement(u,i(i({ref:t},p),{},{components:a})):n.createElement(u,i({ref:t},p))}));function h(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var s=a.length,i=new Array(s);i[0]=d;var o={};for(var m in t)hasOwnProperty.call(t,m)&&(o[m]=t[m]);o.originalType=e,o.mdxType="string"==typeof e?e:r,i[1]=o;for(var l=2;l<s;l++)i[l]=a[l];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},82746:function(e,t,a){a.r(t),a.d(t,{assets:function(){return p},contentTitle:function(){return m},default:function(){return h},frontMatter:function(){return o},metadata:function(){return l},toc:function(){return c}});var n=a(87462),r=a(63366),s=(a(67294),a(3905)),i=["components"],o={},m="Emerging Concepts in Recommender Systems",l={unversionedId:"concept-extras/emerging-concepts-in-recommender-systems",id:"concept-extras/emerging-concepts-in-recommender-systems",title:"Emerging Concepts in Recommender Systems",description:"Real-time Learning and Inference",source:"@site/docs/concept-extras/emerging-concepts-in-recommender-systems.mdx",sourceDirName:"concept-extras",slug:"/concept-extras/emerging-concepts-in-recommender-systems",permalink:"/ai/docs/concept-extras/emerging-concepts-in-recommender-systems",editUrl:"https://github.com/sparsh-ai/ai/docs/concept-extras/emerging-concepts-in-recommender-systems.mdx",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Diversity",permalink:"/ai/docs/concept-extras/diversity"},next:{title:"Graph Embeddings",permalink:"/ai/docs/concept-extras/graph-embeddings"}},p={},c=[{value:"Real-time Learning and Inference",id:"real-time-learning-and-inference",level:2},{value:"Batch vs. Real-time",id:"batch-vs-real-time",level:3},{value:"<strong>Why real-time recommendations then?</strong>",id:"why-real-time-recommendations-then",level:3},{value:"Example",id:"example",level:3},{value:"Industry examples",id:"industry-examples",level:3},{value:"Interesting reads",id:"interesting-reads",level:3},{value:"Contextual Bandits",id:"contextual-bandits",level:2},{value:"Online learning with multi-armed bandits",id:"online-learning-with-multi-armed-bandits",level:3},{value:"<strong>Multi-armed bandit algorithms can...</strong>",id:"multi-armed-bandit-algorithms-can",level:3},{value:"Contextual bandits",id:"contextual-bandits-1",level:3},{value:"A/B Testing v. Multi-Armed Bandits",id:"ab-testing-v-multi-armed-bandits",level:3},{value:"Hands-on",id:"hands-on",level:3},{value:"Graphs Networks and Embeddings",id:"graphs-networks-and-embeddings",level:2},{value:"Graph Neural Networks",id:"graph-neural-networks",level:3},{value:"Models",id:"models",level:3},{value:"Industrial examples",id:"industrial-examples",level:3},{value:"Hands-on",id:"hands-on-1",level:3},{value:"Conversational Recommenders",id:"conversational-recommenders",level:2},{value:"System designs",id:"system-designs",level:3},{value:"Interesting reads",id:"interesting-reads-1",level:3},{value:"Hands-on",id:"hands-on-2",level:3},{value:"Context-aware Recommenders",id:"context-aware-recommenders",level:2},{value:"Interesting hands-on",id:"interesting-hands-on",level:3},{value:"AutoML approach to Recommenders",id:"automl-approach-to-recommenders",level:2},{value:"Hands-on",id:"hands-on-3",level:3},{value:"Multi-task recommenders",id:"multi-task-recommenders",level:2},{value:"Multi-task learning",id:"multi-task-learning",level:3},{value:"Joint Representation Learning",id:"joint-representation-learning",level:3},{value:"Hands-on",id:"hands-on-4",level:3},{value:"References",id:"references",level:2}],d={toc:c};function h(e){var t=e.components,o=(0,r.Z)(e,i);return(0,s.kt)("wrapper",(0,n.Z)({},d,o,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"emerging-concepts-in-recommender-systems"},"Emerging Concepts in Recommender Systems"),(0,s.kt)("h2",{id:"real-time-learning-and-inference"},"Real-time Learning and Inference"),(0,s.kt)("h3",{id:"batch-vs-real-time"},"Batch vs. Real-time"),(0,s.kt)("p",null,"Batch recommendations are computationally cheaper. They are usually generated once a day and benefit from batch processing\u2019s economies of scale. Batch recommendations are also simpler ops-wise. In contrast, real-time recommendations usually require more computation. For example, we might aggregate streamed events (e.g., click, like, purchase) and generate new recommendations on-demand, based on user interactions. Operating real-time recommendations in production is also far tricker and the ops burden also increases. "),(0,s.kt)("h3",{id:"why-real-time-recommendations-then"},(0,s.kt)("strong",{parentName:"h3"},"Why real-time recommendations then?")),(0,s.kt)("p",null,"Real-time recommenders are useful when the customer journey is mission-centric and depends on the context. Such missions are often time-sensitive. Real-time demand fades quickly; demand could be met (on a competitor site) or the user might lose interest. For example, real-time recommendations are useful when the majority of our customers are new (i.e., cold-start). This happens when we\u2019re in the customer acquisition stage, such as when we\u2019ve just launched a new product or entered a new market (e.g., e-commerce in Southeast Asia in 2013 - 2015)."),(0,s.kt)("h3",{id:"example"},"Example"),(0,s.kt)("p",null,"Imagine you\u2019ve just downloaded an e-commerce app. Since we\u2019re uncertain of your gender, the home page will have a mix of categories catering to each gender, from dresses to men\u2019s shirts, from GPUs to makeup. If you click on a dress, we can immediately build a persona (that you\u2019re female) and personalize your shopping experience. In this case, the u2i recommendations on your home page will tilt towards female products."),(0,s.kt)("p",null,"To understand the drawbacks of batch-processed recommendation, imagine that a customer visits your online store, and adds pair of sunglasses to her basket. What does your offline engine typically suggest her to add next? The answer, in most cases, is more sunglasses.Your engine knows customer\u2019s past purchase but it doesn\u2019t know what she\u2019s about to purchase, and it can\u2019t adjust to accommodate this new knowledge. As a result, its subsequent recommendations will not be interesting, and the customer is likely to ignore them. A better approach would be to react to the customer\u2019s actions while they are still browsing your site, and recalculate their recommendations in real time, for example, to suggest accessories, pants or shirt to match the new pair of sunglasses. This would give the customer the feel as talking to a sales assistant in-store and make her experience more personalized."),(0,s.kt)("h3",{id:"industry-examples"},"Industry examples"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://102.alibaba.com/detail?id=183"},"Alibaba")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://dl.acm.org/doi/10.1145/2723372.2742785"},"Tencent")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://dl.acm.org/doi/10.1145/2959100.2959190"},"YouTube")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://ai.facebook.com/blog/powered-by-ai-instagrams-explore-recommender-system/"},"Instagram")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://dl.acm.org/doi/10.1145/2959100.2959174"},"Netflix")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://newsroom.tiktok.com/en-us/how-tiktok-recommends-videos-for-you"},"Tiktok")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://youtu.be/WQ520rWgd9A"},"Weibo")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://logicai.io/blog/building-real-time-recommendations-e-commercie-giant/"},"Sephora"))),(0,s.kt)("h3",{id:"interesting-reads"},"Interesting reads"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://www.amazon.science/publications/temporal-contextual-recommendation-in-real-time"},"Temporal-contextual recommendation in real-time"))),(0,s.kt)("h2",{id:"contextual-bandits"},"Contextual Bandits"),(0,s.kt)("p",null,"An active area of research is recommender systems that incorporate bandit-based approaches. A bandit algorithm is a form of reinforcement learning (RL) that tries to balance exploration of new possibilities with exploitation of profitable ones already discovered. They have been frequently used as an alternative to static A/B testing: a key advantage is their ability to adapt in real time. This could help to overcome the cold start problem. In fact, bandit algorithms could be used to make real-time selections between several recommender systems based on how users respond to the different recommendations provided by each system. An increasingly important application of bandits is in systems that take into account multiple objectives and metrics related to user satisfaction, and/or multiple stakeholders (a \u201cmarketplace\u201d \u2013 users, advertisers, platform holders, content owners etc.). For example, in a music content recommender system, an additional objective might be to provide \u201cfairness\u201d for long-tail artists and content by ensuring they receive at least some recommendations."),(0,s.kt)("h3",{id:"online-learning-with-multi-armed-bandits"},"Online learning with multi-armed bandits"),(0,s.kt)("p",null,"Why are we even building an n-armed bandit in the first place? Why not just use an A/B/n test? Or maybe get your customer on the phone and ask a question? We have data on what customers buy. So you might own a store that sells perfume or cologne, and you know what people are buying. Here\u2019s the thing though, ",(0,s.kt)("strong",{parentName:"p"},"things change"),"; something might be trendy last week isn\u2019t popular next. Season\u2019s shift tastes change, and people change as well. Some people are very open to experience while others aren\u2019t. Some are open to exploration, while others want the same thing every time."),(0,s.kt)("p",null,"In many recommendation domains, such as that of recommending news articles, the cold-start problem is pervasive. New articles and stories appear all the time, and the effectiveness of various algorithms may also vary with time. In such cases, it is crucial for the approach to continuously explore the space of various choices as new data are received. At the same time, the learned data are exploited in order to optimize the payoff in terms of the conversion rate. This type of trade-off between exploration and exploitation is managed with the help of multi-armed bandit algorithms."),(0,s.kt)("h3",{id:"multi-armed-bandit-algorithms-can"},(0,s.kt)("strong",{parentName:"h3"},"Multi-armed bandit algorithms can...")),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"recommend products with the highest expected value while still exploring other products."),(0,s.kt)("li",{parentName:"ul"},"do not suffer from the cold-start problem and therefore don\u2019t require customer preferences or information about products."),(0,s.kt)("li",{parentName:"ul"},"take into account the limitations of how much data you have as well as the cost of gathering data (the opportunity cost of sub-optimal recommendations).")),(0,s.kt)("h3",{id:"contextual-bandits-1"},"Contextual bandits"),(0,s.kt)("p",null,"The contextual bandit problem is a generalization of the multi-armed bandit that extends the model by making actions conditional on the state of the environment. A common real-world contextual bandit example is a news recommendation system. Given a set of presented news articles, a reward is determined by the click-through behavior of the user. If she clicks on the article, a payout of 1 is incurred and 0 otherwise. Click-through-rate (CRT) is used to determine the selection and placement of ads within the news recommendation application."),(0,s.kt)("p",null,"Now suppose rewards are determined by CTR in conjunction with metadata about the user (e.g., age and gender), so recommendations can be further personalized. Take a 65 year old female and an 18 year male for example, both who read news articles from their mobile device. The recommendations for these two users should reflect their contrasting profiles. It wouldn\u2019t make sense to show ads for retirement plans or mature women clothing stores to the 18 year old male."),(0,s.kt)("h3",{id:"ab-testing-v-multi-armed-bandits"},"A/B Testing v. Multi-Armed Bandits"),(0,s.kt)("p",null,"Experiments based on multi-armed bandits are typically much more efficient than \u201cclassical\u201d A-B experiments based on statistical-hypothesis testing. They\u2019re just as statistically valid, and in many circumstances they can produce answers far more quickly. They\u2019re more efficient because they move traffic towards winning variations gradually, instead of forcing you to wait for a \u201cfinal answer\u201d at the end of an experiment. They\u2019re faster because samples that would have gone to obviously inferior variations can be assigned to potential winners. The extra data collected on the high-performing variations can help separate the \u201cgood\u201d arms from the \u201cbest\u201d ones more quickly."),(0,s.kt)("p",null,"Basically, bandits make experiments more efficient, so you can try more of them. You can also allocate a larger fraction of your traffic to your experiments, because traffic will be automatically steered to better performing pages."),(0,s.kt)("p",null,(0,s.kt)("img",{loading:"lazy",alt:"/img/content-concept-raw-emerging-concepts-in-recommender-systems-untitled.png",src:a(15721).Z,width:"602",height:"393"})),(0,s.kt)("p",null,"As you can see from the plot above, A/B testing does not adapt throughout the stages of the experiment, since the exploration and exploitation phases are static and distinct, whereas the multi-armed bandit adjusts through simultaneous exploration and exploitation. Furthermore, with multi-armed bandits, less time and resources are allocated to inferior arms/slices. Instead, traffic is gradually allocated to more optimal slices throughout the duration of the experiment. One of the big benefits of bandit testing is that bandits mitigate \u2018regret,\u2019 which is basically the lost conversion you experience while exploring a potentially worse variation in a test."),(0,s.kt)("p",null,"The key idea is that improvement of the recommender policy is achieved in a trial-and-error fashion by taking user features into account. In general, bandit algorithms can be thought of as \u201conline self-improving A/B testing\u201d. A somewhat cartoonish way of comparison of the usual A/B testing, standard multi-armed bandit approach (online improvement not taking individual user features into account) and contextual bandit approach (online improvement taking individual user features into account) could be viewed as follows."),(0,s.kt)("p",null,(0,s.kt)("img",{loading:"lazy",alt:"/img/content-concept-raw-emerging-concepts-in-recommender-systems-untitled-1.png",src:a(76103).Z,width:"589",height:"259"})),(0,s.kt)("p",null,"Mathematically the player wishes to minimize the regret: ",(0,s.kt)("span",{parentName:"p",className:"math math-inline"},(0,s.kt)("span",{parentName:"span",className:"katex"},(0,s.kt)("span",{parentName:"span",className:"katex-mathml"},(0,s.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,s.kt)("semantics",{parentName:"math"},(0,s.kt)("mrow",{parentName:"semantics"},(0,s.kt)("msub",{parentName:"mrow"},(0,s.kt)("mi",{parentName:"msub"},"R"),(0,s.kt)("mi",{parentName:"msub"},"n")),(0,s.kt)("mo",{parentName:"mrow"},":"),(0,s.kt)("mo",{parentName:"mrow"},"="),(0,s.kt)("mi",{parentName:"mrow"},"n"),(0,s.kt)("msub",{parentName:"mrow"},(0,s.kt)("mi",{parentName:"msub"},"\u03bc"),(0,s.kt)("mo",{parentName:"msub"},"\u2217")),(0,s.kt)("mo",{parentName:"mrow"},"\u2212"),(0,s.kt)("mi",{parentName:"mrow"},"E"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"["),(0,s.kt)("msubsup",{parentName:"mrow"},(0,s.kt)("mo",{parentName:"msubsup"},"\u2211"),(0,s.kt)("mrow",{parentName:"msubsup"},(0,s.kt)("mi",{parentName:"mrow"},"t"),(0,s.kt)("mo",{parentName:"mrow"},"="),(0,s.kt)("mn",{parentName:"mrow"},"1")),(0,s.kt)("mi",{parentName:"msubsup"},"n")),(0,s.kt)("msub",{parentName:"mrow"},(0,s.kt)("mi",{parentName:"msub"},"R"),(0,s.kt)("mi",{parentName:"msub"},"t")),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"]"),(0,s.kt)("mo",{parentName:"mrow",separator:"true"},",")),(0,s.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"R_n := n \\mu_* - E[\\sum_{t=1}^n R_t],")))),(0,s.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"0.8333em",verticalAlign:"-0.15em"}}),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.00773em"}},"R"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.1514em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.55em",marginLeft:"-0.0077em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"n")))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,s.kt)("span",{parentName:"span"})))))),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}}),(0,s.kt)("span",{parentName:"span",className:"mrel"},":="),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}})),(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"0.7778em",verticalAlign:"-0.1944em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"n"),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"\u03bc"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.1757em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mbin mtight"},"\u2217")))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,s.kt)("span",{parentName:"span"})))))),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222em"}}),(0,s.kt)("span",{parentName:"span",className:"mbin"},"\u2212"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222em"}})),(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"1.104em",verticalAlign:"-0.2997em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.05764em"}},"E"),(0,s.kt)("span",{parentName:"span",className:"mopen"},"["),(0,s.kt)("span",{parentName:"span",className:"mop"},(0,s.kt)("span",{parentName:"span",className:"mop op-symbol small-op",style:{position:"relative",top:"0em"}},"\u2211"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.8043em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.4003em",marginLeft:"0em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"t"),(0,s.kt)("span",{parentName:"span",className:"mrel mtight"},"="),(0,s.kt)("span",{parentName:"span",className:"mord mtight"},"1")))),(0,s.kt)("span",{parentName:"span",style:{top:"-3.2029em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"n")))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.2997em"}},(0,s.kt)("span",{parentName:"span"})))))),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.1667em"}}),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.00773em"}},"R"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.2806em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.55em",marginLeft:"-0.0077em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"t")))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,s.kt)("span",{parentName:"span"})))))),(0,s.kt)("span",{parentName:"span",className:"mclose"},"]"),(0,s.kt)("span",{parentName:"span",className:"mpunct"},",")))))," where\xa0\u03bc\u2217 is the expected reward of the best arm (again, a priori this information is obviously unknown)  and\xa0E","[\u2026]"," is the expected value. Given this data and assuming that the distributions\xa0Pi are sufficiently nice (e.g. Gaussian), one can efficiently solve this problem by exploring and exploiting the given actions."),(0,s.kt)("h3",{id:"hands-on"},"Hands-on"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"BEARS: Towards an Evaluation Framework for Bandit-based Interactive Recommender Systems [",(0,s.kt)("a",{parentName:"li",href:"https://youtu.be/GIH_ArJ-ylk"},"Video"),"] [",(0,s.kt)("a",{parentName:"li",href:"https://gitlab.insight-centre.org/andbar/bears/tree/ee55fc872603efc127966b96373d375c0d9e4474/tutorials/RECSYS2020"},"GitLab"),"]"),(0,s.kt)("li",{parentName:"ul"},"Build a Product Recommender Using Multi-Armed Bandit Algorithms [",(0,s.kt)("a",{parentName:"li",href:"https://www.offerzen.com/blog/how-to-build-a-product-recommender-using-multi-armed-bandit-algorithms"},"Blog"),"] [",(0,s.kt)("a",{parentName:"li",href:"https://nbviewer.jupyter.org/gist/sparsh-ai/e591aebd23d4bf8864aa6f93b89fc290"},"Colab"),"]"),(0,s.kt)("li",{parentName:"ul"},"Bandit Algorithms for Website Optimization [",(0,s.kt)("a",{parentName:"li",href:"https://learning.oreilly.com/library/view/bandit-algorithms-for/9781449341565/"},"eBook O'reilly"),"] [",(0,s.kt)("a",{parentName:"li",href:"https://github.com/johnmyleswhite/BanditsBook"},"GitHub"),"] [",(0,s.kt)("a",{parentName:"li",href:"https://nbviewer.jupyter.org/gist/sparsh-ai/b42056d45ca8238fe912baad036597a8"},"Colab"),"]")),(0,s.kt)("h2",{id:"graphs-networks-and-embeddings"},"Graphs Networks and Embeddings"),(0,s.kt)("p",null,"In recommender systems, most information has a graph structure. For example, the social relationships among users and knowledge graphs related to items are naturally graph data. In addition, the interactions between users and items can be considered as the bipartite graph, and the item transitions in sequences can be constructed as graphs as well. Therefore, graph learning approaches have been leveraged to get user/item embeddings. Among graph learning methods, graph neural network (GNN) enjoys a massive hype at the moment."),(0,s.kt)("p",null,(0,s.kt)("img",{loading:"lazy",alt:"/img/content-concept-raw-emerging-concepts-in-recommender-systems-untitled-2.png",src:a(13368).Z,width:"781",height:"555"})),(0,s.kt)("h3",{id:"graph-neural-networks"},"Graph Neural Networks"),(0,s.kt)("p",null,"GNN models exploit graph structure to guide representation learning. The basic idea is the embedding propagation mechanism, which aggregates the embeddings of neighbors to update the target node\u2019s embedding. By recursively performing such propagations, the information from multihop neighbors is encoded into the representation of the target node. GNN models have been widely used in many fundamental tasks due to their strong representation ability, spanning from node classification, link prediction, to graph classification, and achieved remarkable improvements."),(0,s.kt)("p",null,"Graph neural network is able to capture the higher-order interaction in user-item relationships through iterative propagation. Moreover, if the information of social relationship or knowledge graph is available, it enables to integrate such side information in network structure efficiently."),(0,s.kt)("h3",{id:"models"},"Models"),(0,s.kt)("p",null,(0,s.kt)("img",{loading:"lazy",alt:"/img/content-concept-raw-emerging-concepts-in-recommender-systems-untitled-3.png",src:a(3456).Z,width:"786",height:"565"})),(0,s.kt)("h3",{id:"industrial-examples"},"Industrial examples"),(0,s.kt)("p",null,(0,s.kt)("img",{loading:"lazy",alt:"/img/content-concept-raw-emerging-concepts-in-recommender-systems-untitled-4.png",src:a(61288).Z,width:"1338",height:"568"})),(0,s.kt)("h3",{id:"hands-on-1"},"Hands-on"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"Simple recommender with matrix factorization, graph, and NLP [",(0,s.kt)("a",{parentName:"li",href:"https://github.com/eugeneyan/recsys-nlp-graph"},"Git"),"]"),(0,s.kt)("li",{parentName:"ul"},"Application of graph embeddings in recommendation system [",(0,s.kt)("a",{parentName:"li",href:"https://github.com/januverma/Graph-Embeddings-Recommendations-DHS2019"},"Git"),"]")),(0,s.kt)("h2",{id:"conversational-recommenders"},"Conversational Recommenders"),(0,s.kt)("p",null,"Recent years have witnessed the emerging of conversational systems, including both physical devices and mobile-based applications. Both the research community and industry believe that conversational systems will have a major impact on human-computer interaction, and specifically, the RecSys community has begun to explore Conversational Recommendation Systems."),(0,s.kt)("p",null,"Conversational recommendation aims at finding or recommending the most relevant information (e.g., web pages, answers, movies, products) for users based on textual- or spoken-dialogs, through which users can communicate with the system more efficiently using natural language conversations. Due to users\u2019 constant need to look for information to support both work and daily life, conversational recommendation system will be one of the key techniques towards an intelligent web. A personalized conversational sales agent could have much commercial potential. E-commerce companies such as Amazon, eBay, JD, Alibaba etc. are piloting such kind of agents with their users."),(0,s.kt)("h3",{id:"system-designs"},"System designs"),(0,s.kt)("p",null,(0,s.kt)("img",{loading:"lazy",alt:"Typical architecture of a conversational recommender system",src:a(24636).Z,width:"820",height:"459"})),(0,s.kt)("p",null,"Typical architecture of a conversational recommender system"),(0,s.kt)("p",null,(0,s.kt)("img",{loading:"lazy",alt:"Typical Structure of Task-oriented Dialogue System",src:a(44066).Z,width:"787",height:"403"})),(0,s.kt)("p",null,"Typical Structure of Task-oriented Dialogue System"),(0,s.kt)("h3",{id:"interesting-reads-1"},"Interesting reads"),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"http://staff.ustc.edu.cn/~hexn/slides/sigir20-tutorial-CRS-slides.pdf"})),(0,s.kt)("h3",{id:"hands-on-2"},"Hands-on"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://github.com/RUCAIBox/CRSLab"},"CRSLab is an open-source toolkit for building Conversational Recommender System (CRS)"))),(0,s.kt)("h2",{id:"context-aware-recommenders"},"Context-aware Recommenders"),(0,s.kt)("p",null,"The importance of contextual information has been recognized by researchers and practitioners in many disciplines, including e-commerce personalization, information retrieval, ubiquitous and mobile computing, data mining, marketing, and management. While a substantial amount of research has already been performed in the area of recommender systems, many existing approaches focus on recommending the most relevant items to users without taking into account any additional contextual information, such as time, location, or the company of other people (e.g., for watching movies or dining out). There is growing understanding that relevant contextual information does matter in recommender systems and that it is important to take this information into account when providing recommendations."),(0,s.kt)("p",null,(0,s.kt)("img",{loading:"lazy",alt:"/img/content-concept-raw-emerging-concepts-in-recommender-systems-untitled-7.png",src:a(195).Z,width:"1315",height:"626"})),(0,s.kt)("p",null,"Context-aware recommendation systems represent an emerging area of experimentation and research, aiming to provide even more precise content given the context of the user in a particular moment in time. For example, is the user at home, or on the go? Using a larger or smaller screen? Is it morning or night? Given the data available on a certain user, context-aware systems may be able to provide recommendations a user is more likely to take in those scenarios."),(0,s.kt)("h3",{id:"interesting-hands-on"},"Interesting hands-on"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"Contextual movie recommender [",(0,s.kt)("a",{parentName:"li",href:"https://github.com/yadavgaurav251/Context-Aware-Recommender"},"Git"),"]"),(0,s.kt)("li",{parentName:"ul"},"Music recommendation system using Facial Expression and Factorization Machines [",(0,s.kt)("a",{parentName:"li",href:"https://github.com/Pager07/Context-aware-music-recommendation-system"},"Git"),"]")),(0,s.kt)("p",null,(0,s.kt)("img",{loading:"lazy",alt:"/img/content-concept-raw-emerging-concepts-in-recommender-systems-untitled-8.png",src:a(7009).Z,width:"2216",height:"1721"})),(0,s.kt)("h2",{id:"automl-approach-to-recommenders"},"AutoML approach to Recommenders"),(0,s.kt)("p",null,(0,s.kt)("img",{loading:"lazy",alt:"/img/content-concept-raw-emerging-concepts-in-recommender-systems-untitled-9.png",src:a(53416).Z,width:"700",height:"384"})),(0,s.kt)("h3",{id:"hands-on-3"},"Hands-on"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"AutoML: Taking TPOT to the Movies [",(0,s.kt)("a",{parentName:"li",href:"https://colab.research.google.com/drive/1nsFIhZ13uOkHjBzv_26IqAHXcfBeB9rV?usp=sharing"},"Colab"),"]")),(0,s.kt)("h2",{id:"multi-task-recommenders"},"Multi-task recommenders"),(0,s.kt)("p",null,"In many applications, however, there are multiple rich sources of feedback to draw upon. For example, an e-commerce site may record user visits to product pages (abundant, but relatively low signal), image clicks, adding to cart, and, finally, purchases. It may even record post-purchase signals such as reviews and returns."),(0,s.kt)("p",null,"Integrating all these different forms of feedback is critical to building systems that users love to use, and that do not optimize for any one metric at the expense of overall performance."),(0,s.kt)("p",null,"In addition, building a joint model for multiple tasks may produce better results than building a number of task-specific models. This is especially true where some data is abundant (for example, clicks), and some data is sparse (purchases, returns, manual reviews). In those scenarios, a joint model may be able to use representations learned from the abundant task to improve its predictions on the sparse task via a phenomenon known as\xa0",(0,s.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Transfer_learning"},"transfer learning"),". For example,\xa0",(0,s.kt)("a",{parentName:"p",href:"https://openreview.net/pdf?id=SJxPVcSonN"},"this paper"),"\xa0shows that a model predicting explicit user ratings from sparse user surveys can be substantially improved by adding an auxiliary task that uses abundant click log data."),(0,s.kt)("h3",{id:"multi-task-learning"},"Multi-task learning"),(0,s.kt)("p",null,"Multi-task learning is an approach in which multiple learning tasks are solved at the same time while exploiting commonalities and differences across tasks. It has been used successfully in many computer vision and natural language processing tasks. There are many advantages to using deep neural networks based on multi-task learning. It helps prevent overfitting by generalizing the shared hidden representations. It provides interpretable outputs for explaining the recommendation. It implicitly augments the data and thus alleviates the sparsity problem. Finally, we can deploy multi-task learning for cross-domain recommendations with each specific task generating recommendations for each domain."),(0,s.kt)("h3",{id:"joint-representation-learning"},"Joint Representation Learning"),(0,s.kt)("p",null,"A recent framework called Joint Representation Learning is capable of learning multi-modal representations of users and items. In this framework, each type of information source (review text, product image, numerical rating, etc) is adopted to learn the corresponding user and item representations based on available (deep) representation learning architectures. Representations from different sources are integrated with an extra layer to obtain the joint representations for users and items. In the end, both the per-source and the joint representations are trained as a whole using pair-wise learning to rank for the top-N recommendation. By representing users and items into embeddings offline, and using a simple vector multiplication for ranking score calculation online, JRL also has the advantage of fast online prediction compared with other deep learning approaches to a recommendation that learn a complex prediction network for online calculation. Therefore, another promising research direction is to design better inductive biases in an end-to-end pipeline, which can reason over different modalities data for better recommendation performance."),(0,s.kt)("h3",{id:"hands-on-4"},"Hands-on"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"Multi-objective recommender for Movielens [",(0,s.kt)("a",{parentName:"li",href:"https://www.tensorflow.org/recommenders/examples/multitask/"},"Code"),"]")),(0,s.kt)("h2",{id:"references"},"References"),(0,s.kt)("table",null,(0,s.kt)("thead",{parentName:"table"},(0,s.kt)("tr",{parentName:"thead"},(0,s.kt)("th",{parentName:"tr",align:null},"Name"),(0,s.kt)("th",{parentName:"tr",align:null},"Link"))),(0,s.kt)("tbody",{parentName:"table"},(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"Real-time Machine Learning For Recommendations"),(0,s.kt)("td",{parentName:"tr",align:null},(0,s.kt)("a",{parentName:"td",href:"https://eugeneyan.com/writing/real-time-recommendations/"},"https://eugeneyan.com/writing/real-time-recommendations/"))),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"Using Navigation to Improve Recommendations in Real-Time"),(0,s.kt)("td",{parentName:"tr",align:null},(0,s.kt)("a",{parentName:"td",href:"https://dl.acm.org/doi/10.1145/2959100.2959174"},"https://dl.acm.org/doi/10.1145/2959100.2959174"))),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"Powered by AI: Instagram's Explore recommender system"),(0,s.kt)("td",{parentName:"tr",align:null},(0,s.kt)("a",{parentName:"td",href:"https://ai.facebook.com/blog/powered-by-ai-instagrams-explore-recommender-system/"},"https://ai.facebook.com/blog/powered-by-ai-instagrams-explore-recommender-system/"))),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"Deep Neural Networks for YouTube Recommendations"),(0,s.kt)("td",{parentName:"tr",align:null},(0,s.kt)("a",{parentName:"td",href:"https://dl.acm.org/doi/10.1145/2959100.2959190"},"https://dl.acm.org/doi/10.1145/2959100.2959190"))),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"TencentRec"),(0,s.kt)("td",{parentName:"tr",align:null},"Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data")),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"Machine learning is going real-time"),(0,s.kt)("td",{parentName:"tr",align:null},(0,s.kt)("a",{parentName:"td",href:"https://huyenchip.com/2020/12/27/real-time-machine-learning.html"},"https://huyenchip.com/2020/12/27/real-time-machine-learning.html"))),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},(0,s.kt)("a",{parentName:"td",href:"https://www.veed.io/grow/reverse-engineering-how-tiktok-algorithm-works/"},"https://www.veed.io/grow/reverse-engineering-how-tiktok-algorithm-works/")),(0,s.kt)("td",{parentName:"tr",align:null})),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},(0,s.kt)("a",{parentName:"td",href:"http://datasadak.com/what-makes-tiktok-recommendation-system-so-powerful/"},"http://datasadak.com/what-makes-tiktok-recommendation-system-so-powerful/")),(0,s.kt)("td",{parentName:"tr",align:null})),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},(0,s.kt)("a",{parentName:"td",href:"https://newsroom.tiktok.com/en-us/how-tiktok-recommends-videos-for-you"},"https://newsroom.tiktok.com/en-us/how-tiktok-recommends-videos-for-you")),(0,s.kt)("td",{parentName:"tr",align:null})),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},(0,s.kt)("a",{parentName:"td",href:"https://techcrunch.com/2020/06/18/tiktok-explains-how-the-recommendation-system-behind-its-for-you-feed-works/"},"https://techcrunch.com/2020/06/18/tiktok-explains-how-the-recommendation-system-behind-its-for-you-feed-works/")),(0,s.kt)("td",{parentName:"tr",align:null})),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},'"Reinforcement Learning for Recommender Systems: A Case Study on Youtube," by Minmin Chen'),(0,s.kt)("td",{parentName:"tr",align:null},(0,s.kt)("a",{parentName:"td",href:"https://youtu.be/HEqQ2_1XRTs?list=PLN7ADELDRRhjH-LXON13wyKGN7nDOhcL1"},"https://youtu.be/HEqQ2_1XRTs?list=PLN7ADELDRRhjH-LXON13wyKGN7nDOhcL1"))),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"Top-K Off-Policy Correction for a REINFORCE Recommender System"),(0,s.kt)("td",{parentName:"tr",align:null},"AISC")),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"Contextual Bandits for In-App Recommendation"),(0,s.kt)("td",{parentName:"tr",align:null},(0,s.kt)("a",{parentName:"td",href:"https://engineering.nordeus.com/contextual-bandits-for-in-app-recommendation/"},"https://engineering.nordeus.com/contextual-bandits-for-in-app-recommendation/"))),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null}),(0,s.kt)("td",{parentName:"tr",align:null},(0,s.kt)("a",{parentName:"td",href:"https://arxiv.org/pdf/2011.02260v1.pdf"},"https://arxiv.org/pdf/2011.02260v1.pdf"))),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"RecSys 2020 Keynote: Conversational AI Agents That Can Truly Understand and Help Users"),(0,s.kt)("td",{parentName:"tr",align:null},(0,s.kt)("a",{parentName:"td",href:"https://youtu.be/V5xBqcMqT2o?list=PLaZufLfJumb-cVIEsyg4CFocuq4WsvjED"},"https://youtu.be/V5xBqcMqT2o?list=PLaZufLfJumb-cVIEsyg4CFocuq4WsvjED"))),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"RecSys 2020 Tutorial: Conversational Recommender Systems"),(0,s.kt)("td",{parentName:"tr",align:null},(0,s.kt)("a",{parentName:"td",href:"https://youtu.be/RdGnJSRA0aw"},"https://youtu.be/RdGnJSRA0aw"))),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"Towards Conversational Recommender Systems"),(0,s.kt)("td",{parentName:"tr",align:null},(0,s.kt)("a",{parentName:"td",href:"https://youtu.be/nLUfAJqXFUI"},"https://youtu.be/nLUfAJqXFUI"))),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null}),(0,s.kt)("td",{parentName:"tr",align:null},(0,s.kt)("a",{parentName:"td",href:"https://arxiv.org/pdf/2004.00646.pdf"},"https://arxiv.org/pdf/2004.00646.pdf"))),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"Yisong Miao"),(0,s.kt)("td",{parentName:"tr",align:null},(0,s.kt)("a",{parentName:"td",href:"https://yisong.me/readpapers/convrec/"},"https://yisong.me/readpapers/convrec/"))),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"D\xe9j\xe0 vu: A Contextualized Temporal Attention Mechanism for Sequential Recommendation"),(0,s.kt)("td",{parentName:"tr",align:null},(0,s.kt)("a",{parentName:"td",href:"https://arxiv.org/abs/2002.00741v1"},"https://arxiv.org/abs/2002.00741v1"))),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"Context-Aware Recommendation Systems"),(0,s.kt)("td",{parentName:"tr",align:null},(0,s.kt)("a",{parentName:"td",href:"https://youtu.be/TBg9FSAb8zw"},"https://youtu.be/TBg9FSAb8zw"))),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"AutoML for predictive modeling"),(0,s.kt)("td",{parentName:"tr",align:null},(0,s.kt)("a",{parentName:"td",href:"https://towardsdatascience.com/automl-for-predictive-modeling-32b84c5a18f6"},"https://towardsdatascience.com/automl-for-predictive-modeling-32b84c5a18f6"))),(0,s.kt)("tr",{parentName:"tbody"},(0,s.kt)("td",{parentName:"tr",align:null},"Multi-task Learning for Recommender Systems"),(0,s.kt)("td",{parentName:"tr",align:null},(0,s.kt)("a",{parentName:"td",href:"http://glaros.dtc.umn.edu/gkhome/node/747"},"http://glaros.dtc.umn.edu/gkhome/node/747"))))))}h.isMDXComponent=!0},76103:function(e,t,a){t.Z=a.p+"assets/images/content-concept-raw-emerging-concepts-in-recommender-systems-untitled-1-b501da1f06fb3ff8e44f8d46d5a786e2.png"},13368:function(e,t,a){t.Z=a.p+"assets/images/content-concept-raw-emerging-concepts-in-recommender-systems-untitled-2-3d9fccf2640194132506f879d48de40c.png"},3456:function(e,t,a){t.Z=a.p+"assets/images/content-concept-raw-emerging-concepts-in-recommender-systems-untitled-3-d6dafc51f2a1e916a069075fb08d0f7f.png"},61288:function(e,t,a){t.Z=a.p+"assets/images/content-concept-raw-emerging-concepts-in-recommender-systems-untitled-4-67b9c77c059ada439d1801bfd4d99af2.png"},24636:function(e,t,a){t.Z=a.p+"assets/images/content-concept-raw-emerging-concepts-in-recommender-systems-untitled-5-713ea13af61354f6ce4074b32f11d32d.png"},44066:function(e,t,a){t.Z=a.p+"assets/images/content-concept-raw-emerging-concepts-in-recommender-systems-untitled-6-f7fe44db61b349a03275df716bc66d45.png"},195:function(e,t,a){t.Z=a.p+"assets/images/content-concept-raw-emerging-concepts-in-recommender-systems-untitled-7-86d81c19a0a5920ef355d1f65a25012a.png"},7009:function(e,t,a){t.Z=a.p+"assets/images/content-concept-raw-emerging-concepts-in-recommender-systems-untitled-8-00a637c27a682bc0fb87f06cca75fe41.png"},53416:function(e,t,a){t.Z=a.p+"assets/images/content-concept-raw-emerging-concepts-in-recommender-systems-untitled-9-3c6b6e5f1329155a4e441c603519e2c2.png"},15721:function(e,t,a){t.Z=a.p+"assets/images/content-concept-raw-emerging-concepts-in-recommender-systems-untitled-7a3a1e7a6d4b82c7b4777e99ebbdd3f9.png"}}]);