{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-14-cotrec-sess-retailrocket.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/P740460%20%7C%20Training%20COTREC%20Session-based%20recommendation%20model%20on%20Diginetica%2C%20Tmall%2C%20and%20RetailRocket%20datasets.ipynb","timestamp":1644613149413}],"collapsed_sections":[],"authorship_tag":"ABX9TyO5SQouncJ9gK/Z07uiSfqn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Training COTREC Session-based recommendation model on Diginetica, Tmall, and RetailRocket datasets"],"metadata":{"id":"F7Ac-gx2fLdU"}},{"cell_type":"markdown","source":["## Executive summary\n","\n","| | |\n","| --- | --- |\n","| Problem | Session-based recommendation targets next-item prediction by exploiting user behaviors within a short time period. Compared with other recommendation paradigms, session-based recommendation suffers more from the problem of data sparsity due to the very limited short-term interactions. Self-supervised learning, which can discover ground-truth samples from the raw data, holds vast potentials to tackle this problem. However, existing self-supervised recommendation models mainly rely on item/segment dropout to augment data, which are not fit for session-based recommendation because the dropout leads to sparser data, creating unserviceable self-supervision signals. |\n","| Solution | For informative session-based data augmentation, we combine self-supervised learning with co-training, and then develop a framework to enhance session-based recommendation. Technically, we first exploit the session-based graph to augment two views that exhibit the internal and external connectivities of sessions, and then we build two distinct graph encoders over the two views, which recursively leverage the different connectivity information to generate ground-truth samples to supervise each other by contrastive learning. In contrast to the dropout strategy, the proposed self-supervised graph co-training preserves the complete session information and fulfills genuine data augmentation. |\n","| Dataset | Diginetica, Tmall, and RetailRocket. |\n","| Preprocessing | We filter out all sessions whose length is 1 and items appearing less than 5 times. Latest data (such as, the data of last week) is set to be test set and previous data is used as training set. Then, we augment and label the training and test datasets by using a sequence splitting method, which generates multiple labeled sequences with the corresponding labels ( [ùëñùë†,1],ùëñùë†,2), ( [ùëñùë†,1,ùëñùë†,2],ùëñùë†,3), ..., ( [ùëñùë†,1,ùëñùë†,2, ...,ùëñùë†,ùëö‚àí1],ùëñùë†,ùëö) for every session ùë† = [ùëñùë†,1,ùëñùë†,2,ùëñùë†,3, ...,ùëñùë†,ùëö]. Note that the label of each sequence is the last click item in it. |\n","| Metrics | Precision, MRR. |\n","| Hyperparams | We set the embedding size to 100, the batch size for mini-batch to 100, and the ùêø2 regularization to 10‚àí5 . All parameters are initialized with the Gaussian Distribution N (0, 0.1). We use Adam with the learning rate of 0.001 to optimize our model. For the number of layers of graph convolution on the three datasets, a three-layer setting achieves the best performance. |\n","| Models | CO-Training framework for session-based RECommendation (COTREC) |\n","| Cluster | Python 3.6+, PyTorch, CUDA GPU. |\n","| Tags | `SessionRecommender`, `SelfSupervisedLearning`, `GraphNeuralNetwork`, `CoTraining` |\n","| Credits | Xin Xia |"],"metadata":{"id":"LnuDlaMJfO7-"}},{"cell_type":"markdown","source":["## Process flow\n","\n","![](https://github.com/RecoHut-Stanzas/S969915/raw/main/images/process_flow.svg)"],"metadata":{"id":"bKUmaoaae_81"}},{"cell_type":"markdown","source":["## Codebase"],"metadata":{"id":"LNRmdbRPFLnI"}},{"cell_type":"markdown","source":["### Imports"],"metadata":{"id":"ofJF8YIrFU5k"}},{"cell_type":"code","source":["import numpy as np\n","from scipy.sparse import coo_matrix, csr_matrix\n","from operator import itemgetter\n","import random\n","import datetime\n","import math\n","import torch\n","from torch import nn, backends\n","from torch.nn import Module, Parameter\n","import torch.nn.functional as F\n","import torch.sparse\n","import time\n","from numba import jit\n","import time\n","import os\n","import argparse\n","import pickle"],"metadata":{"id":"BL8LJStJETvq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset"],"metadata":{"id":"MeBacCorFWw9"}},{"cell_type":"code","source":["def data_masks(all_sessions, n_node):\n","    adj = dict()\n","    for sess in all_sessions:\n","        for i, item in enumerate(sess):\n","            if i == len(sess)-1:\n","                break\n","            else:\n","                if sess[i] - 1 not in adj.keys():\n","                    adj[sess[i]-1] = dict()\n","                    adj[sess[i]-1][sess[i]-1] = 1\n","                    adj[sess[i]-1][sess[i+1]-1] = 1\n","                else:\n","                    if sess[i+1]-1 not in adj[sess[i]-1].keys():\n","                        adj[sess[i] - 1][sess[i + 1] - 1] = 1\n","                    else:\n","                        adj[sess[i]-1][sess[i+1]-1] += 1\n","    row, col, data = [], [], []\n","    for i in adj.keys():\n","        item = adj[i]\n","        for j in item.keys():\n","            row.append(i)\n","            col.append(j)\n","            data.append(adj[i][j])\n","    coo = coo_matrix((data, (row, col)), shape=(n_node, n_node))\n","    return coo"],"metadata":{"id":"1NBSGWstETqF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Data():\n","    def __init__(self, data, all_train, shuffle=False, n_node=None):\n","        self.raw = np.asarray(data[0])\n","        adj = data_masks(all_train, n_node)\n","        # # print(adj.sum(axis=0))\n","        self.adjacency = adj.multiply(1.0/adj.sum(axis=0).reshape(1, -1))\n","        self.n_node = n_node\n","        self.targets = np.asarray(data[1])\n","        self.length = len(self.raw)\n","        self.shuffle = shuffle\n","\n","    def get_overlap(self, sessions):\n","        matrix = np.zeros((len(sessions), len(sessions)))\n","        for i in range(len(sessions)):\n","            seq_a = set(sessions[i])\n","            seq_a.discard(0)\n","            for j in range(i+1, len(sessions)):\n","                seq_b = set(sessions[j])\n","                seq_b.discard(0)\n","                overlap = seq_a.intersection(seq_b)\n","                ab_set = seq_a | seq_b\n","                matrix[i][j] = float(len(overlap))/float(len(ab_set))\n","                matrix[j][i] = matrix[i][j]\n","        # matrix = self.dropout(matrix, 0.2)\n","        matrix = matrix + np.diag([1.0]*len(sessions))\n","        degree = np.sum(np.array(matrix), 1)\n","        degree = np.diag(1.0/degree)\n","        return matrix, degree\n","\n","    def generate_batch(self, batch_size):\n","        if self.shuffle:\n","            shuffled_arg = np.arange(self.length)\n","            np.random.shuffle(shuffled_arg)\n","            self.raw = self.raw[shuffled_arg]\n","            self.targets = self.targets[shuffled_arg]\n","        n_batch = int(self.length / batch_size)\n","        if self.length % batch_size != 0:\n","            n_batch += 1\n","        slices = np.split(np.arange(n_batch * batch_size), n_batch)\n","        slices[-1] = np.arange(self.length-batch_size, self.length)\n","        return slices\n","\n","    def get_slice(self, index):\n","        items, num_node = [], []\n","        inp = self.raw[index]\n","        for session in inp:\n","            num_node.append(len(np.nonzero(session)[0]))\n","        max_n_node = np.max(num_node)\n","        session_len = []\n","        reversed_sess_item = []\n","        mask = []\n","        # item_set = set()\n","        for session in inp:\n","            nonzero_elems = np.nonzero(session)[0]\n","            # item_set.update(set([t-1 for t in session]))\n","            session_len.append([len(nonzero_elems)])\n","            items.append(session + (max_n_node - len(nonzero_elems)) * [0])\n","            mask.append([1]*len(nonzero_elems) + (max_n_node - len(nonzero_elems)) * [0])\n","            reversed_sess_item.append(list(reversed(session)) + (max_n_node - len(nonzero_elems)) * [0])\n","        # item_set = list(item_set)\n","        # index_list = [item_set.index(a) for a in self.targets[index]-1]\n","        diff_mask = np.ones(shape=[100, self.n_node]) * (1/(self.n_node - 1))\n","        for count, value in enumerate(self.targets[index]-1):\n","            diff_mask[count][value] = 1\n","        return self.targets[index]-1, session_len,items, reversed_sess_item, mask, diff_mask"],"metadata":{"id":"_9aSNmwTEYi_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Utils"],"metadata":{"id":"fB8DWdW8FaJp"}},{"cell_type":"code","source":["def trans_to_cuda(variable):\n","    if torch.cuda.is_available():\n","        return variable.cuda()\n","    else:\n","        return variable\n","\n","def trans_to_cpu(variable):\n","    if torch.cuda.is_available():\n","        return variable.cpu()\n","    else:\n","        return variable"],"metadata":{"id":"mwdd7VldETkB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model"],"metadata":{"id":"_LFNK75KFbpn"}},{"cell_type":"code","source":["class ItemConv(Module):\n","    def __init__(self, layers, emb_size=100):\n","        super(ItemConv, self).__init__()\n","        self.emb_size = emb_size\n","        self.layers = layers[0]\n","        self.w_item = {}\n","        for i in range(self.layers):\n","            self.w_item['weight_item%d' % (i)] = nn.Linear(self.emb_size, self.emb_size, bias=False)\n","\n","    def forward(self, adjacency, embedding):\n","        values = adjacency.data\n","        indices = np.vstack((adjacency.row, adjacency.col))\n","        i = torch.LongTensor(indices)\n","        v = torch.FloatTensor(values)\n","        shape = adjacency.shape\n","        adjacency = torch.sparse.FloatTensor(i, v, torch.Size(shape))\n","        item_embeddings = embedding\n","        item_embedding_layer0 = item_embeddings\n","        final = [item_embedding_layer0]\n","        for i in range(self.layers):\n","            item_embeddings = trans_to_cuda(self.w_item['weight_item%d' % (i)])(item_embeddings)\n","            item_embeddings = torch.sparse.mm(trans_to_cuda(adjacency), item_embeddings)\n","            final.append(F.normalize(item_embeddings, dim=-1, p=2))\n","        item_embeddings = np.sum(final, 0)/(self.layers+1)\n","        return item_embeddings"],"metadata":{"id":"2o29iEygEz5g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SessConv(Module):\n","    def __init__(self, layers, batch_size, emb_size=100):\n","        super(SessConv, self).__init__()\n","        self.emb_size = emb_size\n","        self.batch_size = batch_size\n","        self.layers = layers[0]\n","        self.w_sess = {}\n","        for i in range(self.layers):\n","            self.w_sess['weight_sess%d' % (i)] = nn.Linear(self.emb_size, self.emb_size, bias=False)\n","\n","    def forward(self, item_embedding, D, A, session_item, session_len):\n","        zeros = torch.cuda.FloatTensor(1, self.emb_size).fill_(0)\n","        # zeros = torch.zeros([1,self.emb_size])\n","        item_embedding = torch.cat([zeros, item_embedding], 0)\n","        seq_h = []\n","        for i in torch.arange(len(session_item)):\n","            seq_h.append(torch.index_select(item_embedding, 0, session_item[i]))\n","        seq_h1 = trans_to_cuda(torch.tensor([item.cpu().detach().numpy() for item in seq_h]))\n","        session_emb = torch.div(torch.sum(seq_h1, 1), session_len)\n","        session = [session_emb]\n","        DA = torch.mm(D, A).float()\n","        for i in range(self.layers):\n","            session_emb = trans_to_cuda(self.w_sess['weight_sess%d' % (i)])(session_emb)\n","            session_emb = torch.mm(DA, session_emb)\n","            session.append(F.normalize(session_emb, p=2, dim=-1))\n","        sess = trans_to_cuda(torch.tensor([item.cpu().detach().numpy() for item in session]))\n","        session_emb = torch.sum(sess, 0)/(self.layers+1)\n","        return session_emb"],"metadata":{"id":"ragoLFq2EyyE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class COTREC(Module):\n","    def __init__(self, adjacency, n_node, lr, layers, l2, beta,lam,eps, dataset, emb_size=100, batch_size=100):\n","        super(COTREC, self).__init__()\n","        self.emb_size = emb_size\n","        self.batch_size = batch_size\n","        self.n_node = n_node\n","        self.dataset = dataset\n","        self.L2 = l2\n","        self.lr = lr\n","        self.layers = layers\n","        self.beta = beta\n","        self.lam = lam\n","        self.eps = eps\n","        self.K = 10\n","        self.w_k = 10\n","        self.num = 5000\n","        self.adjacency = adjacency\n","        self.embedding = nn.Embedding(self.n_node, self.emb_size)\n","        self.pos_len = 200\n","        if self.dataset == 'retailrocket':\n","            self.pos_len = 300\n","        self.pos_embedding = nn.Embedding(self.pos_len, self.emb_size)\n","        self.ItemGraph = ItemConv(self.layers)\n","        self.SessGraph = SessConv(self.layers, self.batch_size)\n","        self.w_1 = nn.Parameter(torch.Tensor(2 * self.emb_size, self.emb_size))\n","        self.w_2 = nn.Parameter(torch.Tensor(self.emb_size, 1))\n","        self.w_i = nn.Linear(self.emb_size, self.emb_size)\n","        self.w_s = nn.Linear(self.emb_size, self.emb_size)\n","        self.glu1 = nn.Linear(self.emb_size, self.emb_size)\n","        self.glu2 = nn.Linear(self.emb_size, self.emb_size, bias=False)\n","\n","        self.adv_item = torch.cuda.FloatTensor(self.n_node, self.emb_size).fill_(0).requires_grad_(True)\n","        self.adv_sess = torch.cuda.FloatTensor(self.n_node, self.emb_size).fill_(0).requires_grad_(True)\n","        # self.adv_item = torch.zeros(self.n_node, self.emb_size).requires_grad_(True)\n","        # self.adv_sess = torch.zeros(self.n_node, self.emb_size).requires_grad_(True)\n","        self.loss_function = nn.CrossEntropyLoss()\n","        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n","        self.init_parameters()\n","\n","    def init_parameters(self):\n","        stdv = 1.0 / math.sqrt(self.emb_size)\n","        for weight in self.parameters():\n","            weight.data.uniform_(-stdv, stdv)\n","\n","    def generate_sess_emb(self, item_embedding, session_item, session_len, reversed_sess_item, mask):\n","        zeros = torch.cuda.FloatTensor(1, self.emb_size).fill_(0)\n","        # zeros = torch.zeros(1, self.emb_size)\n","        item_embedding = torch.cat([zeros, item_embedding], 0)\n","        get = lambda i: item_embedding[reversed_sess_item[i]]\n","        seq_h = torch.cuda.FloatTensor(self.batch_size, list(reversed_sess_item.shape)[1], self.emb_size).fill_(0)\n","        # seq_h = torch.zeros(self.batch_size, list(reversed_sess_item.shape)[1], self.emb_size)\n","        for i in torch.arange(session_item.shape[0]):\n","            seq_h[i] = get(i)\n","        hs = torch.div(torch.sum(seq_h, 1), session_len)\n","        mask = mask.float().unsqueeze(-1)\n","        len = seq_h.shape[1]\n","        pos_emb = self.pos_embedding.weight[:len]\n","        pos_emb = pos_emb.unsqueeze(0).repeat(self.batch_size, 1, 1)\n","\n","        hs = hs.unsqueeze(-2).repeat(1, len, 1)\n","        nh = torch.matmul(torch.cat([pos_emb, seq_h], -1), self.w_1)\n","        nh = torch.tanh(nh)\n","        nh = torch.sigmoid(self.glu1(nh) + self.glu2(hs))\n","        beta = torch.matmul(nh, self.w_2)\n","        beta = beta * mask\n","        select = torch.sum(beta * seq_h, 1)\n","        return select\n","\n","    def generate_sess_emb_npos(self, item_embedding, session_item, session_len, reversed_sess_item, mask):\n","        zeros = torch.cuda.FloatTensor(1, self.emb_size).fill_(0)\n","        # zeros = torch.zeros(1, self.emb_size)\n","        item_embedding = torch.cat([zeros, item_embedding], 0)\n","        get = lambda i: item_embedding[reversed_sess_item[i]]\n","        seq_h = torch.cuda.FloatTensor(self.batch_size, list(reversed_sess_item.shape)[1], self.emb_size).fill_(0)\n","        # seq_h = torch.zeros(self.batch_size, list(reversed_sess_item.shape)[1], self.emb_size)\n","        for i in torch.arange(session_item.shape[0]):\n","            seq_h[i] = get(i)\n","        hs = torch.div(torch.sum(seq_h, 1), session_len)\n","        mask = mask.float().unsqueeze(-1)\n","        len = seq_h.shape[1]\n","\n","        hs = hs.unsqueeze(-2).repeat(1, len, 1)\n","        nh = torch.sigmoid(self.glu1(seq_h) + self.glu2(hs))\n","        beta = torch.matmul(nh, self.w_2)\n","        beta = beta * mask\n","        select = torch.sum(beta * seq_h, 1)\n","        return select\n","\n","    def example_predicting(self, item_emb, sess_emb):\n","        x_u = torch.matmul(item_emb, sess_emb)\n","        pos = torch.softmax(x_u, 0)\n","        return pos\n","\n","    def adversarial_item(self, item_emb, tar,sess_emb):\n","        adv_item_emb = item_emb + self.adv_item\n","        score = torch.mm(sess_emb, torch.transpose(adv_item_emb, 1, 0))\n","        loss = self.loss_function(score, tar)\n","        grad = torch.autograd.grad(loss, self.adv_item,retain_graph=True)[0]\n","        adv = grad.detach()\n","        self.adv_item = (F.normalize(adv, p=2,dim=1) * self.eps).requires_grad_(True)\n","\n","    def adversarial_sess(self, item_emb, tar,sess_emb):\n","        adv_item_emb = item_emb + self.adv_sess\n","        score = torch.mm(sess_emb, torch.transpose(adv_item_emb, 1, 0))\n","        loss = self.loss_function(score, tar)\n","        grad = torch.autograd.grad(loss, self.adv_sess,retain_graph=True)[0]\n","        adv = grad.detach()\n","        self.adv_sess = (F.normalize(adv, p=2,dim=1) * self.eps).requires_grad_(True)\n","\n","    def diff(self, score_item, score_sess, score_adv2, score_adv1, diff_mask):\n","        # compute KL(score_item, score_adv2), KL(score_sess, score_adv1)\n","        score_item = F.softmax(score_item, dim=1)\n","        score_sess = F.softmax(score_sess, dim=1)\n","        score_adv2 = F.softmax(score_adv2, dim=1)\n","        score_adv1 = F.softmax(score_adv1, dim=1)\n","        score_item = torch.mul(score_item, diff_mask)\n","        score_sess = torch.mul(score_sess, diff_mask)\n","        score_adv1 = torch.mul(score_adv1, diff_mask)\n","        score_adv2 = torch.mul(score_adv2, diff_mask)\n","\n","        h1 = torch.sum(torch.mul(score_item, torch.log(1e-8 + ((score_item + 1e-8)/(score_adv2 + 1e-8)))))\n","        h2 = torch.sum(torch.mul(score_sess, torch.log(1e-8 + ((score_sess + 1e-8)/(score_adv1 + 1e-8)))))\n","\n","        return h1+h2\n","\n","    def SSL_topk(self, anchor, sess_emb, pos, neg):\n","        def score(x1, x2):\n","            return torch.sum(torch.mul(x1, x2), 2)\n","\n","        anchor = F.normalize(anchor + sess_emb, p=2, dim=-1)\n","        pos = torch.reshape(pos, (self.batch_size, self.K, self.emb_size)) + sess_emb.unsqueeze(1).repeat(1, self.K, 1)\n","        neg = torch.reshape(neg, (self.batch_size, self.K, self.emb_size)) + sess_emb.unsqueeze(1).repeat(1, self.K, 1)\n","        pos_score = score(anchor.unsqueeze(1).repeat(1, self.K, 1), F.normalize(pos, p=2, dim=-1))\n","        neg_score = score(anchor.unsqueeze(1).repeat(1, self.K, 1), F.normalize(neg, p=2, dim=-1))\n","        pos_score = torch.sum(torch.exp(pos_score / 0.2), 1)\n","        neg_score = torch.sum(torch.exp(neg_score / 0.2), 1)\n","        con_loss = -torch.sum(torch.log(pos_score / (pos_score + neg_score)))\n","        return con_loss\n","\n","    def topk_func_random(self, score1,score2, item_emb_I, item_emb_S):\n","        values, pos_ind_I = score1.topk(self.num, dim=0, largest=True, sorted=True)\n","        values, pos_ind_S = score2.topk(self.num, dim=0, largest=True, sorted=True)\n","        pos_emb_I = torch.cuda.FloatTensor(self.K, self.batch_size, self.emb_size).fill_(0)\n","        pos_emb_S = torch.cuda.FloatTensor(self.K, self.batch_size, self.emb_size).fill_(0)\n","        neg_emb_I = torch.cuda.FloatTensor(self.K, self.batch_size, self.emb_size).fill_(0)\n","        neg_emb_S = torch.cuda.FloatTensor(self.K, self.batch_size, self.emb_size).fill_(0)\n","        for i in torch.arange(self.K):\n","            pos_emb_S[i] = item_emb_S[pos_ind_I[i]]\n","            pos_emb_I[i] = item_emb_I[pos_ind_S[i]]\n","        random_slices = torch.randint(self.K, self.num, (self.K,))  # choose negative items\n","        for i in torch.arange(self.K):\n","            neg_emb_S[i] = item_emb_S[pos_ind_I[random_slices[i]]]\n","            neg_emb_I[i] = item_emb_I[pos_ind_S[random_slices[i]]]\n","        return pos_emb_I, neg_emb_I, pos_emb_S, neg_emb_S\n","\n","    def forward(self, session_item, session_len, D, A, reversed_sess_item, mask, epoch, tar, train, diff_mask):\n","        if train:\n","            item_embeddings_i = self.ItemGraph(self.adjacency, self.embedding.weight)\n","            if self.dataset == 'Tmall':\n","                # for Tmall dataset, we do not use position embedding to learn temporal order\n","                sess_emb_i = self.generate_sess_emb_npos(item_embeddings_i, session_item, session_len,reversed_sess_item, mask)\n","            else:\n","                sess_emb_i = self.generate_sess_emb(item_embeddings_i, session_item, session_len, reversed_sess_item, mask)\n","            sess_emb_i = self.w_k * F.normalize(sess_emb_i, dim=-1, p=2)\n","            item_embeddings_i = F.normalize(item_embeddings_i, dim=-1, p=2)\n","            scores_item = torch.mm(sess_emb_i, torch.transpose(item_embeddings_i, 1, 0))\n","            loss_item = self.loss_function(scores_item, tar)\n","\n","            sess_emb_s = self.SessGraph(self.embedding.weight, D, A, session_item, session_len)\n","            scores_sess = torch.mm(sess_emb_s, torch.transpose(item_embeddings_i, 1, 0))\n","            # compute probability of items to be positive examples\n","            pos_prob_I = self.example_predicting(item_embeddings_i, sess_emb_i)\n","            pos_prob_S = self.example_predicting(self.embedding.weight, sess_emb_s)\n","\n","            # choose top-10 items as positive samples and randomly choose 10 items as negative and get their embedding\n","            pos_emb_I, neg_emb_I, pos_emb_S, neg_emb_S = self.topk_func_random(pos_prob_I,pos_prob_S, item_embeddings_i, self.embedding.weight)\n","\n","            last_item = torch.squeeze(reversed_sess_item[:, 0])\n","            last_item = last_item - 1\n","            last = item_embeddings_i.index_select(0, last_item)\n","            con_loss = self.SSL_topk(last, sess_emb_i, pos_emb_I, neg_emb_I)\n","            last = self.embedding(last_item)\n","            con_loss += self.SSL_topk(last, sess_emb_s, pos_emb_S, neg_emb_S)\n","\n","            # compute and update adversarial examples\n","            self.adversarial_item(item_embeddings_i, tar, sess_emb_i)\n","            self.adversarial_sess(item_embeddings_i, tar, sess_emb_s)\n","\n","            adv_emb_item = item_embeddings_i + self.adv_item\n","            adv_emb_sess = item_embeddings_i + self.adv_sess\n","\n","            score_adv1 = torch.mm(sess_emb_s, torch.transpose(adv_emb_item, 1, 0))\n","            score_adv2 = torch.mm(sess_emb_i, torch.transpose(adv_emb_sess, 1, 0))\n","            # add difference constraint\n","            loss_diff = self.diff(scores_item, scores_sess, score_adv2, score_adv1, diff_mask)\n","        else:\n","            item_embeddings_i = self.ItemGraph(self.adjacency, self.embedding.weight)\n","            if self.dataset == 'Tmall':\n","                sess_emb_i = self.generate_sess_emb_npos(item_embeddings_i, session_item, session_len, reversed_sess_item, mask)\n","            else:\n","                sess_emb_i = self.generate_sess_emb(item_embeddings_i, session_item, session_len, reversed_sess_item, mask)\n","            sess_emb_i = self.w_k * F.normalize(sess_emb_i, dim=-1, p=2)\n","            item_embeddings_i = F.normalize(item_embeddings_i, dim=-1, p=2)\n","            scores_item = torch.mm(sess_emb_i, torch.transpose(item_embeddings_i, 1, 0))\n","            loss_item = self.loss_function(scores_item, tar)\n","            loss_diff = 0\n","            con_loss = 0\n","        return self.beta * con_loss, loss_item, scores_item, loss_diff*self.lam"],"metadata":{"id":"lV-0ePwdExw5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def forward(model, i, data, epoch, train):\n","    tar, session_len, session_item, reversed_sess_item, mask, diff_mask = data.get_slice(i)\n","    diff_mask = trans_to_cuda(torch.Tensor(diff_mask).long())\n","    A_hat, D_hat = data.get_overlap(session_item)\n","    session_item = trans_to_cuda(torch.Tensor(session_item).long())\n","    session_len = trans_to_cuda(torch.Tensor(session_len).long())\n","    A_hat = trans_to_cuda(torch.Tensor(A_hat))\n","    D_hat = trans_to_cuda(torch.Tensor(D_hat))\n","    tar = trans_to_cuda(torch.Tensor(tar).long())\n","    mask = trans_to_cuda(torch.Tensor(mask).long())\n","    reversed_sess_item = trans_to_cuda(torch.Tensor(reversed_sess_item).long())\n","    con_loss, loss_item, scores_item, loss_diff = model(session_item, session_len, D_hat, A_hat, reversed_sess_item, mask, epoch,tar, train, diff_mask)\n","    return tar, scores_item, con_loss, loss_item, loss_diff"],"metadata":{"id":"F4RYYnkVEvjH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@jit(nopython=True)\n","def find_k_largest(K, candidates):\n","    n_candidates = []\n","    for iid,score in enumerate(candidates[:K]):\n","        n_candidates.append((iid, score))\n","    n_candidates.sort(key=lambda d: d[1], reverse=True)\n","    k_largest_scores = [item[1] for item in n_candidates]\n","    ids = [item[0] for item in n_candidates]\n","    # find the N biggest scores\n","    for iid,score in enumerate(candidates):\n","        ind = K\n","        l = 0\n","        r = K - 1\n","        if k_largest_scores[r] < score:\n","            while r >= l:\n","                mid = int((r - l) / 2) + l\n","                if k_largest_scores[mid] >= score:\n","                    l = mid + 1\n","                elif k_largest_scores[mid] < score:\n","                    r = mid - 1\n","                if r < l:\n","                    ind = r\n","                    break\n","        # move the items backwards\n","        if ind < K - 2:\n","            k_largest_scores[ind + 2:] = k_largest_scores[ind + 1:-1]\n","            ids[ind + 2:] = ids[ind + 1:-1]\n","        if ind < K - 1:\n","            k_largest_scores[ind + 1] = score\n","            ids[ind + 1] = iid\n","    return ids#,k_largest_scores"],"metadata":{"id":"aR_vVZ6tEuoL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_test(model, train_data, test_data, epoch):\n","    print('start training: ', datetime.datetime.now())\n","    total_loss = 0.0\n","    slices = train_data.generate_batch(model.batch_size)\n","    for i in slices:\n","        model.zero_grad()\n","        tar, scores_item, con_loss, loss_item, loss_diff = forward(model, i, train_data, epoch, train=True)\n","        loss = loss_item + con_loss + loss_diff\n","        loss.backward()\n","        model.optimizer.step()\n","        total_loss += loss.item()\n","    print('\\tLoss:\\t%.3f' % total_loss)\n","    top_K = [5, 10, 20]\n","    metrics = {}\n","    for K in top_K:\n","        metrics['hit%d' % K] = []\n","        metrics['mrr%d' % K] = []\n","    print('start predicting: ', datetime.datetime.now())\n","\n","    model.eval()\n","    slices = test_data.generate_batch(model.batch_size)\n","    for i in slices:\n","        tar,scores_item, con_loss, loss_item, loss_diff = forward(model, i, test_data, epoch, train=False)\n","        scores = trans_to_cpu(scores_item).detach().numpy()\n","        index = []\n","        for idd in range(model.batch_size):\n","            index.append(find_k_largest(20, scores[idd]))\n","        index = np.array(index)\n","        tar = trans_to_cpu(tar).detach().numpy()\n","        for K in top_K:\n","            for prediction, target in zip(index[:, :K], tar):\n","                metrics['hit%d' % K].append(np.isin(target, prediction))\n","                if len(np.where(prediction == target)[0]) == 0:\n","                    metrics['mrr%d' % K].append(0)\n","                else:\n","                    metrics['mrr%d' % K].append(1 / (np.where(prediction == target)[0][0] + 1))\n","    return metrics, total_loss"],"metadata":{"id":"rGN4s_UzEtKp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Experiment"],"metadata":{"id":"DQIMDzHsFQ4y"}},{"cell_type":"markdown","source":["### Config"],"metadata":{"id":"lFs2goSZFTK3"}},{"cell_type":"code","source":["class Args:\n","    def __init__(self, dataset='tmall'):\n","        self.dataset = dataset\n","        self.epoch = 30\n","        self.batchSize = 100\n","        self.embSize = 100\n","        self.l2 = 1e-5 # l2 penalty\n","        self.lr = 0.001 # learning rate\n","        self.layer = 2, # he number of layer used\n","        self.beta = 0.005 # ssl task maginitude\n","        self.lam = 0.005 # diff task maginitude\n","        self.eps = 0.2 # eps\n","        if self.dataset == 'diginetica':\n","            self.n_node = 43097\n","            self.eps = 0.5\n","        elif self.dataset == 'tmall':\n","            self.n_node = 40727\n","        elif self.dataset == 'retailrocket':\n","            self.n_node = 36968"],"metadata":{"id":"N4K7VTjpFkyy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["opt = Args()\n","\n","opt.dataset = input('Dataset options=tmall/diginetica/retail_rocket, default=tmall:') or 'tmall'\n","opt.epoch = int(input('Training epochs type=int, default=30:') or '30')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jO3LB3xrE6VD","executionInfo":{"status":"ok","timestamp":1639321562209,"user_tz":-330,"elapsed":6489,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"eded1e1f-70c3-4ba1-8ba7-e847daca009d"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset options=tmall/diginetica/retail_rocket, default=tmall:\n","Training epochs type=int, default=30:2\n"]}]},{"cell_type":"markdown","source":["### Data acquisition"],"metadata":{"id":"dZOWaIHMHlaE"}},{"cell_type":"code","source":["if opt.dataset == 'tmall':\n","    !git clone --branch v1 https://github.com/RecoHut-Datasets/tmall.git\n","elif opt.dataset == 'diginetica':\n","    !git clone --branch v2 https://github.com/RecoHut-Datasets/diginetica.git\n","elif opt.dataset == 'retail_rocket':\n","    !git clone --branch v1 https://github.com/RecoHut-Datasets/retail_rocket.git\n","else:\n","    raise ValueError()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pXIZ-LNXE6SM","executionInfo":{"status":"ok","timestamp":1639321568933,"user_tz":-330,"elapsed":1889,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"f7c431f7-aa68-4549-c1b7-a166d7170bf7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'tmall'...\n","remote: Enumerating objects: 19, done.\u001b[K\n","remote: Counting objects: 100% (19/19), done.\u001b[K\n","remote: Compressing objects: 100% (13/13), done.\u001b[K\n","remote: Total 19 (delta 2), reused 15 (delta 2), pack-reused 0\u001b[K\n","Unpacking objects: 100% (19/19), done.\n"]}]},{"cell_type":"markdown","source":["### Main"],"metadata":{"id":"F4OC5AzQHjhZ"}},{"cell_type":"code","source":["def main():\n","    train_data = pickle.load(open(opt.dataset + '/train.txt', 'rb'))\n","    test_data = pickle.load(open(opt.dataset + '/test.txt', 'rb'))\n","    all_train = pickle.load(open(opt.dataset + '/all_train_seq.txt', 'rb'))\n","\n","    train_data = Data(train_data,all_train, shuffle=True, n_node=opt.n_node)\n","    test_data = Data(test_data,all_train, shuffle=True, n_node=opt.n_node)\n","    model = trans_to_cuda(COTREC(adjacency=train_data.adjacency,n_node=opt.n_node,lr=opt.lr, l2=opt.l2, beta=opt.beta,lam= opt.lam,eps=opt.eps,layers=opt.layer,emb_size=opt.embSize, batch_size=opt.batchSize,dataset=opt.dataset))\n","    top_K = [5, 10, 20]\n","    best_results = {}\n","    for K in top_K:\n","        best_results['epoch%d' % K] = [0, 0]\n","        best_results['metric%d' % K] = [0, 0]\n","\n","    for epoch in range(opt.epoch):\n","        print('-------------------------------------------------------')\n","        print('epoch: ', epoch)\n","        metrics, total_loss = train_test(model, train_data, test_data, epoch)\n","        for K in top_K:\n","            metrics['hit%d' % K] = np.mean(metrics['hit%d' % K]) * 100\n","            metrics['mrr%d' % K] = np.mean(metrics['mrr%d' % K]) * 100\n","            if best_results['metric%d' % K][0] < metrics['hit%d' % K]:\n","                best_results['metric%d' % K][0] = metrics['hit%d' % K]\n","                best_results['epoch%d' % K][0] = epoch\n","            if best_results['metric%d' % K][1] < metrics['mrr%d' % K]:\n","                best_results['metric%d' % K][1] = metrics['mrr%d' % K]\n","                best_results['epoch%d' % K][1] = epoch\n","        print(metrics)\n","        for K in top_K:\n","            print('train_loss:\\t%.4f\\tRecall@%d: %.4f\\tMRR%d: %.4f\\tEpoch: %d,  %d' %\n","                  (total_loss, K, best_results['metric%d' % K][0], K, best_results['metric%d' % K][1],\n","                   best_results['epoch%d' % K][0], best_results['epoch%d' % K][1]))"],"metadata":{"id":"AQ_fUnI7E6Pd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Run"],"metadata":{"id":"sedn_4vAHiQn"}},{"cell_type":"code","source":["if __name__ == '__main__':\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kCwmqwv1E6MW","executionInfo":{"status":"ok","timestamp":1639325397177,"user_tz":-330,"elapsed":3484549,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"8897f524-16ef-444f-89fa-3e6748769765"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order)\n","/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order)\n"]},{"output_type":"stream","name":"stdout","text":["-------------------------------------------------------\n","epoch:  0\n","start training:  2021-12-12 15:12:03.091838\n","\tLoss:\t23375.937\n","start predicting:  2021-12-12 15:40:28.735932\n","{'hit5': 22.393822393822393, 'mrr5': 15.647490347490345, 'hit10': 27.459459459459463, 'mrr10': 16.32417877060734, 'hit20': 32.58687258687259, 'mrr20': 16.678519347935534}\n","train_loss:\t23375.9368\tRecall@5: 22.3938\tMRR5: 15.6475\tEpoch: 0,  0\n","train_loss:\t23375.9368\tRecall@10: 27.4595\tMRR10: 16.3242\tEpoch: 0,  0\n","train_loss:\t23375.9368\tRecall@20: 32.5869\tMRR20: 16.6785\tEpoch: 0,  0\n","-------------------------------------------------------\n","epoch:  1\n","start training:  2021-12-12 15:41:06.338573\n","\tLoss:\t20742.387\n","start predicting:  2021-12-12 16:09:31.978426\n","{'hit5': 23.142857142857142, 'mrr5': 15.88153153153153, 'hit10': 28.474903474903474, 'mrr10': 16.597807501378934, 'hit20': 34.11969111969112, 'mrr20': 16.991088820478474}\n","train_loss:\t20742.3874\tRecall@5: 23.1429\tMRR5: 15.8815\tEpoch: 1,  1\n","train_loss:\t20742.3874\tRecall@10: 28.4749\tMRR10: 16.5978\tEpoch: 1,  1\n","train_loss:\t20742.3874\tRecall@20: 34.1197\tMRR20: 16.9911\tEpoch: 1,  1\n"]}]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"mVXR1idQaHda"}},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jf-FC4UBaKyL","executionInfo":{"status":"ok","timestamp":1639325418944,"user_tz":-330,"elapsed":719,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"f2ea18f0-8a53-4bec-c06a-a4a22097d65a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Dec 12 16:10:26 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   71C    P0    70W / 149W |   1533MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["!pip install -q watermark\n","%reload_ext watermark\n","%watermark -a \"Sparsh A.\" -m -iv -u -t -d"],"metadata":{"id":"sl-MH_X4aHdb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639325433456,"user_tz":-330,"elapsed":3935,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"fba3eb4b-8b63-4c81-dd18-8b07b0089d4e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Author: Sparsh A.\n","\n","Last updated: 2021-12-12 16:10:41\n","\n","Compiler    : GCC 7.5.0\n","OS          : Linux\n","Release     : 5.4.104+\n","Machine     : x86_64\n","Processor   : x86_64\n","CPU cores   : 2\n","Architecture: 64bit\n","\n","argparse: 1.1\n","numpy   : 1.19.5\n","torch   : 1.10.0+cu111\n","IPython : 5.5.0\n","\n"]}]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"YfczFAHcaHdc"}},{"cell_type":"markdown","source":["**END**"],"metadata":{"id":"LN23OxNzaHdc"}}]}